# –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –¥–ª—è DevOps: –ï–∂–µ–≥–æ–¥–Ω—ã–π/–ü–æ–ª—É–≥–æ–¥–æ–≤–æ–π –∫—É—Ä—Å-–æ—Å–≤–µ–∂–∏—Ç–µ–ª—å

**–¶–µ–ª—å:** –û—Å–≤–µ–∂–∏—Ç—å –≤ –ø–∞–º—è—Ç–∏ –∫–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∑–∞ 2-3 —á–∞—Å–∞ –ø—Ä–∞–∫—Ç–∏–∫–∏ –∏ —É–∑–Ω–∞—Ç—å 1-2 –Ω–æ–≤—ã–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏.

**–§–æ—Ä–º–∞—Ç:** –ö–∞–∂–¥—ã–π —Ä–∞–∑–¥–µ–ª —Å–æ—Å—Ç–æ–∏—Ç –∏–∑:
1. **–ö—Ä–∞—Ç–∫–æ–π —Ç–µ–æ—Ä–∏–∏ (–ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞)**: –°–∞–º–æ–µ –≥–ª–∞–≤–Ω–æ–µ, —á—Ç–æ –≤—ã –º–æ–≥–ª–∏ –∑–∞–±—ã—Ç—å
2. **–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–≥–æ –∑–∞–¥–∞–Ω–∏—è**: –†–µ–∞–ª—å–Ω–∞—è –∑–∞–¥–∞—á–∞, –∫–æ—Ç–æ—Ä—É—é –Ω—É–∂–Ω–æ —Ä–µ—à–∏—Ç—å
3. **–ë–æ–Ω—É—Å–Ω–æ–≥–æ –∑–∞–¥–∞–Ω–∏—è (–¥–ª—è —Ä–æ—Å—Ç–∞)**: –ó–∞–¥–∞—á–∞ –ø–æ—Å–ª–æ–∂–Ω–µ–µ –∏–ª–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –Ω–æ–≤–æ–π —Ñ–∏—á–∏

**–ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è:**
- –ë–∞–∑–æ–≤–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ Linux/Unix
- –î–æ—Å—Ç—É–ø –∫ —Å–µ—Ä–≤–µ—Ä—É –∏–ª–∏ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–π –º–∞—à–∏–Ω–µ
- Docker —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω (–¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ –∑–∞–¥–∞–Ω–∏–π)
- –ë–∞–∑–æ–≤—ã–µ –∑–Ω–∞–Ω–∏—è –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏

---

## –ú–æ–¥—É–ª—å 1: –û—Å–Ω–æ–≤—ã –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∏ –º–µ—Ç—Ä–∏–∫–∏ (20 –º–∏–Ω—É—Ç)

### üéØ –ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞

**–ß–µ—Ç—ã—Ä–µ –∑–æ–ª–æ—Ç—ã—Ö —Å–∏–≥–Ω–∞–ª–∞ (Four Golden Signals):**
```
1. Latency (–ó–∞–¥–µ—Ä–∂–∫–∞)      - –í—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –∑–∞–ø—Ä–æ—Å—ã
2. Traffic (–¢—Ä–∞—Ñ–∏–∫)        - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø—Ä–æ—Å–æ–≤
3. Errors (–û—à–∏–±–∫–∏)         - –ü—Ä–æ—Ü–µ–Ω—Ç –Ω–µ—É–¥–∞—á–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
4. Saturation (–ù–∞—Å—ã—â–µ–Ω–∏–µ)  - –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤ (CPU, –ø–∞–º—è—Ç—å, –¥–∏—Å–∫)
```

**–¢–∏–ø—ã –º–µ—Ç—Ä–∏–∫:**
```
Counter   - –ú–æ–Ω–æ—Ç–æ–Ω–Ω–æ –≤–æ–∑—Ä–∞—Å—Ç–∞—é—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ (–∑–∞–ø—Ä–æ—Å—ã, –æ—à–∏–±–∫–∏)
Gauge     - –¢–µ–∫—É—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ (CPU, –ø–∞–º—è—Ç—å, —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞)
Histogram - –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π (latency buckets)
Summary   - –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∑–∞ –ø–µ—Ä–∏–æ–¥ (percentiles)
```

**USE Method (–¥–ª—è —Ä–µ—Å—É—Ä—Å–æ–≤):**
```
Utilization - –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –∑–∞–Ω—è—Ç–æ—Å—Ç–∏ —Ä–µ—Å—É—Ä—Å–∞
Saturation  - –°—Ç–µ–ø–µ–Ω—å –ø–µ—Ä–µ–≥—Ä—É–∑–∫–∏
Errors      - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—à–∏–±–æ–∫
```

**RED Method (–¥–ª—è —Å–µ—Ä–≤–∏—Å–æ–≤):**
```
Rate     - –ó–∞–ø—Ä–æ—Å–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É
Errors   - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—à–∏–±–æ–∫
Duration - –í—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞
```

**–£—Ä–æ–≤–Ω–∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Application (APM)             ‚îÇ  - –ö–æ–¥, —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ   Service/Container             ‚îÇ  - Docker, K8s
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ   Operating System              ‚îÇ  - CPU, RAM, Disk
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ   Infrastructure                ‚îÇ  - Network, Hardware
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**–ö–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ Linux:**
```bash
# CPU
top, htop
mpstat -P ALL 1

# Memory
free -m
vmstat 1

# Disk I/O
iostat -x 1
iotop

# Network
iftop
nethogs
ss -s

# Process
ps aux --sort=-%mem | head
ps aux --sort=-%cpu | head
```

**–ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π:**
```
- Request rate (req/s)
- Error rate (%)
- Response time (ms) - p50, p95, p99
- Active connections
- Queue depth
- Database query time
- Cache hit ratio
```

### üíª –ó–∞–¥–∞–Ω–∏–µ

–ù–∞—Å—Ç—Ä–æ–π –±–∞–∑–æ–≤—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Å–∏—Å—Ç–µ–º—ã:

1. **–£—Å—Ç–∞–Ω–æ–≤–∏ –∏ –∑–∞–ø—É—Å—Ç–∏ Node Exporter** (–¥–ª—è —Å–±–æ—Ä–∞ –º–µ—Ç—Ä–∏–∫ —Ö–æ—Å—Ç–∞):
```bash
# –ß–µ—Ä–µ–∑ Docker
docker run -d \
  --name node-exporter \
  --net="host" \
  --pid="host" \
  -v "/:/host:ro,rslave" \
  prom/node-exporter:latest \
  --path.rootfs=/host

# –ü—Ä–æ–≤–µ—Ä–∫–∞
curl http://localhost:9100/metrics | head -20
```

2. **–ò–∑—É—á–∏ –æ—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏**:
```bash
# CPU
curl -s http://localhost:9100/metrics | grep node_cpu_seconds_total

# Memory
curl -s http://localhost:9100/metrics | grep node_memory

# Disk
curl -s http://localhost:9100/metrics | grep node_disk

# Network
curl -s http://localhost:9100/metrics | grep node_network
```

3. **–°–æ–∑–¥–∞–π –ø—Ä–æ—Å—Ç–æ–π bash —Å–∫—Ä–∏–ø—Ç** –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ (`monitor.sh`):
```bash
#!/bin/bash

echo "=== System Monitoring Report ==="
echo "Date: $(date)"
echo ""

# CPU Usage
echo "CPU Usage:"
top -bn1 | grep "Cpu(s)" | awk '{print "  User: " $2 ", System: " $4 ", Idle: " $8}'

# Memory Usage
echo ""
echo "Memory Usage:"
free -h | awk 'NR==2{printf "  Total: %s, Used: %s (%.2f%%)\n", $2, $3, $3*100/$2}'

# Disk Usage
echo ""
echo "Disk Usage:"
df -h / | awk 'NR==2{printf "  Total: %s, Used: %s (%s)\n", $2, $3, $5}'

# Load Average
echo ""
echo "Load Average:"
uptime | awk -F'load average:' '{print "  " $2}'

# Top 5 processes by CPU
echo ""
echo "Top 5 processes by CPU:"
ps aux --sort=-%cpu | head -6 | tail -5 | awk '{printf "  %s: %.1f%%\n", $11, $3}'

# Top 5 processes by Memory
echo ""
echo "Top 5 processes by Memory:"
ps aux --sort=-%mem | head -6 | tail -5 | awk '{printf "  %s: %.1f%%\n", $11, $4}'
```

4. –ó–∞–ø—É—Å—Ç–∏ —Å–∫—Ä–∏–ø—Ç:
```bash
chmod +x monitor.sh
./monitor.sh
```

### üöÄ –ë–æ–Ω—É—Å (–Ω–æ–≤–æ–µ)

**–ù–∞—Å—Ç—Ä–æ–π cAdvisor** –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ Docker –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤:
```bash
docker run -d \
  --name=cadvisor \
  --volume=/:/rootfs:ro \
  --volume=/var/run:/var/run:ro \
  --volume=/sys:/sys:ro \
  --volume=/var/lib/docker/:/var/lib/docker:ro \
  --publish=8080:8080 \
  --detach=true \
  gcr.io/cadvisor/cadvisor:latest

# –û—Ç–∫—Ä–æ–π –≤ –±—Ä–∞—É–∑–µ—Ä–µ
http://localhost:8080
```

**–°–æ–∑–¥–∞–π —Å–≤–æ–π custom exporter** –Ω–∞ Python:
```python
# custom_exporter.py
from prometheus_client import start_http_server, Gauge, Counter
import time
import random

# –°–æ–∑–¥–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
request_gauge = Gauge('app_requests_in_progress', 'Number of requests in progress')
request_counter = Counter('app_requests_total', 'Total number of requests')
error_counter = Counter('app_errors_total', 'Total number of errors')

def process_request():
    """–°–∏–º—É–ª–∏—Ä—É–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –∑–∞–ø—Ä–æ—Å–∞"""
    request_gauge.inc()
    request_counter.inc()
    
    # –°–ª—É—á–∞–π–Ω–∞—è –æ—à–∏–±–∫–∞
    if random.random() < 0.1:
        error_counter.inc()
    
    time.sleep(random.uniform(0.1, 0.5))
    request_gauge.dec()

if __name__ == '__main__':
    start_http_server(8000)
    print("Exporter started on port 8000")
    
    while True:
        process_request()
        time.sleep(random.uniform(0.5, 2))
```

---

## –ú–æ–¥—É–ª—å 2: Prometheus - —Å–±–æ—Ä –∏ —Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ (30 –º–∏–Ω—É—Ç)

### üéØ –ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞

**–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ Prometheus:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Targets   ‚îÇ ‚Üê HTTP Pull (scrape)
‚îÇ  (Metrics)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
   ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ Prom-  ‚îÇ
   ‚îÇ etheus ‚îÇ ‚Üê Time Series DB (TSDB)
   ‚îÇ Server ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
   ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ Alert- ‚îÇ
   ‚îÇ manager‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Prometheus config structure:**
```yaml
global:
  scrape_interval: 15s      # –ö–∞–∫ —á–∞—Å—Ç–æ —Å–æ–±–∏—Ä–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏
  evaluation_interval: 15s  # –ö–∞–∫ —á–∞—Å—Ç–æ –ø—Ä–æ–≤–µ—Ä—è—Ç—å –ø—Ä–∞–≤–∏–ª–∞

scrape_configs:
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']
```

**PromQL –æ—Å–Ω–æ–≤—ã:**
```promql
# Instant vector - —Ç–µ–∫—É—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
node_cpu_seconds_total

# Range vector - –∑–Ω–∞—á–µ–Ω–∏—è –∑–∞ –ø–µ—Ä–∏–æ–¥
node_cpu_seconds_total[5m]

# –§–∏–ª—å—Ç—Ä—ã
node_cpu_seconds_total{mode="idle"}
node_cpu_seconds_total{mode!="idle"}
node_cpu_seconds_total{mode=~"user|system"}

# –ê–≥—Ä–µ–≥–∞—Ü–∏—è
sum(node_cpu_seconds_total)
avg(node_cpu_seconds_total)
max(node_cpu_seconds_total)
min(node_cpu_seconds_total)
count(node_cpu_seconds_total)

# –ü–æ label
sum(node_cpu_seconds_total) by (mode)
sum(node_cpu_seconds_total) by (cpu)

# –§—É–Ω–∫—Ü–∏–∏
rate(node_cpu_seconds_total[5m])           # –°–∫–æ—Ä–æ—Å—Ç—å –∏–∑–º–µ–Ω–µ–Ω–∏—è
irate(node_cpu_seconds_total[5m])          # –ú–≥–Ω–æ–≤–µ–Ω–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å
increase(node_cpu_seconds_total[5m])       # –£–≤–µ–ª–∏—á–µ–Ω–∏–µ –∑–∞ –ø–µ—Ä–∏–æ–¥
delta(node_cpu_seconds_total[5m])          # –ò–∑–º–µ–Ω–µ–Ω–∏–µ
```

**–†–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã:**
```promql
# CPU utilization
100 - (avg by (instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)

# Memory usage %
(1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100

# Disk usage %
100 - ((node_filesystem_avail_bytes / node_filesystem_size_bytes) * 100)

# Network traffic
rate(node_network_receive_bytes_total[5m])
rate(node_network_transmit_bytes_total[5m])

# HTTP request rate
rate(http_requests_total[5m])

# Error rate
rate(http_requests_total{status=~"5.."}[5m]) / rate(http_requests_total[5m])

# Latency percentiles (–¥–ª—è histogram)
histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))
histogram_quantile(0.99, rate(http_request_duration_seconds_bucket[5m]))
```

**Metric types –≤ –¥–µ—Ç–∞–ª—è—Ö:**
```promql
# Counter - —Ç–æ–ª—å–∫–æ —Ä–∞—Å—Ç–µ—Ç
http_requests_total
# –ò—Å–ø–æ–ª—å–∑—É–π rate() –∏–ª–∏ increase()
rate(http_requests_total[5m])

# Gauge - –º–æ–∂–µ—Ç —Ä–∞—Å—Ç–∏ –∏ –ø–∞–¥–∞—Ç—å
node_memory_MemAvailable_bytes
# –ò—Å–ø–æ–ª—å–∑—É–π –Ω–∞–ø—Ä—è–º—É—é –∏–ª–∏ —Å —Ñ—É–Ω–∫—Ü–∏—è–º–∏ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏
avg(node_memory_MemAvailable_bytes)

# Histogram - —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π
http_request_duration_seconds_bucket
http_request_duration_seconds_sum
http_request_duration_seconds_count
# –ò—Å–ø–æ–ª—å–∑—É–π histogram_quantile()
histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))

# Summary - –ø—Ä–µ–¥—Ä–∞—Å—á–∏—Ç–∞–Ω–Ω—ã–µ –∫–≤–∞–Ω—Ç–∏–ª–∏
http_request_duration_seconds{quantile="0.95"}
```

**Recording rules** (–¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏):
```yaml
groups:
  - name: example
    interval: 30s
    rules:
    - record: job:node_cpu_utilization:avg
      expr: 100 - (avg by (job) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)
```

**Alerting rules:**
```yaml
groups:
  - name: alerts
    rules:
    - alert: HighCPUUsage
      expr: job:node_cpu_utilization:avg > 80
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "High CPU usage on {{ $labels.instance }}"
        description: "CPU usage is {{ $value }}%"
```

### üíª –ó–∞–¥–∞–Ω–∏–µ

–ù–∞—Å—Ç—Ä–æ–π –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π Prometheus:

1. **–°–æ–∑–¥–∞–π docker-compose.yml**:
```yaml
version: '3.8'

services:
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - ./alerts.yml:/etc/prometheus/alerts.yml
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--web.enable-lifecycle'
    restart: unless-stopped

  node-exporter:
    image: prom/node-exporter:latest
    container_name: node-exporter
    ports:
      - "9100:9100"
    command:
      - '--path.rootfs=/host'
    pid: host
    restart: unless-stopped
    volumes:
      - '/:/host:ro,rslave'

  cadvisor:
    image: gcr.io/cadvisor/cadvisor:latest
    container_name: cadvisor
    ports:
      - "8080:8080"
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
      - /dev/disk/:/dev/disk:ro
    privileged: true
    restart: unless-stopped

volumes:
  prometheus-data:
```

2. **–°–æ–∑–¥–∞–π prometheus.yml**:
```yaml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–∞–≤–∏–ª –∞–ª–µ—Ä—Ç–æ–≤
rule_files:
  - "alerts.yml"

scrape_configs:
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  - job_name: 'node-exporter'
    static_configs:
      - targets: ['node-exporter:9100']

  - job_name: 'cadvisor'
    static_configs:
      - targets: ['cadvisor:8080']
```

3. **–°–æ–∑–¥–∞–π alerts.yml**:
```yaml
groups:
  - name: system_alerts
    rules:
    - alert: HighCPUUsage
      expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "High CPU usage detected"
        description: "CPU usage is above 80% (current value: {{ $value }}%)"

    - alert: HighMemoryUsage
      expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 90
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "High memory usage detected"
        description: "Memory usage is above 90% (current value: {{ $value }}%)"

    - alert: DiskSpaceLow
      expr: (1 - (node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"})) * 100 > 85
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Low disk space"
        description: "Disk usage is above 85% (current value: {{ $value }}%)"

    - alert: InstanceDown
      expr: up == 0
      for: 1m
      labels:
        severity: critical
      annotations:
        summary: "Instance {{ $labels.instance }} down"
        description: "{{ $labels.instance }} has been down for more than 1 minute"
```

4. **–ó–∞–ø—É—Å—Ç–∏ stack**:
```bash
docker-compose up -d

# –ü—Ä–æ–≤–µ—Ä–∫–∞
docker-compose ps
curl http://localhost:9090/api/v1/targets
```

5. **–û—Ç–∫—Ä–æ–π Prometheus UI** –∏ –ø–æ–ø—Ä–æ–±—É–π –∑–∞–ø—Ä–æ—Å—ã:
```
–ü–µ—Ä–µ–π–¥–∏: http://localhost:9090

–ü–æ–ø—Ä–æ–±—É–π –∑–∞–ø—Ä–æ—Å—ã:
- node_cpu_seconds_total
- rate(node_cpu_seconds_total[5m])
- 100 - (avg(irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)
- node_memory_MemAvailable_bytes / 1024 / 1024 / 1024
```

### üöÄ –ë–æ–Ω—É—Å (–Ω–æ–≤–æ–µ)

**–ù–∞—Å—Ç—Ä–æ–π Service Discovery** –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è —Ü–µ–ª–µ–π:

**File-based SD** (`file_sd.json`):
```json
[
  {
    "targets": ["node-exporter:9100"],
    "labels": {
      "job": "node",
      "env": "production"
    }
  },
  {
    "targets": ["cadvisor:8080"],
    "labels": {
      "job": "containers",
      "env": "production"
    }
  }
]
```

–î–æ–±–∞–≤—å –≤ `prometheus.yml`:
```yaml
scrape_configs:
  - job_name: 'dynamic-targets'
    file_sd_configs:
      - files:
        - '/etc/prometheus/file_sd.json'
        refresh_interval: 30s
```

**–ù–∞—Å—Ç—Ä–æ–π Pushgateway** –¥–ª—è –º–µ—Ç—Ä–∏–∫ batch jobs:
```bash
docker run -d \
  --name pushgateway \
  -p 9091:9091 \
  prom/pushgateway

# Push –º–µ—Ç—Ä–∏–∫—É
echo "backup_duration_seconds 125.5" | curl --data-binary @- http://localhost:9091/metrics/job/backup/instance/db1

# –î–æ–±–∞–≤—å –≤ prometheus.yml
scrape_configs:
  - job_name: 'pushgateway'
    static_configs:
      - targets: ['pushgateway:9091']
    honor_labels: true
```

**Recording rules –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏**:
```yaml
# recording_rules.yml
groups:
  - name: performance_rules
    interval: 30s
    rules:
    # CPU utilization per instance
    - record: instance:node_cpu_utilization:rate5m
      expr: 100 - (avg by (instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)
    
    # Memory utilization per instance
    - record: instance:node_memory_utilization:ratio
      expr: 1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)
    
    # Request rate per job
    - record: job:http_requests:rate5m
      expr: sum(rate(http_requests_total[5m])) by (job)
```

---

## –ú–æ–¥—É–ª—å 3: Grafana - –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö (30 –º–∏–Ω—É—Ç)

### üéØ –ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞

**–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ Grafana:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Data     ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ Grafana  ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ Users    ‚îÇ
‚îÇ Sources  ‚îÇ      ‚îÇ Server   ‚îÇ      ‚îÇ          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
   ‚îÇ                    ‚îÇ
   ‚îÇ                    ‚îÇ
   ‚ñº                    ‚ñº
Prometheus       Dashboards
InfluxDB         Alerts
Elasticsearch    Users
Loki             Teams
```

**–¢–∏–ø—ã –ø–∞–Ω–µ–ª–µ–π:**
```
Graph        - –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä—è–¥—ã
Stat         - –û–¥–Ω–æ –∑–Ω–∞—á–µ–Ω–∏–µ
Gauge        - –®–∫–∞–ª–∞
Bar Gauge    - –ì–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω—ã–µ –ø–æ–ª–æ—Å–∫–∏
Table        - –¢–∞–±–ª–∏—Ü–∞
Heatmap      - –¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞
Logs         - –õ–æ–≥–∏
```

**–ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–∞—à–±–æ—Ä–¥–∞:**
```
Query      - –ò–∑ –¥–∞–Ω–Ω—ã—Ö (label_values(metric, label))
Custom     - –°–ø–∏—Å–æ–∫ –∑–Ω–∞—á–µ–Ω–∏–π
Constant   - –ö–æ–Ω—Å—Ç–∞–Ω—Ç–∞
Interval   - –í—Ä–µ–º–µ–Ω–Ω–æ–π –∏–Ω—Ç–µ—Ä–≤–∞–ª
Data source - –í—ã–±–æ—Ä –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö
```

**–ü–æ–ª–µ–∑–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ Grafana:**
```
$__interval        - –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –∏–Ω—Ç–µ—Ä–≤–∞–ª
$__rate_interval   - –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª –¥–ª—è rate()
$timeFilter        - –í—Ä–µ–º–µ–Ω–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä
$__from / $__to    - –ù–∞—á–∞–ª–æ/–∫–æ–Ω–µ—Ü –ø–µ—Ä–∏–æ–¥–∞

# –ü—Ä–∏–º–µ—Ä —Å –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
rate(http_requests_total{job="$job"}[$__rate_interval])
```

**Templating examples:**
```promql
# –ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è instance
label_values(node_cpu_seconds_total, instance)

# –ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è job
label_values(up, job)

# –ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è mountpoint
label_values(node_filesystem_size_bytes, mountpoint)

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ –∑–∞–ø—Ä–æ—Å–µ
node_filesystem_avail_bytes{instance="$instance", mountpoint="$mountpoint"}
```

**Alert channels:**
```
Email
Slack
PagerDuty
Webhook
Telegram
Discord
Teams
OpsGenie
```

**Dashboard best practices:**
```
1. –ò—Å–ø–æ–ª—å–∑—É–π Row –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ –ø–∞–Ω–µ–ª–µ–π
2. –î–æ–±–∞–≤–ª—è–π –æ–ø–∏—Å–∞–Ω–∏—è –∫ –ø–∞–Ω–µ–ª—è–º
3. –ò—Å–ø–æ–ª—å–∑—É–π –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –≥–∏–±–∫–æ—Å—Ç–∏
4. –£–∫–∞–∑—ã–≤–∞–π –µ–¥–∏–Ω–∏—Ü—ã –∏–∑–º–µ—Ä–µ–Ω–∏—è
5. –ò—Å–ø–æ–ª—å–∑—É–π —Ü–≤–µ—Ç–æ–≤—ã–µ –ø–æ—Ä–æ–≥–∏
6. –î–æ–±–∞–≤–ª—è–π —Å—Å—ã–ª–∫–∏ –Ω–∞ runbook'–∏
7. –ì—Ä—É–ø–ø–∏—Ä—É–π —Å–≤—è–∑–∞–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
8. –ò—Å–ø–æ–ª—å–∑—É–π consistent naming
```

### üíª –ó–∞–¥–∞–Ω–∏–µ

–ù–∞—Å—Ç—Ä–æ–π Grafana –∏ —Å–æ–∑–¥–∞–π dashboard:

1. **–î–æ–±–∞–≤—å Grafana –≤ docker-compose.yml**:
```yaml
  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - grafana-data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning
    restart: unless-stopped
    depends_on:
      - prometheus

volumes:
  grafana-data:
```

2. **–°–æ–∑–¥–∞–π provisioning –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏** (`grafana/provisioning/datasources/prometheus.yml`):
```yaml
apiVersion: 1

datasources:
  - name: Prometheus
    type: prometheus
    access: proxy
    url: http://prometheus:9090
    isDefault: true
    editable: true
```

3. **–°–æ–∑–¥–∞–π provisioning –¥–ª—è dashboard** (`grafana/provisioning/dashboards/dashboard.yml`):
```yaml
apiVersion: 1

providers:
  - name: 'Default'
    orgId: 1
    folder: ''
    type: file
    disableDeletion: false
    updateIntervalSeconds: 10
    allowUiUpdates: true
    options:
      path: /etc/grafana/provisioning/dashboards
```

4. **–ó–∞–ø—É—Å—Ç–∏ Grafana**:
```bash
# –°–æ–∑–¥–∞–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
mkdir -p grafana/provisioning/datasources
mkdir -p grafana/provisioning/dashboards

docker-compose up -d grafana

# –û—Ç–∫—Ä–æ–π –≤ –±—Ä–∞—É–∑–µ—Ä–µ
http://localhost:3000
# Login: admin
# Password: admin
```

5. **–°–æ–∑–¥–∞–π System Monitoring Dashboard** –≤—Ä—É—á–Ω—É—é:

**Panel 1: CPU Usage**
- Visualization: Time series
- Query: `100 - (avg(irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)`
- Legend: CPU Usage %
- Unit: Percent (0-100)
- Threshold: Yellow at 70, Red at 90

**Panel 2: Memory Usage**
- Visualization: Time series
- Query: `(1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100`
- Legend: Memory Usage %
- Unit: Percent (0-100)

**Panel 3: Disk Usage**
- Visualization: Gauge
- Query: `100 - ((node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"}) * 100)`
- Unit: Percent (0-100)
- Threshold: Green 0-70, Yellow 70-85, Red 85-100

**Panel 4: Network Traffic**
- Visualization: Time series
- Query A: `rate(node_network_receive_bytes_total[5m]) / 1024 / 1024`
- Query B: `rate(node_network_transmit_bytes_total[5m]) / 1024 / 1024`
- Unit: MB/s

**Panel 5: Top Processes by CPU**
- Visualization: Table
- Query: `topk(5, irate(process_cpu_seconds_total[5m]))`

6. **–°–æ–∑–¥–∞–π –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è dashboard**:
- Variable: instance
  - Type: Query
  - Query: `label_values(node_cpu_seconds_total, instance)`
  
–ò–∑–º–µ–Ω–∏ –∑–∞–ø—Ä–æ—Å—ã –Ω–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π:
```promql
100 - (avg(irate(node_cpu_seconds_total{instance="$instance", mode="idle"}[5m])) * 100)
```

### üöÄ –ë–æ–Ω—É—Å (–Ω–æ–≤–æ–µ)

**–°–æ–∑–¥–∞–π JSON dashboard —á–µ—Ä–µ–∑ provisioning** (`grafana/provisioning/dashboards/system-overview.json`):
```json
{
  "dashboard": {
    "title": "System Overview",
    "tags": ["system", "monitoring"],
    "timezone": "browser",
    "panels": [
      {
        "id": 1,
        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 0},
        "type": "timeseries",
        "title": "CPU Usage",
        "targets": [
          {
            "expr": "100 - (avg(irate(node_cpu_seconds_total{instance=\"$instance\",mode=\"idle\"}[5m])) * 100)",
            "legendFormat": "CPU Usage %"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "percent",
            "thresholds": {
              "steps": [
                {"value": 0, "color": "green"},
                {"value": 70, "color": "yellow"},
                {"value": 90, "color": "red"}
              ]
            }
          }
        }
      }
    ],
    "templating": {
      "list": [
        {
          "name": "instance",
          "type": "query",
          "datasource": "Prometheus",
          "query": "label_values(node_cpu_seconds_total, instance)",
          "refresh": 1
        }
      ]
    }
  }
}
```

**–ù–∞—Å—Ç—Ä–æ–π Alerting –≤ Grafana**:
1. Configuration ‚Üí Alerting ‚Üí Contact points
2. –°–æ–∑–¥–∞–π Email contact point
3. –°–æ–∑–¥–∞–π Alert rule:
   - Name: High CPU Alert
   - Query: `avg(irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100 < 20`
   - Condition: WHEN last() OF query(A) IS BELOW 20
   - For: 5m

**–£—Å—Ç–∞–Ω–æ–≤–∏ Grafana plugins**:
```bash
# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ —á–µ—Ä–µ–∑ UI
Configuration ‚Üí Plugins ‚Üí Search

# –ü–æ–ª–µ–∑–Ω—ã–µ –ø–ª–∞–≥–∏–Ω—ã:
- Pie Chart
- Worldmap Panel
- Clock Panel
- Status Panel

# –ß–µ—Ä–µ–∑ CLI (–≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–µ)
docker exec grafana grafana-cli plugins install grafana-piechart-panel
docker restart grafana
```

---

## –ú–æ–¥—É–ª—å 4: –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–∞—Ü–∏—è –ª–æ–≥–æ–≤ (30 –º–∏–Ω—É—Ç)

### üéØ –ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞

**–£—Ä–æ–≤–Ω–∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è:**

```
TRACE   - –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
DEBUG   - –û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
INFO    - –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
WARN    - –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
ERROR   - –û—à–∏–±–∫–∏, –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω—ã–µ –¥–ª—è —Ä–∞–±–æ—Ç—ã
FATAL   - –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏, –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –ø–∞–¥–∞–µ—Ç
```

**Structured logging (JSON):**

json

````json
{
  "timestamp": "2025-01-15T10:30:00Z",
  "level": "ERROR",
  "service": "api",
  "message": "Database connection failed",
  "error": "connection timeout",
  "user_id": "12345",
  "request_id": "abc-123",
  "duration_ms": 5000
}
````

**ELK Stack:**
```
Elasticsearch  - –•—Ä–∞–Ω–µ–Ω–∏–µ –∏ –ø–æ–∏—Å–∫
Logstash       - –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –ø–∞—Ä—Å–∏–Ω–≥
Kibana         - –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
```

**Alternative: Loki Stack:**
```
Loki           - –•—Ä–∞–Ω–µ–Ω–∏–µ –ª–æ–≥–æ–≤ (–∫–∞–∫ Prometheus –¥–ª—è –ª–æ–≥–æ–≤)
Promtail       - –ê–≥–µ–Ω—Ç —Å–±–æ—Ä–∞ (–∫–∞–∫ node-exporter)
Grafana        - –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
```

**Log aggregation patterns:**
````
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   App    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
                ‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îú‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ Log     ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ Centralized  ‚îÇ
‚îÇ   App    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚î§    ‚îÇ Shipper ‚îÇ    ‚îÇ Log Storage  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ   App    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
````

**–ü–æ–ª–µ–∑–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã –¥–ª—è –ª–æ–≥–æ–≤:**

bash

```bash
# journalctl (systemd)
journalctl -u nginx                  # –õ–æ–≥–∏ —Å–µ—Ä–≤–∏—Å–∞
journalctl -f                        # Follow –ª–æ–≥–∏
journalctl --since "1 hour ago"
journalctl -p err                    # –¢–æ–ª—å–∫–æ –æ—à–∏–±–∫–∏
journalctl --disk-usage              # –†–∞–∑–º–µ—Ä –ª–æ–≥–æ–≤

# Docker logs
docker logs <container>
docker logs -f <container>
docker logs --tail 100 <container>
docker logs --since 1h <container>

# –¢—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–µ –ª–æ–≥–∏ Linux
tail -f /var/log/syslog
tail -f /var/log/nginx/access.log
grep "ERROR" /var/log/application.log
zgrep "pattern" /var/log/old.log.gz  # –ü–æ–∏—Å–∫ –≤ —Å–∂–∞—Ç—ã—Ö –ª–æ–≥–∞—Ö

# –õ–æ–≥–∏ —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏
tail -f /var/log/app.log | ts '%Y-%m-%d %H:%M:%S'

# –ú–Ω–æ–≥–æ—Ñ–∞–π–ª–æ–≤—ã–π tail
multitail /var/log/nginx/access.log /var/log/nginx/error.log

# –ê–Ω–∞–ª–∏–∑ –ª–æ–≥–æ–≤
awk '{print $1}' access.log | sort | uniq -c | sort -rn | head -10  # Top 10 IP
grep "500" access.log | wc -l  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ 500 –æ—à–∏–±–æ–∫
```

**Log rotation:**

bash

```bash
# logrotate –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (/etc/logrotate.d/app)
/var/log/app/*.log {
    daily                # –†–æ—Ç–∞—Ü–∏—è –∫–∞–∂–¥—ã–π –¥–µ–Ω—å
    rotate 7             # –•—Ä–∞–Ω–∏—Ç—å 7 –∞—Ä—Ö–∏–≤–æ–≤
    compress             # –°–∂–∏–º–∞—Ç—å —Å—Ç–∞—Ä—ã–µ
    delaycompress        # –ù–µ —Å–∂–∏–º–∞—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–π
    missingok            # –ù–µ –æ—à–∏–±–∞—Ç—å—Å—è –µ—Å–ª–∏ —Ñ–∞–π–ª–∞ –Ω–µ—Ç
    notifempty           # –ù–µ —Ä–æ—Ç–∏—Ä–æ–≤–∞—Ç—å –ø—É—Å—Ç—ã–µ
    create 0640 app app  # –°–æ–∑–¥–∞—Ç—å —Å –ø—Ä–∞–≤–∞–º–∏
    sharedscripts
    postrotate
        systemctl reload app > /dev/null
    endscript
}

# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
logrotate -d /etc/logrotate.d/app    # Dry run
logrotate -f /etc/logrotate.d/app    # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —Ä–æ—Ç–∞—Ü–∏—è
```

**–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è—Ö:**

**Python (structured logging):**

python

```python
import logging
import json
from datetime import datetime

class JSONFormatter(logging.Formatter):
    def format(self, record):
        log_data = {
            "timestamp": datetime.utcnow().isoformat(),
            "level": record.levelname,
            "service": "my-api",
            "message": record.getMessage(),
            "module": record.module,
            "function": record.funcName,
            "line": record.lineno
        }
        if record.exc_info:
            log_data["exception"] = self.formatException(record.exc_info)
        return json.dumps(log_data)

logging.basicConfig(level=logging.INFO)
handler = logging.StreamHandler()
handler.setFormatter(JSONFormatter())
logger = logging.getLogger()
logger.handlers = [handler]

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
logger.info("User logged in", extra={"user_id": "123", "ip": "192.168.1.1"})
logger.error("Database error", extra={"query": "SELECT *", "duration_ms": 5000})
```

**Node.js (Winston):**

javascript

```javascript
const winston = require('winston');

const logger = winston.createLogger({
  level: 'info',
  format: winston.format.combine(
    winston.format.timestamp(),
    winston.format.json()
  ),
  defaultMeta: { service: 'api-service' },
  transports: [
    new winston.transports.File({ filename: 'error.log', level: 'error' }),
    new winston.transports.File({ filename: 'combined.log' }),
    new winston.transports.Console({
      format: winston.format.simple()
    })
  ]
});

// –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
logger.info('User action', { user_id: '123', action: 'login' });
logger.error('Database error', { error: err.message, query: sql });
```

**Loki query patterns (LogQL):**

logql

```logql
# –ë–∞–∑–æ–≤—ã–π –ø–æ–∏—Å–∫
{job="varlogs"}

# –§–∏–ª—å—Ç—Ä—ã
{job="varlogs"} |= "error"                    # –°–æ–¥–µ—Ä–∂–∏—Ç "error"
{job="varlogs"} != "debug"                    # –ù–µ —Å–æ–¥–µ—Ä–∂–∏—Ç "debug"
{job="varlogs"} |~ "error|ERROR"              # Regex
{job="varlogs"} !~ "info|INFO"                # –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–π regex

# JSON parsing
{job="varlogs"} | json | level="error"
{job="varlogs"} | json | response_time > 1000

# –ê–≥—Ä–µ–≥–∞—Ü–∏—è
rate({job="varlogs"}[5m])                     # –õ–æ–≥-–∑–∞–ø–∏—Å–µ–π –≤ —Å–µ–∫—É–Ω–¥—É
sum(rate({job="varlogs"}[5m])) by (level)     # –ü–æ —É—Ä–æ–≤–Ω—é
count_over_time({job="varlogs"}[1h])          # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞ —á–∞—Å

# Pattern extraction
{job="varlogs"} | pattern `<_> level=<level> <_>`
{job="varlogs"} | regexp `status=(?P<status>\d+)`

# –ú–µ—Ç—Ä–∏–∫–∏ –∏–∑ –ª–æ–≥–æ–≤
sum(rate({job="api"} | json | status="500" [5m]))
```

**Elasticsearch query patterns:**

json

```json
// –ë–∞–∑–æ–≤—ã–π –ø–æ–∏—Å–∫
GET /logs-*/_search
{
  "query": {
    "match": {
      "message": "error"
    }
  }
}

// –í—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏–∞–ø–∞–∑–æ–Ω
GET /logs-*/_search
{
  "query": {
    "range": {
      "@timestamp": {
        "gte": "now-1h",
        "lte": "now"
      }
    }
  }
}

// –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å
GET /logs-*/_search
{
  "query": {
    "bool": {
      "must": [
        { "match": { "level": "ERROR" }},
        { "match": { "service": "api" }}
      ],
      "filter": [
        { "range": { "@timestamp": { "gte": "now-1h" }}}
      ]
    }
  },
  "aggs": {
    "errors_by_service": {
      "terms": { "field": "service.keyword" }
    }
  }
}
```

**Fluentd/Fluent Bit basics:**

conf

````conf
# Fluentd –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (fluent.conf)
<source>
  @type tail
  path /var/log/nginx/access.log
  pos_file /var/log/td-agent/nginx-access.log.pos
  tag nginx.access
  <parse>
    @type nginx
  </parse>
</source>

<filter nginx.access>
  @type record_transformer
  <record>
    hostname "#{Socket.gethostname}"
    service "nginx"
  </record>
</filter>

<match nginx.access>
  @type elasticsearch
  host elasticsearch
  port 9200
  index_name nginx-access
  type_name _doc
</match>

# Fluent Bit –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (–±–æ–ª–µ–µ –ª–µ–≥–∫–æ–≤–µ—Å–Ω–∞—è –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞)
[INPUT]
    Name              tail
    Path              /var/log/containers/*.log
    Parser            docker
    Tag               kube.*

[FILTER]
    Name                kubernetes
    Match               kube.*
    Kube_URL            https://kubernetes.default.svc:443
    Kube_CA_File        /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
    Kube_Token_File     /var/run/secrets/kubernetes.io/serviceaccount/token

[OUTPUT]
    Name              loki
    Match             *
    Host              loki
    Port              3100
```

**Log best practices:**
```
1. –í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–π structured logging (JSON)
2. –í–∫–ª—é—á–∞–π –∫–æ–Ω—Ç–µ–∫—Å—Ç: request_id, user_id, trace_id
3. –õ–æ–≥–∏—Ä—É–π –Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —É—Ä–æ–≤–Ω–µ:
   - DEBUG: –¥–µ—Ç–∞–ª–∏ –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏
   - INFO: –Ω–æ—Ä–º–∞–ª—å–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏
   - WARN: –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã
   - ERROR: –æ—à–∏–±–∫–∏ —Ç—Ä–µ–±—É—é—â–∏–µ –≤–Ω–∏–º–∞–Ω–∏—è
4. –ù–µ –ª–æ–≥–∏—Ä—É–π sensitive data (–ø–∞—Ä–æ–ª–∏, —Ç–æ–∫–µ–Ω—ã, PII)
5. –ò—Å–ø–æ–ª—å–∑—É–π correlation IDs –¥–ª—è —Ç—Ä–µ–π—Å–∏–Ω–≥–∞
6. –†–æ—Ç–∏—Ä—É–π –ª–æ–≥–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
7. –¶–µ–Ω—Ç—Ä–∞–ª–∏–∑—É–π –ª–æ–≥–∏ —Å–æ –≤—Å–µ—Ö —Å–∏—Å—Ç–µ–º
8. –ù–∞—Å—Ç—Ä–æ–π –∞–ª–µ—Ä—Ç—ã –Ω–∞ –∫—Ä–∏—Ç–∏—á–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
````

### üíª –ó–∞–¥–∞–Ω–∏–µ

–ù–∞—Å—Ç—Ä–æ–π —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å Loki:

1. **–°–æ–∑–¥–∞–π docker-compose.yml –¥–ª—è Loki stack**:

yaml

```yaml
version: '3.8'

services:
  loki:
    image: grafana/loki:2.9.3
    container_name: loki
    ports:
      - "3100:3100"
    volumes:
      - ./loki-config.yml:/etc/loki/local-config.yaml
      - loki-data:/loki
    command: -config.file=/etc/loki/local-config.yaml
    restart: unless-stopped

  promtail:
    image: grafana/promtail:2.9.3
    container_name: promtail
    volumes:
      - ./promtail-config.yml:/etc/promtail/config.yml
      - /var/log:/var/log:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
    command: -config.file=/etc/promtail/config.yml
    restart: unless-stopped
    depends_on:
      - loki

  grafana:
    image: grafana/grafana:10.2.3
    container_name: grafana-logs
    ports:
      - "3001:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - grafana-logs-data:/var/lib/grafana
      - ./grafana-datasources.yml:/etc/grafana/provisioning/datasources/datasources.yml
    restart: unless-stopped
    depends_on:
      - loki

  # –¢–µ—Å—Ç–æ–≤–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∏—Ä—É—é—â–µ–µ –ª–æ–≥–∏
  log-generator:
    image: mingrammer/flog
    container_name: log-generator
    command: -f json -l -d 1 -s 1
    restart: unless-stopped

volumes:
  loki-data:
  grafana-logs-data:
```

2. **–°–æ–∑–¥–∞–π loki-config.yml**:

yaml

```yaml
auth_enabled: false

server:
  http_listen_port: 3100
  grpc_listen_port: 9096

common:
  path_prefix: /loki
  storage:
    filesystem:
      chunks_directory: /loki/chunks
      rules_directory: /loki/rules
  replication_factor: 1
  ring:
    instance_addr: 127.0.0.1
    kvstore:
      store: inmemory

query_range:
  results_cache:
    cache:
      embedded_cache:
        enabled: true
        max_size_mb: 100

schema_config:
  configs:
    - from: 2020-10-24
      store: boltdb-shipper
      object_store: filesystem
      schema: v11
      index:
        prefix: index_
        period: 24h

ruler:
  alertmanager_url: http://localhost:9093

# Retention (—É–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö –ª–æ–≥–æ–≤)
limits_config:
  retention_period: 168h  # 7 –¥–Ω–µ–π
```

3. **–°–æ–∑–¥–∞–π promtail-config.yml**:

yaml

```yaml
server:
  http_listen_port: 9080
  grpc_listen_port: 0

positions:
  filename: /tmp/positions.yaml

clients:
  - url: http://loki:3100/loki/api/v1/push

scrape_configs:
  # Docker –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã
  - job_name: docker
    docker_sd_configs:
      - host: unix:///var/run/docker.sock
        refresh_interval: 5s
    relabel_configs:
      - source_labels: ['__meta_docker_container_name']
        regex: '/(.*)'
        target_label: 'container'
      - source_labels: ['__meta_docker_container_log_stream']
        target_label: 'stream'
    pipeline_stages:
      - json:
          expressions:
            level: level
            message: message
            timestamp: timestamp
      - labels:
          level:
          stream:

  # –°–∏—Å—Ç–µ–º–Ω—ã–µ –ª–æ–≥–∏
  - job_name: system
    static_configs:
      - targets:
          - localhost
        labels:
          job: varlogs
          __path__: /var/log/*.log

  # Application logs (—Å –ø–∞—Ä—Å–∏–Ω–≥–æ–º JSON)
  - job_name: app-logs
    static_configs:
      - targets:
          - localhost
        labels:
          job: app
          __path__: /var/log/app/*.log
    pipeline_stages:
      - json:
          expressions:
            timestamp: timestamp
            level: level
            service: service
            message: message
            user_id: user_id
      - timestamp:
          source: timestamp
          format: RFC3339
      - labels:
          level:
          service:
```

4. **–°–æ–∑–¥–∞–π grafana-datasources.yml**:

yaml

```yaml
apiVersion: 1

datasources:
  - name: Loki
    type: loki
    access: proxy
    url: http://loki:3100
    isDefault: true
    editable: true
    jsonData:
      maxLines: 1000
```

5. **–ó–∞–ø—É—Å—Ç–∏ stack**:

bash

```bash
# –°–æ–∑–¥–∞–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
mkdir -p logs/app

# –ó–∞–ø—É—Å—Ç–∏
docker-compose up -d

# –ü—Ä–æ–≤–µ—Ä—å —Å—Ç–∞—Ç—É—Å
docker-compose ps
curl http://localhost:3100/ready

# –ü—Ä–æ–≤–µ—Ä—å –ª–æ–≥–∏
curl http://localhost:3100/loki/api/v1/label
```

6. **–°–æ–∑–¥–∞–π Python —Å–∫—Ä–∏–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ—Å—Ç–æ–≤—ã—Ö –ª–æ–≥–æ–≤** (`generate_logs.py`):

python

````python
#!/usr/bin/env python3
import json
import random
import time
from datetime import datetime

levels = ['DEBUG', 'INFO', 'WARN', 'ERROR']
services = ['api', 'frontend', 'database', 'cache']
messages = {
    'DEBUG': ['Query executed', 'Cache hit', 'Function called'],
    'INFO': ['User logged in', 'Request processed', 'Task completed'],
    'WARN': ['Slow query detected', 'High memory usage', 'Rate limit approaching'],
    'ERROR': ['Database connection failed', 'Timeout occurred', '500 Internal Server Error']
}

def generate_log():
    level = random.choices(levels, weights=[10, 60, 20, 10])[0]
    service = random.choice(services)
    message = random.choice(messages[level])
    
    log_entry = {
        "timestamp": datetime.utcnow().isoformat() + "Z",
        "level": level,
        "service": service,
        "message": message,
        "request_id": f"req-{random.randint(1000, 9999)}",
        "user_id": f"user-{random.randint(1, 100)}",
        "duration_ms": random.randint(10, 5000) if level in ['WARN', 'ERROR'] else random.randint(10, 500)
    }
    
    return json.dumps(log_entry)

if __name__ == "__main__":
    print("Starting log generation...")
    while True:
        log = generate_log()
        print(log)
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Ñ–∞–π–ª
        with open('/var/log/app/application.log', 'a') as f:
            f.write(log + '\n')
        time.sleep(random.uniform(0.1, 2))
````

7. **–û—Ç–∫—Ä–æ–π Grafana –∏ —Å–æ–∑–¥–∞–π dashboard**:
```
URL: http://localhost:3001
Login: admin
Password: admin

–ü—Ä–∏–º–µ—Ä—ã –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –ø–∞–Ω–µ–ª–µ–π:

# –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–æ–≥–æ–≤ –ø–æ —É—Ä–æ–≤–Ω—é
sum(rate({job="docker"}[1m])) by (level)

# –õ–æ–≥–∏ —Å –æ—à–∏–±–∫–∞–º–∏
{job="docker"} |= "ERROR"

# Top services –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –ª–æ–≥–æ–≤
topk(5, sum(rate({job="docker"}[5m])) by (container))

# –õ–æ–≥–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Å–µ—Ä–≤–∏—Å–∞
{job="docker", container="log-generator"}

# –ú–µ–¥–ª–µ–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã (–µ—Å–ª–∏ duration > 1000ms)
{job="docker"} | json | duration_ms > 1000
````

8. **–ü—Ä–æ–≤–µ—Ä—å —Ä–∞–±–æ—Ç—É**:

bash

```bash
# –õ–æ–≥–∏ –≤ Loki
curl -G -s "http://localhost:3100/loki/api/v1/query" \
  --data-urlencode 'query={job="docker"}' | jq

# –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–æ–≥–æ–≤
curl -G -s "http://localhost:3100/loki/api/v1/query" \
  --data-urlencode 'query=count_over_time({job="docker"}[1h])' | jq

# –ú–µ—Ç—Ä–∏–∫–∏ Promtail
curl http://localhost:9080/metrics
```

### üöÄ –ë–æ–Ω—É—Å (–Ω–æ–≤–æ–µ)

**1. –ù–∞—Å—Ç—Ä–æ–π ELK Stack –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è**:

`docker-compose-elk.yml`:

yaml

```yaml
version: '3.8'

services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.3
    container_name: elasticsearch
    environment:
      - discovery.type=single-node
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
      - xpack.security.enabled=false
    ports:
      - "9200:9200"
    volumes:
      - elasticsearch-data:/usr/share/elasticsearch/data
    restart: unless-stopped

  logstash:
    image: docker.elastic.co/logstash/logstash:8.11.3
    container_name: logstash
    volumes:
      - ./logstash.conf:/usr/share/logstash/pipeline/logstash.conf
    ports:
      - "5000:5000"
      - "9600:9600"
    environment:
      - "LS_JAVA_OPTS=-Xmx256m -Xms256m"
    depends_on:
      - elasticsearch
    restart: unless-stopped

  kibana:
    image: docker.elastic.co/kibana/kibana:8.11.3
    container_name: kibana
    ports:
      - "5601:5601"
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    depends_on:
      - elasticsearch
    restart: unless-stopped

  filebeat:
    image: docker.elastic.co/beats/filebeat:8.11.3
    container_name: filebeat
    user: root
    volumes:
      - ./filebeat.yml:/usr/share/filebeat/filebeat.yml:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
    command: filebeat -e -strict.perms=false
    depends_on:
      - elasticsearch
    restart: unless-stopped

volumes:
  elasticsearch-data:
```

`logstash.conf`:

conf

```conf
input {
  beats {
    port => 5000
  }
}

filter {
  if [message] =~ /^\{.*\}$/ {
    json {
      source => "message"
    }
  }
  
  date {
    match => ["timestamp", "ISO8601"]
    target => "@timestamp"
  }
  
  mutate {
    remove_field => ["message"]
  }
}

output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "logs-%{+YYYY.MM.dd}"
  }
  
  stdout {
    codec => rubydebug
  }
}
```

`filebeat.yml`:

yaml

```yaml
filebeat.inputs:
  - type: container
    paths:
      - '/var/lib/docker/containers/*/*.log'
    processors:
      - add_docker_metadata:
          host: "unix:///var/run/docker.sock"

output.logstash:
  hosts: ["logstash:5000"]

logging.level: info
```

**2. –°–æ–∑–¥–∞–π log alerting rules**:

–î–ª—è Loki (—á–µ—Ä–µ–∑ Grafana Alerting):

yaml

```yaml
# Alert: High Error Rate
groups:
  - name: log_alerts
    interval: 1m
    rules:
      - alert: HighErrorRate
        expr: |
          sum(rate({job="docker"} |= "ERROR" [5m])) > 10
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value }} errors/sec"

      - alert: ServiceDown
        expr: |
          absent(rate({job="docker", container="api"}[5m]))
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Service {{ $labels.container }} is down"
```

**3. –ù–∞—Å—Ç—Ä–æ–π log parsing –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤**:

Nginx access log parsing –≤ Promtail:

yaml

````yaml
- job_name: nginx
  static_configs:
    - targets:
        - localhost
      labels:
        job: nginx
        __path__: /var/log/nginx/access.log
  pipeline_stages:
    - regex:
        expression: '^(?P<remote_addr>[\w\.]+) - (?P<remote_user>[^ ]*) \[(?P<time_local>.*)\] "(?P<method>[^ ]*) (?P<request>[^ ]*) (?P<protocol>[^ ]*)" (?P<status>[\d]+) (?P<body_bytes_sent>[\d]+) "(?P<http_referer>[^"]*)" "(?P<http_user_agent>[^"]*)"'
    - labels:
        method:
        status:
    - timestamp:
        source: time_local
        format: 02/Jan/2006:15:04:05 -0700
````

**4. –°–æ–∑–¥–∞–π log analysis dashboard**:

Grafana panels –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ª–æ–≥–æ–≤:
````
Panel 1: Log volume over time
Query: sum(rate({job="docker"}[1m])) by (level)
Visualization: Time series

Panel 2: Top error messages
Query: topk(10, sum(rate({job="docker"} |= "ERROR" [5m])) by (message))
Visualization: Bar chart

Panel 3: Logs table
Query: {job="docker"}
Visualization: Logs

Panel 4: Response time distribution
Query: quantile_over_time(0.95, {job="docker"} | json | unwrap duration_ms [5m])
Visualization: Gauge

Panel 5: Service health
Query: count(rate({job="docker"}[1m])) by (container)
Visualization: Stat
````

**5. –ù–∞—Å—Ç—Ä–æ–π log sampling –¥–ª—è –≤—ã—Å–æ–∫–æ–Ω–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —Å–∏—Å—Ç–µ–º**:

yaml

```yaml
# Promtail sampling configuration
scrape_configs:
  - job_name: high-volume-app
    static_configs:
      - targets:
          - localhost
        labels:
          job: app
          __path__: /var/log/app/*.log
    pipeline_stages:
      # –°–æ—Ö—Ä–∞–Ω—è–π —Ç–æ–ª—å–∫–æ ERROR –∏ WARN + sample INFO/DEBUG
      - match:
          selector: '{job="app"}'
          stages:
            - json:
                expressions:
                  level: level
            - drop:
                expression: "level == 'DEBUG' and __sample__ > 0.1"  # 10% DEBUG
            - drop:
                expression: "level == 'INFO' and __sample__ > 0.5"   # 50% INFO
```

**6. Log retention –∏ archiving**:

yaml

```yaml
# Loki retention config
limits_config:
  retention_period: 168h  # 7 –¥–Ω–µ–π

# Compactor –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ —Å—Ç–∞—Ä—ã—Ö –ª–æ–≥–æ–≤
compactor:
  working_directory: /loki/compactor
  shared_store: filesystem
  compaction_interval: 10m
  retention_enabled: true
  retention_delete_delay: 2h
  retention_delete_worker_count: 150
```

**7. –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Alertmanager**:

yaml

```yaml
# Loki ruler config –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –∞–ª–µ—Ä—Ç–æ–≤
ruler:
  storage:
    type: local
    local:
      directory: /loki/rules
  rule_path: /tmp/rules
  alertmanager_url: http://alertmanager:9093
  ring:
    kvstore:
      store: inmemory
  enable_api: true
```

Rules file (`/loki/rules/alerts.yml`):

yaml

````yaml
groups:
  - name: logs
    interval: 1m
    rules:
      - alert: HighErrorRate
        expr: |
          sum(rate({job="docker"} |= "ERROR" [5m])) > 1
        for: 5m
        labels:
          severity: warning
          team: backend
        annotations:
          summary: "High error rate in {{ $labels.container }}"
          description: "Error rate: {{ $value }} errors/sec"
          dashboard: "http://grafana:3000/d/logs"
````

**8. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ Loki vs ELK**:
```
Loki –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:
‚úÖ –õ–µ–≥–∫–æ–≤–µ—Å–Ω—ã–π (–º–µ–Ω—å—à–µ —Ä–µ—Å—É—Ä—Å–æ–≤)
‚úÖ –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Prometheus/Grafana
‚úÖ –ü—Ä–æ—Å—Ç–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
‚úÖ –•–æ—Ä–æ—à–æ –¥–ª—è Kubernetes
‚úÖ –î–µ—à–µ–≤–ª–µ –≤ —ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏–∏

ELK –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:
‚úÖ –ú–æ—â–Ω—ã–π –ø–æ–ª–Ω–æ—Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫
‚úÖ –ë–æ–≥–∞—Ç—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
‚úÖ Advanced analytics
‚úÖ –ë–æ–ª—å—à–µ –ø–ª–∞–≥–∏–Ω–æ–≤ –∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–π
‚úÖ Mature ecosystem

–í—ã–±–æ—Ä:
- Loki: –¥–ª—è –º–µ—Ç—Ä–∏–∫-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞, K8s
- ELK: –¥–ª—è —Å–ª–æ–∂–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –ª–æ–≥–æ–≤, compliance
````

---

## –ò—Ç–æ–≥–∏ –º–æ–¥—É–ª—è 4

–ü–æ—Å–ª–µ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏—è —ç—Ç–æ–≥–æ –º–æ–¥—É–ª—è —Ç—ã –¥–æ–ª–∂–µ–Ω —É–º–µ—Ç—å:

‚úÖ –ü–æ–Ω–∏–º–∞—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã –∫ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—é 
‚úÖ –ù–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å Loki + Promtail + Grafana
‚úÖ –ü–∏—Å–∞—Ç—å LogQL –∑–∞–ø—Ä–æ—Å—ã 
‚úÖ –ü–∞—Ä—Å–∏—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –ª–æ–≥–æ–≤ 
‚úÖ –°–æ–∑–¥–∞–≤–∞—Ç—å –¥–∞—à–±–æ—Ä–¥—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ª–æ–≥–æ–≤ 
‚úÖ –ù–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å –∞–ª–µ—Ä—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –ª–æ–≥–æ–≤ 
‚úÖ –£–ø—Ä–∞–≤–ª—è—Ç—å retention –∏ rotation 
‚úÖ –°—Ä–∞–≤–Ω–∏–≤–∞—Ç—å Loki –∏ ELK —Å—Ç–µ–∫–∏

**–°–ª–µ–¥—É—é—â–∏–π –º–æ–¥—É–ª—å:** Alerting –∏ Notification (–∫–∞–∫ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å —É–º–Ω—ã–µ –∞–ª–µ—Ä—Ç—ã –∏ –∏–∑–±–µ–∂–∞—Ç—å alert fatigue)


## –ú–æ–¥—É–ª—å 5: Alerting –∏ Notification - —É–º–Ω—ã–µ –∞–ª–µ—Ä—Ç—ã –±–µ–∑ alert fatigue (35 –º–∏–Ω—É—Ç)

### üéØ –ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞

**–§–∏–ª–æ—Å–æ—Ñ–∏—è –∞–ª–µ—Ä—Ç–∏–Ω–≥–∞:**

```
–•–æ—Ä–æ—à–∏–π –∞–ª–µ—Ä—Ç = Actionable + Urgent + Real Problem

‚ùå –ü–ª–æ—Ö–æ–π –∞–ª–µ—Ä—Ç: "CPU usage > 80%"
‚úÖ –•–æ—Ä–æ—à–∏–π –∞–ª–µ—Ä—Ç: "API response time > 1s for 5min, affecting users"

–ü—Ä–∞–≤–∏–ª–æ: –ï—Å–ª–∏ –∞–ª–µ—Ä—Ç –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –¥–µ–π—Å—Ç–≤–∏—è –ø—Ä—è–º–æ —Å–µ–π—á–∞—Å - —ç—Ç–æ –Ω–µ –∞–ª–µ—Ä—Ç, —ç—Ç–æ –º–µ—Ç—Ä–∏–∫–∞
```

**–£—Ä–æ–≤–Ω–∏ severity:**

```
CRITICAL (P1)  - –ü–æ–ª–Ω—ã–π outage, —Ç—Ä–µ–±—É–µ—Ç –Ω–µ–º–µ–¥–ª–µ–Ω–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π
                 –ü—Ä–∏–º–µ—Ä: —Å–µ—Ä–≤–∏—Å –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –ø–æ—Ç–µ—Ä—è –¥–∞–Ω–Ω—ã—Ö

WARNING (P2)   - –î–µ–≥—Ä–∞–¥–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–∞, —Ç—Ä–µ–±—É–µ—Ç –¥–µ–π—Å—Ç–≤–∏–π –≤ –±–ª–∏–∂–∞–π—à–µ–µ –≤—Ä–µ–º—è
                 –ü—Ä–∏–º–µ—Ä: –≤—ã—Å–æ–∫–∞—è latency, —Å–∫–æ—Ä–æ –∑–∞–∫–æ–Ω—á–∏—Ç—Å—è –º–µ—Å—Ç–æ

INFO (P3)      - –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–µ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ, –Ω–µ —Ç—Ä–µ–±—É–µ—Ç —Å—Ä–æ—á–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π
                 –ü—Ä–∏–º–µ—Ä: deployment –∑–∞–≤–µ—Ä—à–µ–Ω, –ø–ª–∞–Ω–æ–≤–æ–µ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ
```

**Alertmanager –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Prometheus  ‚îÇ‚îÄ‚îê
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
                ‚îú‚îÄ‚îÄ‚ñ∫ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ    ‚îÇ Alertmanager ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ Receivers   ‚îÇ
‚îÇ    Loki     ‚îÇ‚îÄ‚î§    ‚îÇ              ‚îÇ     ‚îÇ (Slack/etc) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ    ‚îÇ - Grouping   ‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚îÇ    ‚îÇ - Inhibition ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ    ‚îÇ - Silencing  ‚îÇ
‚îÇ   Custom    ‚îÇ‚îÄ‚îò    ‚îÇ - Routing    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Alert states:**

```
Inactive  ‚îÄ‚îÄ‚ñ∫ Pending  ‚îÄ‚îÄ‚ñ∫ Firing  ‚îÄ‚îÄ‚ñ∫ Resolved
               (for)         ‚îÇ
                            ‚Üì
                         Silenced
```

**–ö–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏:**

**1. Grouping** - –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø–æ—Ö–æ–∂–∏—Ö –∞–ª–µ—Ä—Ç–æ–≤:

yaml

```yaml
# –í–º–µ—Å—Ç–æ 100 –∞–ª–µ—Ä—Ç–æ–≤ –æ down –Ω–æ–¥–∞—Ö
# –û–¥–∏–Ω grouped –∞–ª–µ—Ä—Ç: "50 nodes are down in cluster-prod"
route:
  group_by: ['alertname', 'cluster']
  group_wait: 30s
  group_interval: 5m
```

**2. Inhibition** - –ø–æ–¥–∞–≤–ª–µ–Ω–∏–µ –∑–∞–≤–∏—Å–∏–º—ã—Ö –∞–ª–µ—Ä—Ç–æ–≤:

yaml

```yaml
# –ï—Å–ª–∏ –∫–ª–∞—Å—Ç–µ—Ä down, –Ω–µ —Å–ª–∞—Ç—å –∞–ª–µ—Ä—Ç—ã –æ –∫–∞–∂–¥–æ–º —Å–µ—Ä–≤–∏—Å–µ –≤ –Ω–µ–º
inhibit_rules:
  - source_match:
      alertname: ClusterDown
    target_match:
      cluster: production
    equal: ['cluster']
```

**3. Silencing** - –≤—Ä–µ–º–µ–Ω–Ω–æ–µ –æ—Ç–∫–ª—é—á–µ–Ω–∏–µ –∞–ª–µ—Ä—Ç–æ–≤:

bash

```bash
# –í–æ –≤—Ä–µ–º—è maintenance window
amtool silence add alertname=HighCPU --duration=2h --comment="Planned maintenance"
```

**4. Routing** - –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è –ø–æ –∫–æ–º–∞–Ω–¥–∞–º/–∫–∞–Ω–∞–ª–∞–º:

yaml

```yaml
route:
  routes:
    - match:
        team: backend
      receiver: backend-team
    - match:
        severity: critical
      receiver: pagerduty
```

**Prometheus alerting rules —Å—Ç—Ä—É–∫—Ç—É—Ä–∞:**

yaml

```yaml
groups:
  - name: example
    interval: 30s
    rules:
    - alert: HighErrorRate
      expr: |
        rate(http_requests_total{status=~"5.."}[5m]) 
        / 
        rate(http_requests_total[5m]) 
        > 0.05
      for: 5m
      labels:
        severity: warning
        team: backend
        service: api
      annotations:
        summary: "High error rate on {{ $labels.instance }}"
        description: "Error rate is {{ $value | humanizePercentage }}"
        dashboard: "https://grafana.com/d/api-dashboard"
        runbook: "https://wiki.com/runbooks/high-error-rate"
```

**Alert best practices:**

**1. –ù–∞–∑–≤–∞–Ω–∏–µ –∞–ª–µ—Ä—Ç–∞ (–≥–æ–≤–æ—Ä—è—â–µ–µ):**

yaml

```yaml
‚ùå alert: HighCPU
‚úÖ alert: InstanceHighCPUUsage

‚ùå alert: Error
‚úÖ alert: APIHighErrorRate5xx
```

**2. For clause (–∏–∑–±–µ–≥–∞–µ–º flapping):**

yaml

```yaml
# –ù–µ –∞–ª–µ—Ä—Ç–∏—Ç—å –Ω–∞ –∫—Ä–∞—Ç–∫–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Å–ø–∞–π–∫–∏
for: 5m  # –ê–ª–µ—Ä—Ç —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —É—Å–ª–æ–≤–∏–µ true 5 –º–∏–Ω—É—Ç –ø–æ–¥—Ä—è–¥
```

**3. –ê–Ω–Ω–æ—Ç–∞—Ü–∏–∏ (–ø–æ–ª–µ–∑–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç):**

yaml

```yaml
annotations:
  summary: "–ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º—ã"
  description: "{{ $labels.instance }} has {{ $value }}% CPU usage"
  dashboard: "–°—Å—ã–ª–∫–∞ –Ω–∞ dashboard"
  runbook: "–°—Å—ã–ª–∫–∞ –Ω–∞ runbook —Å —Ä–µ—à–µ–Ω–∏–µ–º"
  impact: "Users experiencing slow response times"
```

**4. Labels –¥–ª—è routing:**

yaml

```yaml
labels:
  severity: critical|warning|info
  team: backend|frontend|data
  service: api|web|worker
  environment: prod|staging|dev
```

**–¢–∏–ø–∏—á–Ω—ã–µ –∞–ª–µ—Ä—Ç—ã –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã:**

yaml

```yaml
# Instance down
- alert: InstanceDown
  expr: up == 0
  for: 5m
  labels:
    severity: critical
  annotations:
    summary: "Instance {{ $labels.instance }} down"

# High CPU
- alert: HighCPUUsage
  expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
  for: 10m
  labels:
    severity: warning

# High Memory
- alert: HighMemoryUsage
  expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 90
  for: 5m
  labels:
    severity: warning

# Disk space low
- alert: DiskSpaceLow
  expr: (1 - (node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"})) * 100 > 85
  for: 5m
  labels:
    severity: warning

# High disk I/O
- alert: HighDiskIO
  expr: rate(node_disk_io_time_seconds_total[5m]) > 0.9
  for: 10m
  labels:
    severity: warning
```

**–¢–∏–ø–∏—á–Ω—ã–µ –∞–ª–µ—Ä—Ç—ã –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π:**

yaml

````yaml
# High error rate
- alert: HighErrorRate
  expr: |
    sum(rate(http_requests_total{status=~"5.."}[5m])) by (service)
    /
    sum(rate(http_requests_total[5m])) by (service)
    > 0.05
  for: 5m
  labels:
    severity: critical

# Slow response time
- alert: SlowResponseTime
  expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 1
  for: 10m
  labels:
    severity: warning

# High request rate (DDoS?)
- alert: UnusuallyHighTraffic
  expr: sum(rate(http_requests_total[5m])) > 1000
  for: 5m
  labels:
    severity: warning

# Database connection pool exhausted
- alert: DatabaseConnectionPoolNearLimit
  expr: database_connections_active / database_connections_max > 0.9
  for: 5m
  labels:
    severity: warning

# Queue backed up
- alert: QueueBacklog
  expr: queue_depth > 1000
  for: 10m
  labels:
    severity: warning

# Certificate expiring soon
- alert: CertificateExpiringSoon
  expr: (ssl_certificate_expiry_timestamp - time()) / 86400 < 30
  for: 1h
  labels:
    severity: warning
```

**Alert fatigue - –∫–∞–∫ –∏–∑–±–µ–∂–∞—Ç—å:**
```
–ü—Ä–æ–±–ª–µ–º–∞: –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –∞–ª–µ—Ä—Ç–æ–≤ ‚Üí –∏–≥–Ω–æ—Ä–∏—Ä—É—é—Ç—Å—è ‚Üí –ø—Ä–æ–ø—É—â–µ–Ω—ã —Ä–µ–∞–ª—å–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã

–†–µ—à–µ–Ω–∏—è:
1. ‚úÖ –ê–ª–µ—Ä—Ç–∏—Ç—å —Ç–æ–ª—å–∫–æ –Ω–∞ —Å–∏–º–ø—Ç–æ–º—ã, –∞ –Ω–µ –ø—Ä–∏—á–∏–Ω—ã
   ‚ùå CPU high, Memory high, Disk full (–ø—Ä–∏—á–∏–Ω—ã)
   ‚úÖ Users can't login, API is slow (—Å–∏–º–ø—Ç–æ–º—ã)

2. ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–π –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ threshold
   ‚ùå CPU > 50% (—Å–ª–∏—à–∫–æ–º —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ)
   ‚úÖ CPU > 80% for 10 minutes (—Ä–∞–∑—É–º–Ω–æ)

3. ‚úÖ –ì—Ä—É–ø–ø–∏—Ä—É–π –ø–æ—Ö–æ–∂–∏–µ –∞–ª–µ—Ä—Ç—ã
   ‚ùå 50 –∞–ª–µ—Ä—Ç–æ–≤ "pod X down"
   ‚úÖ 1 –∞–ª–µ—Ä—Ç "50 pods down in namespace Y"

4. ‚úÖ Inhibition rules –¥–ª—è –∑–∞–≤–∏—Å–∏–º—ã—Ö –∞–ª–µ—Ä—Ç–æ–≤
   –ï—Å–ª–∏ –∫–ª–∞—Å—Ç–µ—Ä down ‚Üí –Ω–µ —Å–ª–∞—Ç—å –∞–ª–µ—Ä—Ç—ã –æ —Å–µ—Ä–≤–∏—Å–∞—Ö

5. ‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω–æ–µ –≤—Ä–µ–º—è —Å—É—Ç–æ–∫
   Non-critical –∞–ª–µ—Ä—Ç—ã —Ç–æ–ª—å–∫–æ –≤ —Ä–∞–±–æ—á–µ–µ –≤—Ä–µ–º—è

6. ‚úÖ SLO-based alerting
   –ê–ª–µ—Ä—Ç–∏—Ç—å –∫–æ–≥–¥–∞ error budget –∏—Å—á–µ—Ä–ø—ã–≤–∞–µ—Ç—Å—è

7. ‚úÖ –†–µ–≥—É–ª—è—Ä–Ω—ã–π review –∏ cleanup
   –£–¥–∞–ª—è–π –Ω–µ–∞–∫—Ç—É–∞–ª—å–Ω—ã–µ –∞–ª–µ—Ä—Ç—ã
```

**Notification channels:**
```
–ö—Ä–∏—Ç–∏—á–Ω–æ—Å—Ç—å    –ö–∞–Ω–∞–ª           –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
Critical       PagerDuty       Production outage, —Ç—Ä–µ–±—É–µ—Ç –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è
               OpsGenie        
               
Warning        Slack           –¢—Ä–µ–±—É–µ—Ç –≤–Ω–∏–º–∞–Ω–∏—è, –Ω–æ –Ω–µ —Å—Ä–æ—á–Ω–æ
               Teams           
               
Info           Email           FYI, —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞, –æ—Ç—á–µ—Ç—ã
               Webhook         –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –¥—Ä—É–≥–∏–º–∏ —Å–∏—Å—Ç–µ–º–∞–º–∏
               
–í—Å–µ —É—Ä–æ–≤–Ω–∏     Grafana         –î–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ –∞–Ω–∞–ª–∏–∑–∞
````

**Alertmanager –∫–æ–º–∞–Ω–¥—ã:**

bash

```bash
# –°—Ç–∞—Ç—É—Å
amtool config show
amtool config routes
amtool alert query

# Silences
amtool silence add alertname=HighCPU --duration=2h --comment="Maintenance"
amtool silence query
amtool silence expire <silence-id>

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–Ω—Ñ–∏–≥–∞
amtool check-config alertmanager.yml

# –û—Ç–ø—Ä–∞–≤–∫–∞ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –∞–ª–µ—Ä—Ç–∞
amtool alert add alertname=Test severity=warning

# API –∑–∞–ø—Ä–æ—Å—ã
curl -X GET http://localhost:9093/api/v2/alerts
curl -X GET http://localhost:9093/api/v2/silences
curl -X GET http://localhost:9093/api/v2/status
```

### üíª –ó–∞–¥–∞–Ω–∏–µ

–ù–∞—Å—Ç—Ä–æ–π –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É –∞–ª–µ—Ä—Ç–∏–Ω–≥–∞:

1. **–î–æ–±–∞–≤—å Alertmanager –≤ docker-compose.yml**:

yaml

```yaml
version: '3.8'

services:
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - ./alerts.yml:/etc/prometheus/alerts.yml
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.enable-lifecycle'
    restart: unless-stopped

  alertmanager:
    image: prom/alertmanager:latest
    container_name: alertmanager
    ports:
      - "9093:9093"
    volumes:
      - ./alertmanager.yml:/etc/alertmanager/alertmanager.yml
      - alertmanager-data:/alertmanager
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
    restart: unless-stopped

  node-exporter:
    image: prom/node-exporter:latest
    container_name: node-exporter
    ports:
      - "9100:9100"
    command:
      - '--path.rootfs=/host'
    pid: host
    restart: unless-stopped
    volumes:
      - '/:/host:ro,rslave'

  # Webhook receiver –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
  webhook-receiver:
    image: ghcr.io/tarampampam/webhook-tester:latest
    container_name: webhook-receiver
    ports:
      - "8080:8080"
    restart: unless-stopped

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_UNIFIED_ALERTING_ENABLED=true
    volumes:
      - grafana-data:/var/lib/grafana
      - ./grafana-datasources.yml:/etc/grafana/provisioning/datasources/datasources.yml
    restart: unless-stopped
    depends_on:
      - prometheus

volumes:
  prometheus-data:
  alertmanager-data:
  grafana-data:
```

2. **–°–æ–∑–¥–∞–π prometheus.yml —Å –∞–ª–µ—Ä—Ç–∏–Ω–≥–æ–º**:

yaml

```yaml
global:
  scrape_interval: 15s
  evaluation_interval: 15s
  external_labels:
    cluster: 'local'
    environment: 'dev'

# Alertmanager configuration
alerting:
  alertmanagers:
    - static_configs:
        - targets:
            - alertmanager:9093

# Load rules
rule_files:
  - "alerts.yml"

scrape_configs:
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  - job_name: 'node-exporter'
    static_configs:
      - targets: ['node-exporter:9100']

  - job_name: 'alertmanager'
    static_configs:
      - targets: ['alertmanager:9093']
```

3. **–°–æ–∑–¥–∞–π alerts.yml —Å –ø—Ä–∞–≤–∏–ª–∞–º–∏**:

yaml

```yaml
groups:
  - name: infrastructure
    interval: 30s
    rules:
    # Instance down
    - alert: InstanceDown
      expr: up == 0
      for: 2m
      labels:
        severity: critical
        team: infrastructure
      annotations:
        summary: "Instance {{ $labels.instance }} is down"
        description: "{{ $labels.job }} on {{ $labels.instance }} has been down for more than 2 minutes."
        dashboard: "http://localhost:3000/d/node-exporter"
        runbook: "https://runbooks.example.com/InstanceDown"

    # High CPU
    - alert: HighCPUUsage
      expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
      for: 5m
      labels:
        severity: warning
        team: infrastructure
      annotations:
        summary: "High CPU usage on {{ $labels.instance }}"
        description: "CPU usage is {{ $value | humanize }}% on {{ $labels.instance }}"
        dashboard: "http://localhost:3000/d/node-exporter"

    # High Memory
    - alert: HighMemoryUsage
      expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 85
      for: 5m
      labels:
        severity: warning
        team: infrastructure
      annotations:
        summary: "High memory usage on {{ $labels.instance }}"
        description: "Memory usage is {{ $value | humanize }}% on {{ $labels.instance }}"

    # Disk space critical
    - alert: DiskSpaceCritical
      expr: (1 - (node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"})) * 100 > 90
      for: 5m
      labels:
        severity: critical
        team: infrastructure
      annotations:
        summary: "Critical disk space on {{ $labels.instance }}"
        description: "Disk usage is {{ $value | humanize }}% on {{ $labels.instance }}"
        impact: "System may become unresponsive if disk fills up"

    # Disk space warning
    - alert: DiskSpaceWarning
      expr: (1 - (node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"})) * 100 > 80
      for: 10m
      labels:
        severity: warning
        team: infrastructure
      annotations:
        summary: "Low disk space on {{ $labels.instance }}"
        description: "Disk usage is {{ $value | humanize }}% on {{ $labels.instance }}"

  - name: alertmanager
    interval: 30s
    rules:
    # Alertmanager down
    - alert: AlertmanagerDown
      expr: up{job="alertmanager"} == 0
      for: 2m
      labels:
        severity: critical
        team: monitoring
      annotations:
        summary: "Alertmanager is down"
        description: "Alertmanager has been down for more than 2 minutes. Alerts may not be delivered!"

    # Too many alerts firing
    - alert: TooManyAlerts
      expr: count(ALERTS{alertstate="firing"}) > 10
      for: 5m
      labels:
        severity: warning
        team: monitoring
      annotations:
        summary: "Too many alerts firing"
        description: "There are {{ $value }} alerts currently firing. This may indicate a systemic issue."

  - name: prometheus
    interval: 30s
    rules:
    # Prometheus target missing
    - alert: PrometheusTargetMissing
      expr: up == 0
      for: 2m
      labels:
        severity: critical
        team: monitoring
      annotations:
        summary: "Prometheus target missing"
        description: "A Prometheus target has disappeared. Instance: {{ $labels.instance }}"

    # Prometheus config reload failed
    - alert: PrometheusConfigReloadFailed
      expr: prometheus_config_last_reload_successful == 0
      for: 5m
      labels:
        severity: critical
        team: monitoring
      annotations:
        summary: "Prometheus config reload failed"
        description: "Prometheus config reload has failed on {{ $labels.instance }}"

  - name: deadman
    interval: 30s
    rules:
    # Deadman switch - –∞–ª–µ—Ä—Ç –∫–æ—Ç–æ—Ä—ã–π –≤—Å–µ–≥–¥–∞ –¥–æ–ª–∂–µ–Ω firing
    - alert: DeadMansSwitch
      expr: vector(1)
      labels:
        severity: info
        team: monitoring
      annotations:
        summary: "Monitoring system is alive"
        description: "This is a deadman switch. It should always be firing. If you don't receive this, monitoring is broken."
```

4. **–°–æ–∑–¥–∞–π alertmanager.yml —Å routing –∏ receivers**:

yaml

```yaml
global:
  resolve_timeout: 5m
  # Slack (—Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π –∏ –Ω–∞—Å—Ç—Ä–æ–π –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏)
  # slack_api_url: 'https://hooks.slack.com/services/YOUR/WEBHOOK/URL'

# Templates –¥–ª—è –∫—Ä–∞—Å–∏–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
templates:
  - '/etc/alertmanager/templates/*.tmpl'

# Route tree
route:
  receiver: 'default'
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 10s
  group_interval: 5m
  repeat_interval: 4h
  
  routes:
    # Critical –∞–ª–µ—Ä—Ç—ã ‚Üí webhook + log
    - match:
        severity: critical
      receiver: critical-alerts
      group_wait: 10s
      repeat_interval: 1h
      continue: true

    # Infrastructure team
    - match:
        team: infrastructure
      receiver: infrastructure-team
      group_wait: 30s
      repeat_interval: 4h

    # Monitoring team
    - match:
        team: monitoring
      receiver: monitoring-team

    # Deadman switch (–¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —á—Ç–æ alerting —Ä–∞–±–æ—Ç–∞–µ—Ç)
    - match:
        alertname: DeadMansSwitch
      receiver: deadman
      repeat_interval: 5m

# Inhibition rules (–ø–æ–¥–∞–≤–ª–µ–Ω–∏–µ –∑–∞–≤–∏—Å–∏–º—ã—Ö –∞–ª–µ—Ä—Ç–æ–≤)
inhibit_rules:
  # –ï—Å–ª–∏ instance down, –Ω–µ —Å–ª–∞—Ç—å –¥—Ä—É–≥–∏–µ –∞–ª–µ—Ä—Ç—ã —Å —Ç–æ–≥–æ –∂–µ instance
  - source_match:
      severity: critical
      alertname: InstanceDown
    target_match:
      severity: warning
    equal: ['instance']

  # –ï—Å–ª–∏ –¥–∏—Å–∫ –∫—Ä–∏—Ç–∏—á–µ–Ω, –Ω–µ —Å–ª–∞—Ç—å warning –æ –¥–∏—Å–∫–µ
  - source_match:
      alertname: DiskSpaceCritical
    target_match:
      alertname: DiskSpaceWarning
    equal: ['instance', 'mountpoint']

# Receivers (–∫–∞–Ω–∞–ª—ã —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π)
receivers:
  - name: 'default'
    webhook_configs:
      - url: 'http://webhook-receiver:8080/webhook/default'
        send_resolved: true

  - name: 'critical-alerts'
    webhook_configs:
      - url: 'http://webhook-receiver:8080/webhook/critical'
        send_resolved: true
    # Uncomment for Slack
    # slack_configs:
    #   - channel: '#alerts-critical'
    #     title: 'üö® CRITICAL ALERT'
    #     text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
    #     send_resolved: true

  - name: 'infrastructure-team'
    webhook_configs:
      - url: 'http://webhook-receiver:8080/webhook/infrastructure'
        send_resolved: true
    # Uncomment for Slack
    # slack_configs:
    #   - channel: '#team-infrastructure'
    #     title: '‚ö†Ô∏è Infrastructure Alert'
    #     text: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'

  - name: 'monitoring-team'
    webhook_configs:
      - url: 'http://webhook-receiver:8080/webhook/monitoring'
        send_resolved: true

  - name: 'deadman'
    webhook_configs:
      - url: 'http://webhook-receiver:8080/webhook/deadman'
        send_resolved: false
```

5. **–°–æ–∑–¥–∞–π grafana-datasources.yml**:

yaml

```yaml
apiVersion: 1

datasources:
  - name: Prometheus
    type: prometheus
    access: proxy
    url: http://prometheus:9090
    isDefault: true
    editable: true
    jsonData:
      httpMethod: POST
      
  - name: Alertmanager
    type: alertmanager
    access: proxy
    url: http://alertmanager:9093
    editable: true
    jsonData:
      implementation: prometheus
```

6. **–ó–∞–ø—É—Å—Ç–∏ –∏ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–π**:

bash

```bash
# –ó–∞–ø—É—Å–∫
docker-compose up -d

# –ü—Ä–æ–≤–µ—Ä–∫–∞ Prometheus
curl http://localhost:9090/api/v1/rules

# –ü—Ä–æ–≤–µ—Ä–∫–∞ Alertmanager
curl http://localhost:9093/api/v2/status

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –∞–ª–µ—Ä—Ç–æ–≤ –≤ Prometheus
curl http://localhost:9090/api/v1/alerts | jq

# –°–ø–∏—Å–æ–∫ firing –∞–ª–µ—Ä—Ç–æ–≤
curl http://localhost:9093/api/v2/alerts | jq '.[] | select(.status.state == "active")'
```

7. **–°–æ–∑–¥–∞–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ—Å—Ç–æ–≤–æ–π –Ω–∞–≥—Ä—É–∑–∫–∏** (`stress_test.sh`):

bash

```bash
#!/bin/bash

echo "Starting stress test to trigger alerts..."

# CPU stress (—Ç—Ä–∏–≥–≥–µ—Ä–Ω–µ—Ç HighCPUUsage)
echo "Generating CPU load..."
docker run --rm --name cpu-stress \
  polinux/stress \
  stress --cpu 4 --timeout 300s &

# –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –¥–∏—Å–∫–∞ (–¥–ª—è DiskSpaceWarning)
# –í–ù–ò–ú–ê–ù–ò–ï: –ë—É–¥—å –æ—Å—Ç–æ—Ä–æ–∂–µ–Ω —Å —ç—Ç–∏–º –Ω–∞ –ø—Ä–æ–¥–µ!
# echo "Filling disk space..."
# dd if=/dev/zero of=/tmp/largefile bs=1M count=10000

echo "Stress test running. Check alerts in:"
echo "- Prometheus: http://localhost:9090/alerts"
echo "- Alertmanager: http://localhost:9093"
echo "- Webhook receiver: http://localhost:8080"
echo ""
echo "Wait 5-10 minutes for alerts to fire..."
```

8. **–ü—Ä–æ–≤–µ—Ä—å UI –∏ –∞–ª–µ—Ä—Ç—ã**:

bash

```bash
# Prometheus Alerts UI
open http://localhost:9090/alerts

# Alertmanager UI
open http://localhost:9093

# Grafana Alerting
open http://localhost:3000/alerting/list

# Webhook receiver (–ø—Ä–æ–≤–µ—Ä—å –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –∞–ª–µ—Ä—Ç—ã)
open http://localhost:8080
```

9. **–ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–π silencing**:

bash

```bash
# –£—Å—Ç–∞–Ω–æ–≤–∏ amtool
go install github.com/prometheus/alertmanager/cmd/amtool@latest
# –∏–ª–∏
brew install amtool

# –ù–∞—Å—Ç—Ä–æ–π amtool
cat > ~/.config/amtool/config.yml <<EOF
alertmanager.url: http://localhost:9093
EOF

# –°–æ–∑–¥–∞–π silence –Ω–∞ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∞
amtool silence add \
  alertname=HighCPUUsage \
  --duration=1h \
  --comment="Testing alert system" \
  --author="devops@example.com"

# –ü—Ä–æ–≤–µ—Ä—å silences
amtool silence query

# –£–¥–∞–ª–∏ silence
amtool silence expire <silence-id>
```

### üöÄ –ë–æ–Ω—É—Å (–Ω–æ–≤–æ–µ)

**1. –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å–æ Slack**:

–û–±–Ω–æ–≤–∏ `alertmanager.yml`:

yaml

```yaml
global:
  slack_api_url: 'https://hooks.slack.com/services/YOUR/WEBHOOK/URL'

receivers:
  - name: 'slack-critical'
    slack_configs:
      - channel: '#alerts-critical'
        username: 'Alertmanager'
        icon_emoji: ':fire:'
        title: 'üö® {{ .GroupLabels.alertname }}'
        text: |
          {{ range .Alerts }}
          *Alert:* {{ .Labels.alertname }}
          *Severity:* {{ .Labels.severity }}
          *Instance:* {{ .Labels.instance }}
          *Description:* {{ .Annotations.description }}
          *Dashboard:* {{ .Annotations.dashboard }}
          {{ end }}
        send_resolved: true
        color: '{{ if eq .Status "firing" }}danger{{ else }}good{{ end }}'
```

**2. Custom notification template**:

–°–æ–∑–¥–∞–π `templates/slack.tmpl`:

gotmpl

```gotmpl
{{ define "slack.title" }}
[{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ .GroupLabels.alertname }}
{{ end }}

{{ define "slack.text" }}
{{ range .Alerts }}
*Alert:* {{ .Labels.alertname }} - `{{ .Labels.severity }}`
*Instance:* {{ .Labels.instance }}
*Summary:* {{ .Annotations.summary }}
*Description:* {{ .Annotations.description }}
{{ if .Annotations.runbook }}*Runbook:* {{ .Annotations.runbook }}{{ end }}
{{ if .Annotations.dashboard }}*Dashboard:* {{ .Annotations.dashboard }}{{ end }}
*Started:* {{ .StartsAt.Format "2006-01-02 15:04:05 MST" }}
{{ if .EndsAt }}*Ended:* {{ .EndsAt.Format "2006-01-02 15:04:05 MST" }}{{ end }}
{{ end }}
{{ end }}

{{ define "slack.color" }}
{{ if eq .Status "firing" }}
  {{ if eq .CommonLabels.severity "critical" }}danger{{ else }}warning{{ end }}
{{ else }}
good
{{ end }}
{{ end }}
```

**3. PagerDuty –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è** (–¥–ª—è critical alerts):

yaml

```yaml
receivers:
  - name: 'pagerduty-critical'
    pagerduty_configs:
      - service_key: 'YOUR_PAGERDUTY_SERVICE_KEY'
        description: '{{ .GroupLabels.alertname }}: {{ .Annotations.summary }}'
        severity: '{{ .CommonLabels.severity }}'
        details:
          firing: '{{ .Alerts.Firing | len }}'
          resolved: '{{ .Alerts.Resolved | len }}'
          instance: '{{ .CommonLabels.instance }}'
        client: 'Alertmanager'
        client_url: 'http://alertmanager:9093'
        send_resolved: true
```

**4. Email notifications —Å HTML template**:

yaml

```yaml
receivers:
  - name: 'email-team'
    email_configs:
      - to: 'team@example.com'
        from: 'alertmanager@example.com'
        smarthost: 'smtp.gmail.com:587'
        auth_username: 'alertmanager@example.com'
        auth_password: 'your-app-password'
        headers:
          Subject: '{{ if eq .Status "firing" }}üö®{{ else }}‚úÖ{{ end }} [{{ .Status | toUpper }}] {{ .GroupLabels.alertname }}'
        html: |
          <!DOCTYPE html>
          <html>
          <body>
            <h2 style="color: {{ if eq .Status "firing" }}#d9534f{{ else }}#5cb85c{{ end }}">
              {{ if eq .Status "firing" }}üö® Firing Alerts{{ else }}‚úÖ Resolved{{ end }}
            </h2>
            {{ range .Alerts }}
            <div style="border-left: 4px solid {{ if eq .Status "firing" }}#d9534f{{ else }}#5cb85c{{ end }}; padding: 10px; margin: 10px 0;">
              <h3>{{ .Labels.alertname }}</h3>
              <p><strong>Severity:</strong> {{ .Labels.severity }}</p>
              <p><strong>Instance:</strong> {{ .Labels.instance }}</p>
              <p><strong>Description:</strong> {{ .Annotations.description }}</p>
              {{ if .Annotations.runbook }}
              <p><a href="{{ .Annotations.runbook }}">üìñ Runbook</a></p>
              {{ end }}
              {{ if .Annotations.dashboard }}
              <p><a href="{{ .Annotations.dashboard }}">üìä Dashboard</a></p>
              {{ end }}
            </div>
            {{ end }}
          </body>
          </html>
        send_resolved: true
```

**5. Webhook –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å Jira/ServiceNow**:

–°–æ–∑–¥–∞–π `webhook_handler.py`:

python

```python
#!/usr/bin/env python3
from flask import Flask, request, jsonify
import json
import requests

app = Flask(__name__)

@app.route('/webhook/jira', methods=['POST'])
def jira_webhook():
    """–°–æ–∑–¥–∞–µ—Ç Jira ticket –¥–ª—è –∫—Ä–∏—Ç–∏—á–Ω—ã—Ö –∞–ª–µ—Ä—Ç–æ–≤"""
    data = request.json
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ firing –∏ critical
    if data['status'] == 'firing':
        for alert in data['alerts']:
            if alert['labels'].get('severity') == 'critical':
                create_jira_ticket(alert)
    
    return jsonify({'status': 'ok'}), 200

def create_jira_ticket(alert):
    """–°–æ–∑–¥–∞–µ—Ç Jira ticket —á–µ—Ä–µ–∑ API"""
    jira_url = "https://your-jira.atlassian.net/rest/api/2/issue"
    
    ticket = {
        "fields": {
            "project": {"key": "OPS"},
            "summary": f"[ALERT] {alert['labels']['alertname']}",
            "description": alert['annotations']['description'],
            "issuetype": {"name": "Incident"}, 
            "priority": {"name": "Critical"}, 
            "labels": ["alert", "monitoring"] } }
            # –û—Ç–ø—Ä–∞–≤–∫–∞ –≤ Jira
	response = requests.post(
		jira_url,
		json=ticket,
		auth=('user@example.com', 'jira-api-token'),
		headers={'Content-Type': 'application/json'}
	)
	print(f"Jira ticket created: {response.json().get('key')}")	
	if name == 'main': app.run(host='0.0.0.0', port=5000)
```
**6. SLO-based alerting** (–ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø–æ–¥—Ö–æ–¥):
```yaml
groups:
  - name: slo_alerts
    interval: 30s
    rules:
    # Error budget burn rate
    - alert: ErrorBudgetBurnRateTooHigh
      expr: |
        (
          sum(rate(http_requests_total{status=~"5.."}[1h]))
          /
          sum(rate(http_requests_total[1h]))
        ) > (1 - 0.999) * 10  # 10x SLO burn rate
      for: 5m
      labels:
        severity: critical
        team: sre
      annotations:
        summary: "Error budget burning too fast"
        description: "Current error rate is {{ $value | humanizePercentage }}. At this rate, monthly error budget will be exhausted in {{ with printf \"(1-0.999)*730/%f\" $value }}{{ . }}{{ end }} hours."
        dashboard: "http://localhost:3000/d/slo-dashboard"

    # SLO violation
    - alert: SLOViolation
      expr: |
        (
          1 - (
            sum(rate(http_requests_total{status!~"5.."}[30d]))
            /
            sum(rate(http_requests_total[30d]))
          )
        ) > 0.001  # –ù–∞—Ä—É—à–µ–Ω–∏–µ 99.9% SLO
      for: 1h
      labels:
        severity: warning
        team: sre
      annotations:
        summary: "SLO violation detected"
        description: "30-day error rate is {{ $value | humanizePercentage }}, violating 99.9% SLO"
```

**7. Multi-window multi-burn-rate alerts** (Google SRE –ø–æ–¥—Ö–æ–¥):
```yaml
groups:
  - name: multiwindow_multiburn_alerts
    interval: 30s
    rules:
    # Fast burn (–Ω—É–∂–Ω–æ –¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ)
    - alert: ErrorBudgetFastBurn
      expr: |
        (
          sum(rate(http_requests_total{status=~"5.."}[5m]))
          /
          sum(rate(http_requests_total[5m]))
        ) > 14.4 * (1 - 0.999)  # 14.4x burn rate
        and
        (
          sum(rate(http_requests_total{status=~"5.."}[1h]))
          /
          sum(rate(http_requests_total[1h]))
        ) > 14.4 * (1 - 0.999)
      for: 2m
      labels:
        severity: critical
        burn_rate: fast
      annotations:
        summary: "Fast error budget burn"
        description: "Error budget will be exhausted in 2 hours at current rate"

    # Slow burn (—Ç—Ä–µ–±—É–µ—Ç –≤–Ω–∏–º–∞–Ω–∏—è –≤ –±–ª–∏–∂–∞–π—à–µ–µ –≤—Ä–µ–º—è)
    - alert: ErrorBudgetSlowBurn
      expr: |
        (
          sum(rate(http_requests_total{status=~"5.."}[30m]))
          /
          sum(rate(http_requests_total[30m]))
        ) > 6 * (1 - 0.999)  # 6x burn rate
        and
        (
          sum(rate(http_requests_total{status=~"5.."}[6h]))
          /
          sum(rate(http_requests_total[6h]))
        ) > 6 * (1 - 0.999)
      for: 15m
      labels:
        severity: warning
        burn_rate: slow
      annotations:
        summary: "Slow error budget burn"
        description: "Error budget will be exhausted in 5 days at current rate"
```

**8. Alert aggregation dashboard**:

–°–æ–∑–¥–∞–π Python —Å–∫—Ä–∏–ø—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∞–ª–µ—Ä—Ç–æ–≤ (`alert_analysis.py`):
```python
#!/usr/bin/env python3
import requests
from collections import Counter
from datetime import datetime, timedelta

ALERTMANAGER_URL = "http://localhost:9093"

def get_alerts():
    """–ü–æ–ª—É—á–∏—Ç—å –≤—Å–µ –∞–ª–µ—Ä—Ç—ã –∏–∑ Alertmanager"""
    response = requests.get(f"{ALERTMANAGER_URL}/api/v2/alerts")
    return response.json()

def analyze_alerts():
    """–ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –∞–ª–µ—Ä—Ç–æ–≤"""
    alerts = get_alerts()
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    total_alerts = len(alerts)
    firing_alerts = [a for a in alerts if a['status']['state'] == 'active']
    
    # –ü–æ severity
    severity_counter = Counter(
        alert['labels'].get('severity', 'unknown') 
        for alert in firing_alerts
    )
    
    # –ü–æ team
    team_counter = Counter(
        alert['labels'].get('team', 'unknown') 
        for alert in firing_alerts
    )
    
    # –°–∞–º—ã–µ —á–∞—Å—Ç—ã–µ –∞–ª–µ—Ä—Ç—ã
    alert_counter = Counter(
        alert['labels']['alertname'] 
        for alert in firing_alerts
    )
    
    # –í—ã–≤–æ–¥ –æ—Ç—á–µ—Ç–∞
    print("=" * 60)
    print("ALERT ANALYSIS REPORT")
    print("=" * 60)
    print(f"Total alerts: {total_alerts}")
    print(f"Firing alerts: {len(firing_alerts)}")
    print()
    
    print("By Severity:")
    for severity, count in severity_counter.most_common():
        print(f"  {severity}: {count}")
    print()
    
    print("By Team:")
    for team, count in team_counter.most_common():
        print(f"  {team}: {count}")
    print()
    
    print("Top 5 Most Frequent Alerts:")
    for alertname, count in alert_counter.most_common(5):
        print(f"  {alertname}: {count}")
    print("=" * 60)

if __name__ == "__main__":
    analyze_alerts()
```

**9. Alert testing framework**:

–°–æ–∑–¥–∞–π `alert_test.py`:
```python
#!/usr/bin/env python3
"""
–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–ª–µ—Ä—Ç–æ–≤ - –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º
—á—Ç–æ –∞–ª–µ—Ä—Ç—ã —Å—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç
"""
import requests
import time
from prometheus_client import CollectorRegistry, Gauge, push_to_gateway

def test_high_cpu_alert():
    """–¢–µ—Å—Ç –∞–ª–µ—Ä—Ç–∞ HighCPUUsage"""
    print("Testing HighCPUUsage alert...")
    
    registry = CollectorRegistry()
    cpu_gauge = Gauge('node_cpu_seconds_total', 
                      'CPU time', 
                      ['mode', 'instance'], 
                      registry=registry)
    
    # –°–∏–º—É–ª–∏—Ä—É–µ–º –≤—ã—Å–æ–∫—É—é CPU –Ω–∞–≥—Ä—É–∑–∫—É
    cpu_gauge.labels(mode='idle', instance='test-instance').set(0.1)
    cpu_gauge.labels(mode='user', instance='test-instance').set(0.8)
    
    # Push –≤ Pushgateway
    push_to_gateway('localhost:9091', job='test', registry=registry)
    
    print("Metrics pushed. Wait 5 minutes and check alerts...")
    print("http://localhost:9090/alerts")

def test_disk_space_alert():
    """–¢–µ—Å—Ç –∞–ª–µ—Ä—Ç–∞ DiskSpaceCritical"""
    print("Testing DiskSpaceCritical alert...")
    
    registry = CollectorRegistry()
    disk_total = Gauge('node_filesystem_size_bytes',
                       'Filesystem size',
                       ['mountpoint', 'instance'],
                       registry=registry)
    disk_avail = Gauge('node_filesystem_avail_bytes',
                       'Available space',
                       ['mountpoint', 'instance'],
                       registry=registry)
    
    # –°–∏–º—É–ª–∏—Ä—É–µ–º 95% –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –¥–∏—Å–∫–∞
    disk_total.labels(mountpoint='/', instance='test-instance').set(100e9)  # 100GB
    disk_avail.labels(mountpoint='/', instance='test-instance').set(5e9)    # 5GB
    
    push_to_gateway('localhost:9091', job='test', registry=registry)
    
    print("Metrics pushed. Check alerts...")

if __name__ == "__main__":
    print("Starting alert tests...")
    test_high_cpu_alert()
    time.sleep(2)
    test_disk_space_alert()
    print("\nTests completed. Monitor alerts for next 10 minutes.")
```

**10. Alert maintenance calendar integration**:
```python
#!/usr/bin/env python3
"""
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ silences –≤–æ –≤—Ä–µ–º—è maintenance windows
"""
import requests
from datetime import datetime, timedelta

ALERTMANAGER_URL = "http://localhost:9093"

def create_maintenance_silence(service, duration_hours, comment):
    """–°–æ–∑–¥–∞—Ç—å silence –Ω–∞ –≤—Ä–µ–º—è maintenance"""
    
    now = datetime.utcnow()
    starts_at = now.isoformat() + "Z"
    ends_at = (now + timedelta(hours=duration_hours)).isoformat() + "Z"
    
    silence = {
        "matchers": [
            {
                "name": "service",
                "value": service,
                "isRegex": False
            }
        ],
        "startsAt": starts_at,
        "endsAt": ends_at,
        "createdBy": "maintenance-script",
        "comment": comment
    }
    
    response = requests.post(
        f"{ALERTMANAGER_URL}/api/v2/silences",
        json=silence
    )
    
    if response.status_code == 200:
        silence_id = response.json()['silenceID']
        print(f"‚úÖ Silence created: {silence_id}")
        print(f"   Service: {service}")
        print(f"   Duration: {duration_hours} hours")
        print(f"   Ends at: {ends_at}")
        return silence_id
    else:
        print(f"‚ùå Failed to create silence: {response.text}")
        return None

if __name__ == "__main__":
    # –ü—Ä–∏–º–µ—Ä: Maintenance –Ω–∞ API —Å–µ—Ä–≤–∏—Å–µ –Ω–∞ 2 —á–∞—Å–∞
    create_maintenance_silence(
        service="api",
        duration_hours=2,
        comment="Planned database migration"
    )
```

---

## –ò—Ç–æ–≥–∏ –º–æ–¥—É–ª—è 5

–ü–æ—Å–ª–µ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏—è —ç—Ç–æ–≥–æ –º–æ–¥—É–ª—è —Ç—ã –¥–æ–ª–∂–µ–Ω —É–º–µ—Ç—å:

‚úÖ –ù–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å Alertmanager —Å routing –∏ inhibition
‚úÖ –ü–∏—Å–∞—Ç—å –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ alert rules –≤ Prometheus
‚úÖ –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –∫–∞–Ω–∞–ª–∞–º–∏ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π (Slack, PagerDuty, Email)
‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å grouping, inhibition –∏ silencing
‚úÖ –°–æ–∑–¥–∞–≤–∞—Ç—å SLO-based alerts
‚úÖ –ò–∑–±–µ–≥–∞—Ç—å alert fatigue —á–µ—Ä–µ–∑ –ø—Ä–∞–≤–∏–ª—å–Ω—É—é –Ω–∞—Å—Ç—Ä–æ–π–∫—É
‚úÖ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –∏ –æ—Ç–ª–∞–∂–∏–≤–∞—Ç—å alerts
‚úÖ –°–æ–∑–¥–∞–≤–∞—Ç—å custom notification templates
‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å maintenance windows

**–ö–ª—é—á–µ–≤—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã –∞–ª–µ—Ä—Ç–∏–Ω–≥–∞:**
1. Alert –Ω–∞ —Å–∏–º–ø—Ç–æ–º—ã, –∞ –Ω–µ –Ω–∞ –ø—Ä–∏—á–∏–Ω—ã
2. –ö–∞–∂–¥—ã–π –∞–ª–µ—Ä—Ç –¥–æ–ª–∂–µ–Ω —Ç—Ä–µ–±–æ–≤–∞—Ç—å –¥–µ–π—Å—Ç–≤–∏—è
3. –ò—Å–ø–æ–ª—å–∑—É–π –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ severity —É—Ä–æ–≤–Ω–∏
4. –ì—Ä—É–ø–ø–∏—Ä—É–π –∏ –ø–æ–¥–∞–≤–ª—è–π –∑–∞–≤–∏—Å–∏–º—ã–µ –∞–ª–µ—Ä—Ç—ã
5. –†–µ–≥—É–ª—è—Ä–Ω–æ review –∏ cleanup –∞–ª–µ—Ä—Ç–æ–≤
6. –î–æ–∫—É–º–µ–Ω—Ç–∏—Ä—É–π runbooks –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∞–ª–µ—Ä—Ç–∞
7. –¢–µ—Å—Ç–∏—Ä—É–π –∞–ª–µ—Ä—Ç—ã —Ä–µ–≥—É–ª—è—Ä–Ω–æ

**–°–ª–µ–¥—É—é—â–∏–π –º–æ–¥—É–ª—å:** Distributed Tracing –∏ Application Performance Monitoring (APM)
## –ú–æ–¥—É–ª—å 6: Distributed Tracing –∏ Application Performance Monitoring (40 –º–∏–Ω—É—Ç)

### üéØ –ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞

**–¢—Ä–∏ —Å—Ç–æ–ª–ø–∞ Observability:**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   METRICS   ‚îÇ  - –ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç? (CPU, memory, requests/sec)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    LOGS     ‚îÇ  - –ß—Ç–æ –ø—Ä–æ–∏–∑–æ—à–ª–æ? (—Å–æ–±—ã—Ç–∏—è, –æ—à–∏–±–∫–∏)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   TRACES    ‚îÇ  - –ü–æ—á–µ–º—É —ç—Ç–æ –ø—Ä–æ–∏–∑–æ—à–ª–æ? (–ø—É—Ç—å –∑–∞–ø—Ä–æ—Å–∞ —á–µ—Ä–µ–∑ —Å–∏—Å—Ç–µ–º—É)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Distributed Tracing - –∑–∞—á–µ–º –Ω—É–∂–µ–Ω:**

```
–ü—Ä–æ–±–ª–µ–º–∞ –≤ –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å–∞—Ö:
User Request ‚Üí API Gateway ‚Üí Auth Service ‚Üí Order Service ‚Üí Payment Service ‚Üí Database
                                                                   ‚Üì
                                              ‚ùå SLOW RESPONSE (5 seconds)

–í–æ–ø—Ä–æ—Å: –ì–¥–µ bottleneck?
- API Gateway: 50ms
- Auth Service: 100ms
- Order Service: 200ms
- Payment Service: 4500ms ‚Üê –ù–ê–ô–î–ï–ù–û!
- Database: 150ms
```

**–û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏:**

**Trace** - –ø–æ–ª–Ω—ã–π –ø—É—Ç—å –æ–¥–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ —á–µ—Ä–µ–∑ —Å–∏—Å—Ç–µ–º—É:

```
Trace ID: abc123
‚îú‚îÄ Span 1: API Gateway (50ms)
‚îú‚îÄ Span 2: Auth Service (100ms)
‚îú‚îÄ Span 3: Order Service (200ms)
‚îÇ  ‚îú‚îÄ Span 4: DB Query (50ms)
‚îÇ  ‚îî‚îÄ Span 5: Cache Check (10ms)
‚îî‚îÄ Span 6: Payment Service (4500ms)
   ‚îî‚îÄ Span 7: External API Call (4400ms) ‚Üê –ü—Ä–æ–±–ª–µ–º–∞!
```

**Span** - –µ–¥–∏–Ω–∏—Ü–∞ —Ä–∞–±–æ—Ç—ã –≤ —Å–∏—Å—Ç–µ–º–µ:

yaml

````yaml
Span:
  trace_id: "abc123"
  span_id: "span456"
  parent_span_id: "span789"
  operation_name: "POST /api/orders"
  start_time: "2025-01-15T10:00:00Z"
  duration: 200ms
  tags:
    http.method: "POST"
    http.status_code: 200
    service.name: "order-service"
    db.statement: "SELECT * FROM orders"
  logs:
    - timestamp: "2025-01-15T10:00:00.050Z"
      message: "Order validated"
```

**–ü–æ–ø—É–ª—è—Ä–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã —Ç—Ä–µ–π—Å–∏–Ω–≥–∞:**
```
Jaeger       - CNCF –ø—Ä–æ–µ–∫—Ç, –æ—Ç Uber, Go
Zipkin       - –û—Ç Twitter, Java
Tempo        - –û—Ç Grafana Labs, –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Loki
OpenTelemetry - –°—Ç–∞–Ω–¥–∞—Ä—Ç (–æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ OpenTracing + OpenCensus)
AWS X-Ray    - Managed —Å–µ—Ä–≤–∏—Å –æ—Ç AWS
Datadog APM  - Commercial
New Relic    - Commercial
```

**OpenTelemetry (OTel) - —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ     Your Application             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  OpenTelemetry SDK         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Auto-instrumentation    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Manual instrumentation  ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ OTel Collector ‚îÇ - –û–±—Ä–∞–±–æ—Ç–∫–∞, —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ                       ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îê
‚îÇ Jaeger ‚îÇ            ‚îÇ Tempo  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Sampling (–≤—ã–±–æ—Ä–∫–∞ —Ç—Ä–µ–π—Å–æ–≤):**
```
–ü—Ä–æ–±–ª–µ–º–∞: –ù–µ–ª—å–∑—è —Ö—Ä–∞–Ω–∏—Ç—å 100% —Ç—Ä–µ–π—Å–æ–≤ (—Å–ª–∏—à–∫–æ–º –¥–æ—Ä–æ–≥–æ)

–í–∏–¥—ã sampling:
1. Head sampling (—Ä–µ—à–µ–Ω–∏–µ –≤ –Ω–∞—á–∞–ª–µ)
   - Probabilistic: 10% –≤—Å–µ—Ö —Ç—Ä–µ–π—Å–æ–≤
   - Rate limiting: 100 —Ç—Ä–µ–π—Å–æ–≤/—Å–µ–∫
   
2. Tail sampling (—Ä–µ—à–µ–Ω–∏–µ –≤ –∫–æ–Ω—Ü–µ)
   - –í—Å–µ –º–µ–¥–ª–µ–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã (> 1s)
   - –í—Å–µ –∑–∞–ø—Ä–æ—Å—ã —Å –æ—à–∏–±–∫–∞–º–∏
   - 1% –Ω–æ—Ä–º–∞–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤

–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: Tail sampling + –≤—Å–µ–≥–¥–∞ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –æ—à–∏–±–∫–∏
```

**APM (Application Performance Monitoring) - —á—Ç–æ –≤–∫–ª—é—á–∞–µ—Ç:**
```
1. –¢—Ä–µ–π—Å–∏–Ω–≥ (Distributed Tracing)
2. –ü—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏–µ (CPU, Memory profiling)
3. Error tracking
4. Real User Monitoring (RUM)
5. Database query analysis
6. External services monitoring
```

**–ö–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ APM:**

**RED –º–µ—Ç—Ä–∏–∫–∏ (–¥–ª—è —Å–µ—Ä–≤–∏—Å–æ–≤):**
```
Rate     - Requests per second
Error    - Error rate (%)
Duration - Request latency (p50, p95, p99)
```

**USE –º–µ—Ç—Ä–∏–∫–∏ (–¥–ª—è —Ä–µ—Å—É—Ä—Å–æ–≤):**
```
Utilization - % –≤—Ä–µ–º–µ–Ω–∏ –∑–∞–Ω—è—Ç–æ—Å—Ç–∏
Saturation  - –î–ª–∏–Ω–∞ –æ—á–µ—Ä–µ–¥–∏
Errors      - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—à–∏–±–æ–∫
```

**Service metrics:**
```
Apdex Score = (Satisfied + Tolerating/2) / Total Requests
- Satisfied: < 1s
- Tolerating: 1-4s
- Frustrated: > 4s

Throughput = Requests per second
Error Rate = Errors / Total Requests
Availability = Uptime / Total Time
````

**Context Propagation (–∫–∞–∫ –ø–µ—Ä–µ–¥–∞–µ—Ç—Å—è trace_id):**

**HTTP Headers:**

http

````http
# W3C Trace Context (—Å—Ç–∞–Ω–¥–∞—Ä—Ç)
traceparent: 00-abc123def456-span789-01
tracestate: vendor1=value1,vendor2=value2

# Jaeger
uber-trace-id: abc123:span456:0:1

# Zipkin
X-B3-TraceId: abc123
X-B3-SpanId: span456
X-B3-ParentSpanId: parent789
X-B3-Sampled: 1
```

**gRPC Metadata:**
```
grpc-trace-bin: <binary trace context>
````

**Instrumentation –ø–æ–¥—Ö–æ–¥—ã:**

**Auto-instrumentation** (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π):

python

```python
# Python —Å OpenTelemetry
from opentelemetry.instrumentation.flask import FlaskInstrumentor
from opentelemetry.instrumentation.requests import RequestsInstrumentor

FlaskInstrumentor().instrument()      # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ Flask
RequestsInstrumentor().instrument()   # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ requests
```

**Manual instrumentation** (—Ä—É—á–Ω–æ–π):

python

```python
from opentelemetry import trace

tracer = trace.get_tracer(__name__)

@app.route('/api/order')
def create_order():
    with tracer.start_as_current_span("create_order") as span:
        span.set_attribute("order.id", order_id)
        span.set_attribute("user.id", user_id)
        
        # –í–∞—à –∫–æ–¥
        result = process_order(order_id)
        
        span.add_event("Order processed")
        return result
```

**–Ø–∑—ã–∫-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏:**

**Python:**

python

```python
# OpenTelemetry
opentelemetry-api
opentelemetry-sdk
opentelemetry-instrumentation-flask
opentelemetry-instrumentation-django
opentelemetry-instrumentation-sqlalchemy
opentelemetry-exporter-jaeger
```

**Node.js:**

javascript

```javascript
// OpenTelemetry
@opentelemetry/api
@opentelemetry/sdk-node
@opentelemetry/auto-instrumentations-node
@opentelemetry/exporter-jaeger
```

**Go:**

go

```go
// OpenTelemetry
go.opentelemetry.io/otel
go.opentelemetry.io/otel/trace
go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp
```

**Java:**

java

````java
// OpenTelemetry Java Agent (auto-instrumentation)
java -javaagent:opentelemetry-javaagent.jar \
     -Dotel.service.name=my-service \
     -jar myapp.jar
```

**Jaeger UI - –æ—Å–Ω–æ–≤–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:**
```
1. Search traces:
   - –ü–æ service name
   - –ü–æ operation name
   - –ü–æ tags
   - –ü–æ duration
   - –ü–æ –≤—Ä–µ–º–µ–Ω–∏

2. Trace timeline:
   - –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è spans
   - Waterfall view
   - Gantt chart

3. Dependencies graph:
   - –ö–∞—Ä—Ç–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π —Å–µ—Ä–≤–∏—Å–æ–≤
   - –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≤—ã–∑–æ–≤–æ–≤

4. Comparison:
   - –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ç—Ä–µ–π—Å–æ–≤
   - A/B testing —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
```

**Service Map (–∫–∞—Ä—Ç–∞ —Å–µ—Ä–≤–∏—Å–æ–≤):**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   User     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      ‚îÇ HTTP
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ API Gateway‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      ‚îÇ
   ‚îå‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ      ‚îÇ        ‚îÇ
‚îå‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îê ‚îå‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇAuth ‚îÇ ‚îÇOrder‚îÇ ‚îÇUser  ‚îÇ
‚îÇSvc  ‚îÇ ‚îÇSvc  ‚îÇ ‚îÇSvc   ‚îÇ
‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
   ‚îÇ       ‚îÇ
   ‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ   ‚îÇPayment  ‚îÇ
   ‚îÇ   ‚îÇSvc      ‚îÇ
   ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
   ‚îÇ       ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ         ‚îÇ
   ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îê
   ‚îÇ DB   ‚îÇ  ‚îÇCache ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Error tracking –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è:**
```
–°–≤—è–∑—å —Ç—Ä–µ–π—Å–æ–≤ —Å –æ—à–∏–±–∫–∞–º–∏:

Exception –≤ –∫–æ–¥–µ ‚Üí Trace ID ‚Üí –ü–æ–ª–Ω—ã–π –ø—É—Ç—å –∑–∞–ø—Ä–æ—Å–∞
                               + stack trace
                               + request params
                               + user context
```

**Database query analysis:**
```
–ß–∞—Å—Ç—ã–µ –ø—Ä–æ–±–ª–µ–º—ã:
1. N+1 queries
   - 1 –∑–∞–ø—Ä–æ—Å —Å–ø–∏—Å–∫–∞ + N –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–µ—Ç–∞–ª–µ–π
   
2. Missing indexes
   - Full table scan
   
3. Slow queries
   - –°–ª–æ–∂–Ω—ã–µ JOIN
   - –ë–æ–ª—å—à–∏–µ SELECT *
   
4. Connection pool exhaustion
   - –ù–µ –∑–∞–∫—Ä—ã—Ç—ã–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
```

**–ü—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏–µ (CPU/Memory):**
```
Continuous Profiling:
- Flamegraph –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
- –ö–∞–∫–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –∑–∞–Ω–∏–º–∞—é—Ç –±–æ–ª—å—à–µ –≤—Ä–µ–º–µ–Ω–∏
- Memory allocations
- Goroutines/Threads

–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã:
- pprof (Go)
- py-spy (Python)
- async-profiler (Java)
- Pyroscope (unified)
````

**Real User Monitoring (RUM):**

javascript

````javascript
// Frontend —Ç—Ä–µ–π—Å–∏–Ω–≥
import { WebTracerProvider } from '@opentelemetry/sdk-trace-web';

const provider = new WebTracerProvider();
const tracer = provider.getTracer('frontend-app');

// Track page load
const span = tracer.startSpan('page_load');
span.setAttribute('page.url', window.location.href);

window.addEventListener('load', () => {
  span.end();
});

// Track user interactions
button.addEventListener('click', () => {
  const span = tracer.startSpan('button_click');
  span.setAttribute('button.id', button.id);
  // ... –¥–µ–π—Å—Ç–≤–∏–µ
  span.end();
});
```

**Best practices:**
```
1. ‚úÖ –í—Å–µ–≥–¥–∞ –ø–µ—Ä–µ–¥–∞–≤–∞–π trace context –º–µ–∂–¥—É —Å–µ—Ä–≤–∏—Å–∞–º–∏
2. ‚úÖ –î–æ–±–∞–≤–ª—è–π –ø–æ–ª–µ–∑–Ω—ã–µ attributes (user_id, order_id, etc)
3. ‚úÖ –õ–æ–≥–∏—Ä—É–π trace_id –≤–æ –≤—Å–µ—Ö –ª–æ–≥–∞—Ö
4. ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–π semantic conventions (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –∏–º–µ–Ω–∞)
5. ‚úÖ –ù–∞—Å—Ç—Ä–æ–π –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π sampling
6. ‚úÖ –ù–µ –ª–æ–≥–∏—Ä—É–π sensitive –¥–∞–Ω–Ω—ã–µ –≤ spans
7. ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–π tail sampling –¥–ª—è –æ—à–∏–±–æ–∫
8. ‚úÖ –•—Ä–∞–Ω–∏ —Ç—Ä–µ–π—Å—ã –º–∏–Ω–∏–º—É–º 7 –¥–Ω–µ–π
9. ‚úÖ –ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–π —Å –∞–ª–µ—Ä—Ç–∏–Ω–≥–æ–º
10. ‚úÖ –°–æ–∑–¥–∞–π runbook –¥–ª—è —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
````

**Semantic Conventions (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –∏–º–µ–Ω–∞):**

yaml

```yaml
# HTTP
span.name: "GET /api/users"
http.method: "GET"
http.url: "https://api.example.com/users"
http.status_code: 200
http.route: "/api/users"

# Database
span.name: "SELECT users"
db.system: "postgresql"
db.operation: "SELECT"
db.statement: "SELECT * FROM users WHERE id = ?"
db.name: "production"

# RPC
span.name: "UserService.GetUser"
rpc.system: "grpc"
rpc.service: "UserService"
rpc.method: "GetUser"

# Messaging
span.name: "process_order"
messaging.system: "kafka"
messaging.destination: "orders"
messaging.operation: "process"
```

### üíª –ó–∞–¥–∞–Ω–∏–µ

–ù–∞—Å—Ç—Ä–æ–π –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π distributed tracing —Å Jaeger:

1. **–°–æ–∑–¥–∞–π docker-compose.yml –¥–ª—è Jaeger stack**:

yaml

```yaml
version: '3.8'

services:
  # Jaeger all-in-one (–¥–ª—è development)
  jaeger:
    image: jaegertracing/all-in-one:1.52
    container_name: jaeger
    environment:
      - COLLECTOR_ZIPKIN_HOST_PORT=:9411
      - COLLECTOR_OTLP_ENABLED=true
    ports:
      - "5775:5775/udp"   # accept zipkin.thrift (deprecated)
      - "6831:6831/udp"   # accept jaeger.thrift compact
      - "6832:6832/udp"   # accept jaeger.thrift binary
      - "5778:5778"       # serve configs
      - "16686:16686"     # Jaeger UI
      - "14250:14250"     # model.proto
      - "14268:14268"     # jaeger.thrift
      - "14269:14269"     # Admin port: health, metrics
      - "4317:4317"       # OTLP gRPC
      - "4318:4318"       # OTLP HTTP
      - "9411:9411"       # Zipkin compatible
    restart: unless-stopped

  # OpenTelemetry Collector (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏)
  otel-collector:
    image: otel/opentelemetry-collector-contrib:0.91.0
    container_name: otel-collector
    command: ["--config=/etc/otel-collector-config.yml"]
    volumes:
      - ./otel-collector-config.yml:/etc/otel-collector-config.yml
    ports:
      - "4317:4317"   # OTLP gRPC
      - "4318:4318"   # OTLP HTTP
      - "8888:8888"   # Prometheus metrics
      - "8889:8889"   # Prometheus exporter metrics
    restart: unless-stopped
    depends_on:
      - jaeger

  # Demo –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ - Frontend
  frontend:
    build: ./demo-app/frontend
    container_name: frontend
    environment:
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4317
      - OTEL_SERVICE_NAME=frontend
      - BACKEND_URL=http://backend:5000
    ports:
      - "8080:8080"
    depends_on:
      - otel-collector
      - backend
    restart: unless-stopped

  # Demo –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ - Backend
  backend:
    build: ./demo-app/backend
    container_name: backend
    environment:
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4317
      - OTEL_SERVICE_NAME=backend
      - DATABASE_URL=postgresql://user:password@postgres:5432/demo
      - REDIS_URL=redis://redis:6379
    ports:
      - "5000:5000"
    depends_on:
      - postgres
      - redis
      - otel-collector
    restart: unless-stopped

  # PostgreSQL database
  postgres:
    image: postgres:16-alpine
    container_name: postgres
    environment:
      - POSTGRES_USER=user
      - POSTGRES_PASSWORD=password
      - POSTGRES_DB=demo
    ports:
      - "5432:5432"
    volumes:
      - postgres-data:/var/lib/postgresql/data
    restart: unless-stopped

  # Redis cache
  redis:
    image: redis:7-alpine
    container_name: redis
    ports:
      - "6379:6379"
    restart: unless-stopped

  # Grafana –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
  grafana:
    image: grafana/grafana:10.2.3
    container_name: grafana-tracing
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_FEATURE_TOGGLES_ENABLE=traceqlEditor
    volumes:
      - grafana-data:/var/lib/grafana
      - ./grafana-datasources.yml:/etc/grafana/provisioning/datasources/datasources.yml
    restart: unless-stopped
    depends_on:
      - jaeger

volumes:
  postgres-data:
  grafana-data:
```

2. **–°–æ–∑–¥–∞–π otel-collector-config.yml**:

yaml

```yaml
receivers:
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317
      http:
        endpoint: 0.0.0.0:4318

  # Prometheus metrics receiver
  prometheus:
    config:
      scrape_configs:
        - job_name: 'otel-collector'
          scrape_interval: 10s
          static_configs:
            - targets: ['0.0.0.0:8888']

processors:
  # Batch processor –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
  batch:
    timeout: 10s
    send_batch_size: 1024

  # Memory limiter
  memory_limiter:
    check_interval: 1s
    limit_mib: 512

  # Tail sampling - —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ –æ—à–∏–±–∫–∏ –∏ –º–µ–¥–ª–µ–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
  tail_sampling:
    decision_wait: 10s
    num_traces: 100
    expected_new_traces_per_sec: 10
    policies:
      # –í—Å–µ–≥–¥–∞ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –æ—à–∏–±–∫–∏
      - name: error-traces
        type: status_code
        status_code:
          status_codes: [ERROR]
      
      # –ú–µ–¥–ª–µ–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã (> 1s)
      - name: slow-traces
        type: latency
        latency:
          threshold_ms: 1000
      
      # 10% –æ—Å—Ç–∞–ª—å–Ω—ã—Ö
      - name: probabilistic-policy
        type: probabilistic
        probabilistic:
          sampling_percentage: 10

  # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–Ω—ã—Ö –∞—Ç—Ä–∏–±—É—Ç–æ–≤
  resource:
    attributes:
      - key: environment
        value: development
        action: insert

  # Attributes processor
  attributes:
    actions:
      - key: db.statement
        action: delete  # –£–¥–∞–ª—è–µ–º SQL –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)

exporters:
  # Jaeger exporter
  jaeger:
    endpoint: jaeger:14250
    tls:
      insecure: true

  # Logging exporter (–¥–ª—è –æ—Ç–ª–∞–¥–∫–∏)
  logging:
    loglevel: info

  # Prometheus exporter –¥–ª—è –º–µ—Ç—Ä–∏–∫
  prometheus:
    endpoint: "0.0.0.0:8889"

service:
  pipelines:
    # Traces pipeline
    traces:
      receivers: [otlp]
      processors: [memory_limiter, tail_sampling, batch, resource, attributes]
      exporters: [jaeger, logging]
    
    # Metrics pipeline
    metrics:
      receivers: [otlp, prometheus]
      processors: [memory_limiter, batch]
      exporters: [prometheus, logging]
```

3. **–°–æ–∑–¥–∞–π demo-app/backend (Python Flask)**:

`demo-app/backend/Dockerfile`:

dockerfile

````dockerfile
FROM python:3.11-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD ["python", "app.py"]
```

`demo-app/backend/requirements.txt`:
```
flask==3.0.0
psycopg2-binary==2.9.9
redis==5.0.1
requests==2.31.0
opentelemetry-api==1.21.0
opentelemetry-sdk==1.21.0
opentelemetry-instrumentation-flask==0.42b0
opentelemetry-instrumentation-requests==0.42b0
opentelemetry-instrumentation-psycopg2==0.42b0
opentelemetry-instrumentation-redis==0.42b0
opentelemetry-exporter-otlp==1.21.0
````

`demo-app/backend/app.py`:

python

```python
from flask import Flask, jsonify, request
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter
from opentelemetry.instrumentation.flask import FlaskInstrumentor
from opentelemetry.instrumentation.requests import RequestsInstrumentor
from opentelemetry.instrumentation.psycopg2 import Psycopg2Instrumentor
from opentelemetry.instrumentation.redis import RedisInstrumentor
from opentelemetry.sdk.resources import Resource
import psycopg2
import redis
import time
import random
import requests
import os

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ OpenTelemetry
resource = Resource.create({
    "service.name": os.getenv("OTEL_SERVICE_NAME", "backend"),
    "service.version": "1.0.0",
    "deployment.environment": "development"
})

provider = TracerProvider(resource=resource)
processor = BatchSpanProcessor(
    OTLPSpanExporter(
        endpoint=os.getenv("OTEL_EXPORTER_OTLP_ENDPOINT", "http://localhost:4317"),
        insecure=True
    )
)
provider.add_span_processor(processor)
trace.set_tracer_provider(provider)

# –°–æ–∑–¥–∞–µ–º tracer
tracer = trace.get_tracer(__name__)

# –°–æ–∑–¥–∞–µ–º Flask app
app = Flask(__name__)

# Auto-instrumentation
FlaskInstrumentor().instrument_app(app)
RequestsInstrumentor().instrument()
Psycopg2Instrumentor().instrument()
RedisInstrumentor().instrument()

# Database connection
DATABASE_URL = os.getenv("DATABASE_URL", "postgresql://user:password@localhost:5432/demo")
REDIS_URL = os.getenv("REDIS_URL", "redis://localhost:6379")

def get_db_connection():
    """Get database connection"""
    return psycopg2.connect(DATABASE_URL)

def get_redis_connection():
    """Get Redis connection"""
    return redis.from_url(REDIS_URL)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ë–î
def init_db():
    """Initialize database"""
    with get_db_connection() as conn:
        with conn.cursor() as cur:
            cur.execute("""
                CREATE TABLE IF NOT EXISTS users (
                    id SERIAL PRIMARY KEY,
                    name VARCHAR(100),
                    email VARCHAR(100),
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)
            cur.execute("""
                CREATE TABLE IF NOT EXISTS orders (
                    id SERIAL PRIMARY KEY,
                    user_id INTEGER REFERENCES users(id),
                    product VARCHAR(100),
                    amount DECIMAL(10, 2),
                    status VARCHAR(20),
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)
            conn.commit()

# Routes
@app.route('/health')
def health():
    """Health check endpoint"""
    return jsonify({"status": "healthy"}), 200

@app.route('/api/users', methods=['GET'])
def get_users():
    """Get all users"""
    with tracer.start_as_current_span("get_users") as span:
        span.set_attribute("db.operation", "SELECT")
        
        # –°–∏–º—É–ª–∏—Ä—É–µ–º —Å–ª—É—á–∞–π–Ω—É—é –∑–∞–¥–µ—Ä–∂–∫—É
        time.sleep(random.uniform(0.01, 0.1))
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º cache
        r = get_redis_connection()
        cached = r.get("users:all")
        
        if cached:
            span.add_event("Cache hit")
            span.set_attribute("cache.hit", True)
            import json
            return jsonify(json.loads(cached)), 200
        
        span.add_event("Cache miss")
        span.set_attribute("cache.hit", False)
        
        # Query database
        with get_db_connection() as conn:
            with conn.cursor() as cur:
                cur.execute("SELECT id, name, email FROM users")
                users = [
                    {"id": row[0], "name": row[1], "email": row[2]}
                    for row in cur.fetchall()
                ]
        
        # Cache result
        import json
        r.setex("users:all", 60, json.dumps(users))
        
        return jsonify(users), 200

@app.route('/api/users/<int:user_id>', methods=['GET'])
def get_user(user_id):
    """Get user by ID"""
    with tracer.start_as_current_span("get_user_by_id") as span:
        span.set_attribute("user.id", user_id)
        
        # –°–∏–º—É–ª–∏—Ä—É–µ–º –∑–∞–¥–µ—Ä–∂–∫—É
        time.sleep(random.uniform(0.01, 0.05))
        
        with get_db_connection() as conn:
            with conn.cursor() as cur:
                cur.execute(
                    "SELECT id, name, email FROM users WHERE id = %s",
                    (user_id,)
                )
                row = cur.fetchone()
                
                if not row:
                    span.set_attribute("http.status_code", 404)
                    return jsonify({"error": "User not found"}), 404
                
                user = {"id": row[0], "name": row[1], "email": row[2]}
        
        return jsonify(user), 200

@app.route('/api/users', methods=['POST'])
def create_user():
    """Create new user"""
    with tracer.start_as_current_span("create_user") as span:
        data = request.json
        
        span.set_attribute("user.name", data.get("name"))
        span.set_attribute("user.email", data.get("email"))
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è
        if not data.get("name") or not data.get("email"):
            span.set_attribute("error", True)
            span.add_event("Validation failed")
            return jsonify({"error": "Name and email required"}), 400
        
        # –°–∏–º—É–ª–∏—Ä—É–µ–º –∑–∞–¥–µ—Ä–∂–∫—É
        time.sleep(random.uniform(0.05, 0.15))
        
        with get_db_connection() as conn:
            with conn.cursor() as cur:
                cur.execute(
                    "INSERT INTO users (name, email) VALUES (%s, %s) RETURNING id",
                    (data["name"], data["email"])
                )
                user_id = cur.fetchone()[0]
                conn.commit()
        
        # Invalidate cache
        r = get_redis_connection()
        r.delete("users:all")
        
        span.add_event("User created", {"user.id": user_id})
        
        return jsonify({"id": user_id, "name": data["name"], "email": data["email"]}), 201

@app.route('/api/orders', methods=['POST'])
def create_order():
    """Create new order - –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç complex trace"""
    with tracer.start_as_current_span("create_order") as span:
        data = request.json
        
        user_id = data.get("user_id")
        product = data.get("product")
        amount = data.get("amount")
        
        span.set_attribute("order.user_id", user_id)
        span.set_attribute("order.product", product)
        span.set_attribute("order.amount", amount)
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è
        if not all([user_id, product, amount]):
            span.set_attribute("error", True)
            return jsonify({"error": "Missing required fields"}), 400
        
        # Step 1: Check user exists
        with tracer.start_as_current_span("check_user") as user_span:
            user_span.set_attribute("user.id", user_id)
            time.sleep(random.uniform(0.01, 0.05))
            
            with get_db_connection() as conn:
                with conn.cursor() as cur:
                    cur.execute("SELECT id FROM users WHERE id = %s", (user_id,))
                    if not cur.fetchone():
                        span.set_attribute("error", True)
                        return jsonify({"error": "User not found"}), 404
        
        # Step 2: Process payment (—Å–∏–º—É–ª–∏—Ä—É–µ–º external API)
        with tracer.start_as_current_span("process_payment") as payment_span:
            payment_span.set_attribute("payment.amount", amount)
            
            # –°–∏–º—É–ª–∏—Ä—É–µ–º —Å–ª—É—á–∞–π–Ω—É—é –∑–∞–¥–µ—Ä–∂–∫—É payment gateway
            delay = random.uniform(0.1, 0.5)
            
            # 10% —à–∞–Ω—Å –º–µ–¥–ª–µ–Ω–Ω–æ–≥–æ payment
            if random.random() < 0.1:
                delay = random.uniform(2, 5)
                payment_span.set_attribute("payment.slow", True)
            
            # 5% —à–∞–Ω—Å –æ—à–∏–±–∫–∏ payment
            if random.random() < 0.05:
                time.sleep(delay)
                payment_span.set_attribute("error", True)
                payment_span.add_event("Payment failed")
                span.set_attribute("error", True)
                return jsonify({"error": "Payment processing failed"}), 500
            
            time.sleep(delay)
            payment_span.add_event("Payment successful")
        
        # Step 3: Create order
        with tracer.start_as_current_span("save_order") as save_span:
            time.sleep(random.uniform(0.02, 0.08))
            
            with get_db_connection() as conn:
                with conn.cursor() as cur:
                    cur.execute(
                        """
                        INSERT INTO orders (user_id, product, amount, status)
                        VALUES (%s, %s, %s, %s)
                        RETURNING id
                        """,
                        (user_id, product, amount, "completed")
                    )
                    order_id = cur.fetchone()[0]
                conn.commit()
        
        save_span.set_attribute("order.id", order_id)
    
    # Step 4: Send notification (async, –≤ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏)
    with tracer.start_as_current_span("send_notification") as notif_span:
        notif_span.set_attribute("notification.type", "email")
        time.sleep(random.uniform(0.05, 0.15))
        notif_span.add_event("Notification sent")
    
    span.add_event("Order created successfully", {"order.id": order_id})
    
    return jsonify({
        "id": order_id,
        "user_id": user_id,
        "product": product,
        "amount": amount,
        "status": "completed"
    }), 201


@app.route('/api/slow') def slow_endpoint(): """Intentionally slow endpoint –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏""" with tracer.start_as_current_span("slow_operation") as span: span.add_event("Starting slow operation")


    # –°–∏–º—É–ª–∏—Ä—É–µ–º N+1 query –ø—Ä–æ–±–ª–µ–º—É
    with get_db_connection() as conn:
        with conn.cursor() as cur:
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
            cur.execute("SELECT id FROM users LIMIT 10")
            user_ids = [row[0] for row in cur.fetchall()]
            
            # N+1: –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –æ—Ç–¥–µ–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å –∑–∞–∫–∞–∑–æ–≤
            for user_id in user_ids:
                with tracer.start_as_current_span(f"get_orders_for_user_{user_id}"):
                    cur.execute(
                        "SELECT * FROM orders WHERE user_id = %s",
                        (user_id,)
                    )
                    cur.fetchall()
                    time.sleep(0.1)  # –°–∏–º—É–ª–∏—Ä—É–µ–º –∑–∞–¥–µ—Ä–∂–∫—É
    
    span.add_event("Slow operation completed")
    return jsonify({"message": "Completed slow operation"}), 200


if **name** == '**main**': init_db() app.run(host='0.0.0.0', port=5000, debug=True)

````

4. **–°–æ–∑–¥–∞–π demo-app/frontend (Node.js Express)**:

`demo-app/frontend/Dockerfile`:
```dockerfile
FROM node:20-alpine

WORKDIR /app

COPY package*.json ./
RUN npm install

COPY . .

CMD ["node", "app.js"]
```

`demo-app/frontend/package.json`:
```json
{
  "name": "frontend-service",
  "version": "1.0.0",
  "dependencies": {
    "express": "^4.18.2",
    "axios": "^1.6.2",
    "@opentelemetry/api": "^1.7.0",
    "@opentelemetry/sdk-node": "^0.45.1",
    "@opentelemetry/auto-instrumentations-node": "^0.40.1",
    "@opentelemetry/exporter-trace-otlp-grpc": "^0.45.1",
    "@opentelemetry/resources": "^1.19.0",
    "@opentelemetry/semantic-conventions": "^1.19.0"
  }
}
```

`demo-app/frontend/app.js`:
```javascript
const { NodeSDK } = require('@opentelemetry/sdk-node');
const { getNodeAutoInstrumentations } = require('@opentelemetry/auto-instrumentations-node');
const { OTLPTraceExporter } = require('@opentelemetry/exporter-trace-otlp-grpc');
const { Resource } = require('@opentelemetry/resources');
const { SemanticResourceAttributes } = require('@opentelemetry/semantic-conventions');

// Initialize OpenTelemetry SDK
const sdk = new NodeSDK({
  resource: new Resource({
    [SemanticResourceAttributes.SERVICE_NAME]: process.env.OTEL_SERVICE_NAME || 'frontend',
    [SemanticResourceAttributes.SERVICE_VERSION]: '1.0.0',
    [SemanticResourceAttributes.DEPLOYMENT_ENVIRONMENT]: 'development',
  }),
  traceExporter: new OTLPTraceExporter({
    url: process.env.OTEL_EXPORTER_OTLP_ENDPOINT || 'http://localhost:4317',
  }),
  instrumentations: [getNodeAutoInstrumentations()],
});

sdk.start();

// Graceful shutdown
process.on('SIGTERM', () => {
  sdk.shutdown()
    .then(() => console.log('Tracing terminated'))
    .catch((error) => console.log('Error terminating tracing', error))
    .finally(() => process.exit(0));
});

// Express app
const express = require('express');
const axios = require('axios');
const { trace, context } = require('@opentelemetry/api');

const app = express();
const PORT = 8080;
const BACKEND_URL = process.env.BACKEND_URL || 'http://localhost:5000';

app.use(express.json());

const tracer = trace.getTracer('frontend-service');

app.get('/health', (req, res) => {
  res.json({ status: 'healthy' });
});

app.get('/', (req, res) => {
  res.send(`
    <html>
      <head><title>Demo Tracing App</title></head>
      <body>
        <h1>Distributed Tracing Demo</h1>
        <h2>Try these endpoints:</h2>
        <ul>
          <li><a href="/users">GET /users</a> - List users</li>
          <li><a href="/user/1">GET /user/:id</a> - Get user by ID</li>
          <li>POST /user - Create user</li>
          <li>POST /order - Create order (complex trace)</li>
          <li><a href="/slow">GET /slow</a> - Slow endpoint (N+1 problem)</li>
        </ul>
        <p>View traces in <a href="http://localhost:16686" target="_blank">Jaeger UI</a></p>
      </body>
    </html>
  `);
});

app.get('/users', async (req, res) => {
  const span = tracer.startSpan('frontend.get_users');
  
  try {
    span.addEvent('Fetching users from backend');
    const response = await axios.get(`${BACKEND_URL}/api/users`);
    span.addEvent('Users fetched successfully');
    res.json(response.data);
  } catch (error) {
    span.recordException(error);
    span.setStatus({ code: 2, message: error.message });
    res.status(500).json({ error: error.message });
  } finally {
    span.end();
  }
});

app.get('/user/:id', async (req, res) => {
  const span = tracer.startSpan('frontend.get_user');
  span.setAttribute('user.id', req.params.id);
  
  try {
    const response = await axios.get(`${BACKEND_URL}/api/users/${req.params.id}`);
    res.json(response.data);
  } catch (error) {
    span.recordException(error);
    if (error.response && error.response.status === 404) {
      res.status(404).json({ error: 'User not found' });
    } else {
      res.status(500).json({ error: error.message });
    }
  } finally {
    span.end();
  }
});

app.post('/user', async (req, res) => {
  const span = tracer.startSpan('frontend.create_user');
  span.setAttribute('user.name', req.body.name);
  
  try {
    const response = await axios.post(`${BACKEND_URL}/api/users`, req.body);
    res.status(201).json(response.data);
  } catch (error) {
    span.recordException(error);
    res.status(error.response?.status || 500).json({ 
      error: error.response?.data?.error || error.message 
    });
  } finally {
    span.end();
  }
});

app.post('/order', async (req, res) => {
  const span = tracer.startSpan('frontend.create_order');
  span.setAttribute('order.user_id', req.body.user_id);
  span.setAttribute('order.product', req.body.product);
  
  try {
    span.addEvent('Creating order');
    const response = await axios.post(`${BACKEND_URL}/api/orders`, req.body);
    span.addEvent('Order created successfully');
    res.status(201).json(response.data);
  } catch (error) {
    span.recordException(error);
    span.setStatus({ code: 2, message: error.message });
    res.status(error.response?.status || 500).json({ 
      error: error.response?.data?.error || error.message 
    });
  } finally {
    span.end();
  }
});

app.get('/slow', async (req, res) => {
  const span = tracer.startSpan('frontend.slow_endpoint');
  
  try {
    const response = await axios.get(`${BACKEND_URL}/api/slow`);
    res.json(response.data);
  } catch (error) {
    span.recordException(error);
    res.status(500).json({ error: error.message });
  } finally {
    span.end();
  }
});

app.listen(PORT, () => {
  console.log(`Frontend service listening on port ${PORT}`);
  console.log(`Jaeger UI: http://localhost:16686`);
});
```

5. **–°–æ–∑–¥–∞–π grafana-datasources.yml**:
```yaml
apiVersion: 1

datasources:
  - name: Jaeger
    type: jaeger
    access: proxy
    url: http://jaeger:16686
    isDefault: true
    editable: true
    jsonData:
      tracesToLogs:
        datasourceUid: 'loki'
        tags: ['trace_id']
        
  - name: Prometheus
    type: prometheus
    access: proxy
    url: http://prometheus:9090
    editable: true
    jsonData:
      httpMethod: POST
      exemplarTraceIdDestinations:
        - name: trace_id
          datasourceUid: 'jaeger'
```

6. **–ó–∞–ø—É—Å—Ç–∏ –∏ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–π**:
```bash
# –°–æ–±–µ—Ä–∏ –∏ –∑–∞–ø—É—Å—Ç–∏
docker-compose up --build -d

# –ü—Ä–æ–≤–µ—Ä—å —Å—Ç–∞—Ç—É—Å
docker-compose ps

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–π —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
curl -X POST http://localhost:8080/user \
  -H "Content-Type: application/json" \
  -d '{"name": "John Doe", "email": "john@example.com"}'

curl -X POST http://localhost:8080/user \
  -H "Content-Type: application/json" \
  -d '{"name": "Jane Smith", "email": "jane@example.com"}'

# –°–æ–∑–¥–∞–π –Ω–µ—Å–∫–æ–ª—å–∫–æ –∑–∞–∫–∞–∑–æ–≤ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç—Ä–µ–π—Å–æ–≤
for i in {1..10}; do
  curl -X POST http://localhost:8080/order \
    -H "Content-Type: application/json" \
    -d "{\"user_id\": 1, \"product\": \"Product $i\", \"amount\": $((RANDOM % 100 + 10))}"
  sleep 1
done

# –¢–µ—Å—Ç slow endpoint
curl http://localhost:8080/slow
```

7. **–û—Ç–∫—Ä–æ–π Jaeger UI –∏ –∏—Å—Å–ª–µ–¥—É–π —Ç—Ä–µ–π—Å—ã**:
```bash
# Jaeger UI
open http://localhost:16686

# Grafana
open http://localhost:3000

# Frontend app
open http://localhost:8080
```

8. **–°–æ–∑–¥–∞–π load generator –¥–ª—è —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö —Ç—Ä–µ–π—Å–æ–≤** (`load_test.sh`):
```bash
#!/bin/bash

echo "Starting load test..."

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–ª—É—á–∞–π–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
generate_load() {
  local endpoint=$1
  local method=$2
  local data=$3
  
  if [ "$method" == "GET" ]; then
    curl -s "$endpoint" > /dev/null
  else
    curl -s -X POST "$endpoint" \
      -H "Content-Type: application/json" \
      -d "$data" > /dev/null
  fi
}

# –ë–µ—Å–∫–æ–Ω–µ—á–Ω—ã–π —Ü–∏–∫–ª –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç—Ä–∞—Ñ–∏–∫–∞
while true; do
  # 60% GET users
  if [ $((RANDOM % 10)) -lt 6 ]; then
    generate_load "http://localhost:8080/users" "GET"
  fi
  
  # 20% GET specific user
  if [ $((RANDOM % 10)) -lt 2 ]; then
    user_id=$((RANDOM % 5 + 1))
    generate_load "http://localhost:8080/user/$user_id" "GET"
  fi
  
  # 15% Create order
  if [ $((RANDOM % 20)) -lt 3 ]; then
    user_id=$((RANDOM % 5 + 1))
    product="Product-$RANDOM"
    amount=$((RANDOM % 100 + 10))
    generate_load "http://localhost:8080/order" "POST" \
      "{\"user_id\": $user_id, \"product\": \"$product\", \"amount\": $amount}"
  fi
  
  # 5% Slow endpoint
  if [ $((RANDOM % 20)) -lt 1 ]; then
    generate_load "http://localhost:8080/slow" "GET"
  fi
  
  # –°–ª—É—á–∞–π–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
  sleep $(awk -v min=0.1 -v max=2 'BEGIN{srand(); print min+rand()*(max-min)}')
done
```

–ó–∞–ø—É—Å–∫ load test:
```bash
chmod +x load_test.sh
./load_test.sh &
```

### üöÄ –ë–æ–Ω—É—Å (–Ω–æ–≤–æ–µ)

**1. –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Ç—Ä–µ–π—Å–æ–≤ —Å –ª–æ–≥–∞–º–∏**:

–û–±–Ω–æ–≤–∏ `demo-app/backend/app.py` –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —Å trace_id:
```python
import logging
from opentelemetry import trace

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —Å trace context
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - [trace_id=%(otelTraceID)s span_id=%(otelSpanID)s] - %(message)s'
)

class TraceContextFilter(logging.Filter):
    """Add trace context to log records"""
    def filter(self, record):
        span = trace.get_current_span()
        if span:
            ctx = span.get_span_context()
            record.otelTraceID = format(ctx.trace_id, '032x') if ctx.trace_id else ''
            record.otelSpanID = format(ctx.span_id, '016x') if ctx.span_id else ''
        else:
            record.otelTraceID = ''
            record.otelSpanID = ''
        return True

logger = logging.getLogger(__name__)
logger.addFilter(TraceContextFilter())

# –ò—Å–ø–æ–ª—å–∑—É–π –≤ –∫–æ–¥–µ
@app.route('/api/users', methods=['GET'])
def get_users():
    logger.info("Fetching all users")
    # ... –∫–æ–¥
```

**2. Custom span attributes –∏ events**:
```python
@app.route('/api/complex-operation', methods=['POST'])
def complex_operation():
    with tracer.start_as_current_span("complex_operation") as span:
        # Business metrics
        span.set_attribute("business.operation_type", "purchase")
        span.set_attribute("business.customer_tier", "premium")
        span.set_attribute("business.discount_applied", True)
        
        # Performance metrics
        span.set_attribute("cache.enabled", True)
        span.set_attribute("db.pool_size", 10)
        
        # –°–æ–±—ã—Ç–∏—è —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        span.add_event("validation_started", {
            "validator.version": "1.0",
            "rules.count": 5
        })
        
        # –°–∏–º—É–ª–∏—Ä—É–µ–º —Ä–∞–±–æ—Ç—É
        time.sleep(0.1)
        
        span.add_event("validation_completed", {
            "validation.passed": True,
            "rules.failed": 0
        })
        
        # Exception handling
        try:
            risky_operation()
        except Exception as e:
            span.record_exception(e)
            span.set_status(trace.Status(trace.StatusCode.ERROR, str(e)))
            raise
        
        return jsonify({"status": "success"})
```

**3. Grafana Tempo (–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ Jaeger)**:

–î–æ–±–∞–≤—å –≤ `docker-compose.yml`:
```yaml
  tempo:
    image: grafana/tempo:latest
    container_name: tempo
    command: ["-config.file=/etc/tempo.yml"]
    volumes:
      - ./tempo.yml:/etc/tempo.yml
      - tempo-data:/tmp/tempo
    ports:
      - "3200:3200"   # Tempo HTTP
      - "4317:4317"   # OTLP gRPC
      - "4318:4318"   # OTLP HTTP
    restart: unless-stopped

volumes:
  tempo-data:
```

`tempo.yml`:
```yaml
server:
  http_listen_port: 3200

distributor:
  receivers:
    otlp:
      protocols:
        grpc:
          endpoint: 0.0.0.0:4317
        http:
          endpoint: 0.0.0.0:4318

ingester:
  max_block_duration: 5m

compactor:
  compaction:
    block_retention: 168h  # 7 days

storage:
  trace:
    backend: local
    local:
      path: /tmp/tempo/blocks
    wal:
      path: /tmp/tempo/wal

query_frontend:
  search:
    enabled: true
```

**4. Exemplars - —Å–≤—è–∑—å –º–µ—Ç—Ä–∏–∫ –∏ —Ç—Ä–µ–π—Å–æ–≤**:

–í Prometheus metrics –¥–æ–±–∞–≤—å exemplars:
```python
from prometheus_client import Counter, Histogram
from opentelemetry import trace

request_duration = Histogram(
    'http_request_duration_seconds',
    'HTTP request duration',
    ['method', 'endpoint']
)

@app.route('/api/endpoint')
def endpoint():
    span = trace.get_current_span()
    ctx = span.get_span_context()
    trace_id = format(ctx.trace_id, '032x')
    
    with request_duration.labels(method='GET', endpoint='/api/endpoint').time():
        # Add exemplar
        request_duration.labels(method='GET', endpoint='/api/endpoint').observe(
            0.5,
            exemplar={'trace_id': trace_id}
        )
        
        return jsonify({"result": "ok"})
```

**5. Service dependency graph automation**:

–°–æ–∑–¥–∞–π `dependency_analyzer.py`:
```python
#!/usr/bin/env python3
"""
–ê–Ω–∞–ª–∏–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π —Å–µ—Ä–≤–∏—Å–æ–≤ –∏–∑ Jaeger
"""
import requests
from collections import defaultdict
import json

JAEGER_URL = "http://localhost:16686"

def get_services():
    """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ —Å–µ—Ä–≤–∏—Å–æ–≤"""
    response = requests.get(f"{JAEGER_URL}/api/services")
    return response.json()['data']

def get_dependencies():
    """–ü–æ–ª—É—á–∏—Ç—å –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —Å–µ—Ä–≤–∏—Å–æ–≤"""
    # lookback in microseconds (24 hours)
    lookback = 24 * 60 * 60 * 1000000
    
    response = requests.get(
        f"{JAEGER_URL}/api/dependencies",
        params={'endTs': None, 'lookback': lookback}
    )
    return response.json()['data']

def build_graph():
    """–ü–æ—Å—Ç—Ä–æ–∏—Ç—å –≥—Ä–∞—Ñ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π"""
    dependencies = get_dependencies()
    
    graph = defaultdict(list)
    for dep in dependencies:
        parent = dep['parent']
        child = dep['child']
        call_count = dep['callCount']
        
        graph[parent].append({
            'service': child,
            'calls': call_count
        })
    
    return graph

def print_graph(graph):
    """–í—ã–≤–µ—Å—Ç–∏ –≥—Ä–∞—Ñ"""
    print("\n=== Service Dependency Graph ===\n")
    
    for service, dependencies in sorted(graph.items()):
        print(f"{service}")
        for dep in dependencies:
            print(f"  ‚îî‚îÄ> {dep['service']} ({dep['calls']} calls)")
        print()

def export_mermaid(graph):
    """–≠–∫—Å–ø–æ—Ä—Ç –≤ Mermaid –¥–∏–∞–≥—Ä–∞–º–º—É"""
    print("\n=== Mermaid Diagram ===\n")
    print("```mermaid")
    print("graph LR")
    
    for service, dependencies in graph.items():
        for dep in dependencies:
            # –ë–µ–∑–æ–ø–∞—Å–Ω—ã–µ –∏–º–µ–Ω–∞ –¥–ª—è Mermaid
            safe_parent = service.replace('-', '_')
            safe_child = dep['service'].replace('-', '_')
            print(f"    {safe_parent}[{service}] --> {safe_child}[{dep['service']}]")
    
    print("```\n")

if __name__ == "__main__":
    print("Analyzing service dependencies from Jaeger...")
    
    services = get_services()
    print(f"\nFound {len(services)} services:")
    for svc in services:
        print(f"  - {svc}")
    
    graph = build_graph()
    print_graph(graph)
    export_mermaid(graph)
```

**6. Continuous Profiling –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è**:

–î–æ–±–∞–≤—å Pyroscope –¥–ª—è continuous profiling:
```yaml
  pyroscope:
    image: grafana/pyroscope:latest
    container_name: pyroscope
    ports:
      - "4040:4040"
    volumes:
      - pyroscope-data:/var/lib/pyroscope
    restart: unless-stopped

volumes:
  pyroscope-data:
```

–í Python app:
```python
import pyroscope

pyroscope.configure(
    application_name="backend-service",
    server_address="http://pyroscope:4040",
    tags={
        "environment": "development",
        "service": "backend"
    }
)
```

**7. Error tracking —Å Sentry –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è**:
```python
import sentry_sdk
from sentry_sdk.integrations.flask import FlaskIntegration
from opentelemetry import trace

sentry_sdk.init(
    dsn="your-sentry-dsn",
    integrations=[FlaskIntegration()],
    traces_sample_rate=1.0,
    before_send=lambda event, hint: attach_trace_context(event)
)

def attach_trace_context(event):
    """–î–æ–±–∞–≤–∏—Ç—å trace context –≤ Sentry events"""
    span = trace.get_current_span()
    if span:
        ctx = span.get_span_context()
        event['contexts']['trace'] = {
            'trace_id': format(ctx.trace_id, '032x'),
            'span_id': format(ctx.span_id, '016x')
        }
    return event
```

**8. Custom dashboard –≤ Grafana**:

JSON dashboard (`grafana-tracing-dashboard.json`):
```json
{
  "dashboard": {
    "title": "Distributed Tracing Overview",
    "panels": [
      {
        "id": 1,
        "title": "Request Rate by Service",
        "type": "timeseries",
        "targets": [
          {
            "expr": "sum(rate(http_requests_total[5m])) by (service)"
          }
        ]
      },
      {
        "id": 2,
        "title": "P95 Latency by Service",
        "type": "timeseries",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))"
          }
        ]
      },
      {
        "id": 3,
        "title": "Error Rate",
        "type": "stat",
        "targets": [
          {
            "expr": "sum(rate(http_requests_total{status=~\"5..\"}[5m])) / sum(rate(http_requests_total[5m]))"
          }
        ]
      },
      {
        "id": 4,
        "title": "Trace Search",
        "type": "jaeger",
        "datasource": "Jaeger"
      }
    ]
  }
}
```

---

## –ò—Ç–æ–≥–∏ –º–æ–¥—É–ª—è 6

–ü–æ—Å–ª–µ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏—è —ç—Ç–æ–≥–æ –º–æ–¥—É–ª—è —Ç—ã –¥–æ–ª–∂–µ–Ω —É–º–µ—Ç—å:

‚úÖ –ü–æ–Ω–∏–º–∞—Ç—å –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ distributed tracing (trace, span, context)
‚úÖ –ù–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å Jaeger/Tempo –¥–ª—è —Å–±–æ—Ä–∞ —Ç—Ä–µ–π—Å–æ–≤
‚úÖ –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è —Å OpenTelemetry
‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å auto –∏ manual instrumentation
‚úÖ –ù–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å sampling —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
‚úÖ –ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç—Ä–µ–π—Å—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ bottleneck'–æ–≤
‚úÖ –°–≤—è–∑—ã–≤–∞—Ç—å —Ç—Ä–µ–π—Å—ã —Å –ª–æ–≥–∞–º–∏ –∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏
‚úÖ –°—Ç—Ä–æ–∏—Ç—å service dependency graphs
‚úÖ –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å —Å error tracking
‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å continuous profiling

**–ö–ª—é—á–µ–≤—ã–µ –≤—ã–≤–æ–¥—ã:**
1. Tracing –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –ö–ê–ö —Ä–∞–±–æ—Ç–∞–µ—Ç —Å–∏—Å—Ç–µ–º–∞ (–ø—É—Ç—å –∑–∞–ø—Ä–æ—Å–∞)
2. –ò—Å–ø–æ–ª—å–∑—É–π tail sampling –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤–∞–∂–Ω—ã—Ö —Ç—Ä–µ–π—Å–æ–≤
3. –í—Å–µ–≥–¥–∞ –ø–µ—Ä–µ–¥–∞–≤–∞–π trace context –º–µ–∂–¥—É —Å–µ—Ä–≤–∏—Å–∞–º–∏
4. –î–æ–±–∞–≤–ª—è–π –ø–æ–ª–µ–∑–Ω—ã–µ attributes –∏ events
5. –ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–π tracing —Å metrics –∏ logs
6. –ò—Å–ø–æ–ª—å–∑—É–π semantic conventions
7. –†–µ–≥—É–ª—è—Ä–Ω–æ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–π service dependencies

**–°–ª–µ–¥—É—é—â–∏–π –º–æ–¥—É–ª—å:** Infrastructure as Code –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ (–∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è deployment monitoring stack)


---
## –ú–æ–¥—É–ª—å 7: Infrastructure as Code –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ (35 –º–∏–Ω—É—Ç)

### üéØ –ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞

**IaC –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ - –∑–∞—á–µ–º:**

```
–ü—Ä–æ–±–ª–µ–º–∞: –†—É—á–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
‚ùå –î–æ–ª–≥–æ (—á–∞—Å—ã –Ω–∞ setup)
‚ùå –û—à–∏–±–∫–∏ (—á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–π —Ñ–∞–∫—Ç–æ—Ä)
‚ùå –ù–µ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ
‚ùå –°–ª–æ–∂–Ω–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å
‚ùå –ù–µ—Ç version control

–†–µ—à–µ–Ω–∏–µ: Infrastructure as Code
‚úÖ –ë—ã—Å—Ç—Ä–æ (–º–∏–Ω—É—Ç—ã –Ω–∞ deploy)
‚úÖ –ù–∞–¥–µ–∂–Ω–æ (–∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è)
‚úÖ –í–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ (–∏–¥–µ–Ω—Ç–∏—á–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è)
‚úÖ –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ (–ª–µ–≥–∫–æ –¥–æ–±–∞–≤–∏—Ç—å –Ω–æ–≤—ã–µ —Å–µ—Ä–≤–∏—Å—ã)
‚úÖ Version control (Git history)
```

**–û—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã:**

```
1. Configuration Management:
   - Ansible
   - Chef
   - Puppet
   - SaltStack

2. Container Orchestration:
   - Docker Compose
   - Kubernetes (Helm)
   - Docker Swarm

3. Infrastructure Provisioning:
   - Terraform
   - Pulumi
   - CloudFormation (AWS)

4. GitOps:
   - ArgoCD
   - Flux
   - Jenkins X
```

**Terraform –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞:**

```
Provider Support:
- Prometheus (rules, alertmanager config)
- Grafana (dashboards, data sources, folders)
- PagerDuty (services, escalation policies)
- Datadog (monitors, dashboards)
- AWS CloudWatch (alarms, dashboards)

–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:
- –î–µ–∫–ª–∞—Ä–∞—Ç–∏–≤–Ω—ã–π —Å–∏–Ω—Ç–∞–∫—Å–∏—Å
- State management
- Plan/Apply workflow
- Module reusability
```

**Ansible –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞:**

```
–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
- –£—Å—Ç–∞–Ω–æ–≤–∫–∞ monitoring agents
- –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è exporters
- Deployment monitoring stack
- –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ dashboards

–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:
- Agentless (SSH)
- –ü—Ä–æ—Å—Ç–æ–π YAML —Å–∏–Ω—Ç–∞–∫—Å–∏—Å
- –ë–æ–ª—å—à–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ modules
- Idempotent operations
```

**Helm –¥–ª—è Kubernetes:**

```
–û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ charts:
- prometheus-community/kube-prometheus-stack
- grafana/grafana
- grafana/loki-stack
- jaegertracing/jaeger
- elastic/elasticsearch

–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:
- Templating
- Values override
- Release management
- Dependency management
```

**GitOps workflow:**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Git    ‚îÇ (Source of Truth)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ
     ‚îÇ Push
     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  CI/CD   ‚îÇ (Validation, Testing)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ
     ‚îÇ Deploy
     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Cluster  ‚îÇ (Auto-sync)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Principles:
1. Declarative configuration
2. Version controlled
3. Automated deployment
4. Self-healing
```

**Prometheus Configuration Management:**

yaml

```yaml
# prometheus.yml –∫–∞–∫ –∫–æ–¥
global:
  scrape_interval: 15s
  evaluation_interval: 15s
  external_labels:
    cluster: {{ cluster_name }}
    environment: {{ environment }}

# Template –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
alerting:
  alertmanagers:
  - static_configs:
    - targets: 
      {{ range .AlertmanagerTargets }}
      - {{ . }}
      {{ end }}

scrape_configs:
  {{ range .Jobs }}
  - job_name: {{ .Name }}
    static_configs:
      - targets: {{ .Targets }}
        labels: {{ .Labels }}
  {{ end }}
```

**Grafana Dashboard as Code:**

json

```json
{
  "dashboard": {
    "title": "{{ .Title }}",
    "tags": {{ .Tags | toJson }},
    "timezone": "browser",
    "panels": [
      {{ range .Panels }}
      {
        "id": {{ .ID }},
        "title": "{{ .Title }}",
        "type": "{{ .Type }}",
        "targets": [
          {
            "expr": "{{ .Query }}",
            "legendFormat": "{{ .Legend }}"
          }
        ]
      }{{ if not (last $.Panels .) }},{{ end }}
      {{ end }}
    ]
  }
}
```

**Alert Rules as Code:**

yaml

````yaml
# alerts.yml template
groups:
{{ range .AlertGroups }}
  - name: {{ .Name }}
    interval: {{ .Interval }}
    rules:
    {{ range .Rules }}
    - alert: {{ .Name }}
      expr: |
        {{ .Expression }}
      for: {{ .For }}
      labels:
        severity: {{ .Severity }}
        team: {{ .Team }}
      annotations:
        summary: {{ .Summary }}
        description: {{ .Description }}
        runbook: {{ .Runbook }}
    {{ end }}
{{ end }}
````

**Monitoring Stack Components:**
```
Full Stack:
‚îú‚îÄ‚îÄ Metrics Collection
‚îÇ   ‚îú‚îÄ‚îÄ Prometheus
‚îÇ   ‚îú‚îÄ‚îÄ Node Exporter
‚îÇ   ‚îú‚îÄ‚îÄ Blackbox Exporter
‚îÇ   ‚îî‚îÄ‚îÄ Custom Exporters
‚îú‚îÄ‚îÄ Logs Collection
‚îÇ   ‚îú‚îÄ‚îÄ Loki
‚îÇ   ‚îú‚îÄ‚îÄ Promtail
‚îÇ   ‚îî‚îÄ‚îÄ Fluentd/Fluent Bit
‚îú‚îÄ‚îÄ Tracing
‚îÇ   ‚îú‚îÄ‚îÄ Jaeger/Tempo
‚îÇ   ‚îî‚îÄ‚îÄ OpenTelemetry Collector
‚îú‚îÄ‚îÄ Visualization
‚îÇ   ‚îî‚îÄ‚îÄ Grafana
‚îú‚îÄ‚îÄ Alerting
‚îÇ   ‚îî‚îÄ‚îÄ Alertmanager
‚îî‚îÄ‚îÄ Notification
    ‚îú‚îÄ‚îÄ Slack
    ‚îú‚îÄ‚îÄ PagerDuty
    ‚îî‚îÄ‚îÄ Email
```

**Directory Structure (best practices):**
````
monitoring-infrastructure/
‚îú‚îÄ‚îÄ terraform/
‚îÇ   ‚îú‚îÄ‚îÄ modules/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prometheus/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ grafana/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ alertmanager/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ loki/
‚îÇ   ‚îú‚îÄ‚îÄ environments/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dev/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ staging/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ prod/
‚îÇ   ‚îî‚îÄ‚îÄ main.tf
‚îú‚îÄ‚îÄ ansible/
‚îÇ   ‚îú‚îÄ‚îÄ playbooks/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ install-prometheus.yml
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ configure-exporters.yml
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ deploy-dashboards.yml
‚îÇ   ‚îú‚îÄ‚îÄ roles/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prometheus/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ grafana/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ node-exporter/
‚îÇ   ‚îî‚îÄ‚îÄ inventory/
‚îú‚îÄ‚îÄ kubernetes/
‚îÇ   ‚îú‚îÄ‚îÄ helm/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ values-dev.yaml
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ values-staging.yaml
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ values-prod.yaml
‚îÇ   ‚îî‚îÄ‚îÄ manifests/
‚îú‚îÄ‚îÄ docker/
‚îÇ   ‚îú‚îÄ‚îÄ docker-compose.yml
‚îÇ   ‚îî‚îÄ‚îÄ docker-compose.override.yml
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ prometheus/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prometheus.yml
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ alerts/
‚îÇ   ‚îú‚îÄ‚îÄ grafana/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ dashboards/
‚îÇ   ‚îî‚îÄ‚îÄ alertmanager/
‚îÇ       ‚îî‚îÄ‚îÄ alertmanager.yml
‚îî‚îÄ‚îÄ scripts/
    ‚îú‚îÄ‚îÄ deploy.sh
    ‚îú‚îÄ‚îÄ backup.sh
    ‚îî‚îÄ‚îÄ validate.sh
````

**Validation & Testing:**

bash

````bash
# Prometheus config validation
promtool check config prometheus.yml
promtool check rules alerts.yml

# Alert testing
promtool test rules test-alerts.yml

# Grafana dashboard validation
grafana-cli admin validate-dashboard dashboard.json

# Terraform validation
terraform validate
terraform plan

# Ansible syntax check
ansible-playbook --syntax-check playbook.yml
ansible-lint playbook.yml
````

**Backup & Disaster Recovery:**
````
–ß—Ç–æ –±—ç–∫–∞–ø–∏—Ç—å:
1. ‚úÖ Prometheus TSDB (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –¥–∞–Ω–Ω—ã–µ ephemeral)
2. ‚úÖ Grafana database (dashboards, users, settings)
3. ‚úÖ Alertmanager data (silences, notification log)
4. ‚úÖ Configuration files (prometheus.yml, alerts, etc)
5. ‚úÖ Custom exporters config

–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã:
- Prometheus: snapshots API
- Grafana: grafana-backup tool, API export
- Velero: Kubernetes backup
- Restic: filesystem backup
````

**Environment Management:**

yaml

````yaml
# –†–∞–∑–Ω—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è –æ–∫—Ä—É–∂–µ–Ω–∏–π
Dev:
  retention: 7d
  replicas: 1
  resources: small
  scrape_interval: 30s

Staging:
  retention: 14d
  replicas: 2
  resources: medium
  scrape_interval: 15s

Production:
  retention: 30d
  replicas: 3
  resources: large
  scrape_interval: 15s
  high_availability: true
````

**Secrets Management:**
````
Options:
1. HashiCorp Vault
   - Centralized secrets
   - Dynamic credentials
   - Audit logging

2. Kubernetes Secrets
   - Native K8s
   - External Secrets Operator

3. AWS Secrets Manager
   - Managed service
   - Rotation support

4. Sealed Secrets
   - GitOps friendly
   - Encrypted in Git

Best Practice: Never commit secrets to Git!
````

**CI/CD Pipeline –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞:**

yaml

```yaml
# .github/workflows/monitoring.yml
name: Deploy Monitoring Stack

on:
  push:
    branches: [main]
    paths:
      - 'monitoring/**'

jobs:
  validate:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Validate Prometheus Config
        run: |
          docker run --rm -v $PWD:/prometheus prom/prometheus:latest \
            promtool check config /prometheus/config/prometheus.yml
      
      - name: Test Alert Rules
        run: |
          docker run --rm -v $PWD:/prometheus prom/prometheus:latest \
            promtool test rules /prometheus/tests/alerts-test.yml
      
      - name: Validate Grafana Dashboards
        run: |
          for file in grafana/dashboards/*.json; do
            jq empty "$file" || exit 1
          done

  deploy-dev:
    needs: validate
    runs-on: ubuntu-latest
    environment: dev
    steps:
      - name: Deploy to Dev
        run: |
          helm upgrade --install monitoring ./helm \
            --values values-dev.yaml \
            --namespace monitoring \
            --create-namespace

  deploy-prod:
    needs: deploy-dev
    runs-on: ubuntu-latest
    environment: production
    steps:
      - name: Deploy to Production
        run: |
          helm upgrade --install monitoring ./helm \
            --values values-prod.yaml \
            --namespace monitoring
```

### üíª –ó–∞–¥–∞–Ω–∏–µ

–°–æ–∑–¥–∞–π –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—É—é IaC –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞:

1. **–°–æ–∑–¥–∞–π Terraform –ø—Ä–æ–µ–∫—Ç –¥–ª—è Grafana**:

`terraform/main.tf`:

hcl

```hcl
terraform {
  required_version = ">= 1.0"
  
  required_providers {
    grafana = {
      source  = "grafana/grafana"
      version = "~> 2.9.0"
    }
  }
}

provider "grafana" {
  url  = var.grafana_url
  auth = var.grafana_auth
}

# Data source –¥–ª—è Prometheus
resource "grafana_data_source" "prometheus" {
  type = "prometheus"
  name = "Prometheus"
  url  = var.prometheus_url
  
  is_default = true
  
  json_data_encoded = jsonencode({
    httpMethod    = "POST"
    timeInterval  = "30s"
  })
}

# Data source –¥–ª—è Loki
resource "grafana_data_source" "loki" {
  type = "loki"
  name = "Loki"
  url  = var.loki_url
  
  json_data_encoded = jsonencode({
    maxLines = 1000
  })
}

# Folder –¥–ª—è dashboards
resource "grafana_folder" "monitoring" {
  title = "Monitoring"
}

resource "grafana_folder" "applications" {
  title = "Applications"
}

# Dashboard - Node Exporter
resource "grafana_dashboard" "node_exporter" {
  folder      = grafana_folder.monitoring.id
  config_json = file("${path.module}/dashboards/node-exporter.json")
}

# Dashboard - Application Metrics
resource "grafana_dashboard" "application" {
  folder      = grafana_folder.applications.id
  config_json = templatefile("${path.module}/dashboards/application.json.tpl", {
    datasource = grafana_data_source.prometheus.name
    environment = var.environment
  })
}

# Alert notification channel - Slack
resource "grafana_contact_point" "slack" {
  name = "Slack Alerts"
  
  slack {
    url  = var.slack_webhook_url
    text = templatefile("${path.module}/templates/slack-message.tpl", {})
  }
}

# Alert notification channel - PagerDuty
resource "grafana_contact_point" "pagerduty" {
  name = "PagerDuty"
  
  pagerduty {
    integration_key = var.pagerduty_key
    severity        = "critical"
  }
}

# Notification policy
resource "grafana_notification_policy" "main" {
  group_by      = ["alertname", "grafana_folder"]
  group_wait    = "10s"
  group_interval = "5m"
  repeat_interval = "4h"
  
  policy {
    matcher {
      label = "severity"
      match = "="
      value = "critical"
    }
    contact_point = grafana_contact_point.pagerduty.name
    continue      = true
  }
  
  policy {
    matcher {
      label = "severity"
      match = "="
      value = "warning"
    }
    contact_point = grafana_contact_point.slack.name
  }
}

# Alert rule - High CPU
resource "grafana_rule_group" "infrastructure" {
  name             = "Infrastructure Alerts"
  folder_uid       = grafana_folder.monitoring.uid
  interval_seconds = 60
  
  rule {
    name      = "HighCPUUsage"
    condition = "C"
    
    data {
      ref_id = "A"
      
      relative_time_range {
        from = 600
        to   = 0
      }
      
      datasource_uid = grafana_data_source.prometheus.uid
      model = jsonencode({
        expr         = "100 - (avg(irate(node_cpu_seconds_total{mode=\"idle\"}[5m])) * 100)"
        refId        = "A"
        intervalMs   = 1000
        maxDataPoints = 43200
      })
    }
    
    data {
      ref_id = "B"
      
      relative_time_range {
        from = 0
        to   = 0
      }
      
      datasource_uid = "__expr__"
      model = jsonencode({
        expression = "A"
        reducer    = "last"
        refId      = "B"
        type       = "reduce"
      })
    }
    
    data {
      ref_id = "C"
      
      relative_time_range {
        from = 0
        to   = 0
      }
      
      datasource_uid = "__expr__"
      model = jsonencode({
        expression = "B > 80"
        refId      = "C"
        type       = "threshold"
      })
    }
    
    no_data_state  = "NoData"
    exec_err_state = "Error"
    for            = "5m"
    
    annotations = {
      summary     = "High CPU usage detected"
      description = "CPU usage is above 80%"
      runbook_url = "https://runbooks.example.com/high-cpu"
    }
    
    labels = {
      severity = "warning"
      team     = "infrastructure"
    }
  }
}

# Organization settings
resource "grafana_organization_preferences" "main" {
  theme            = "dark"
  home_dashboard_uid = grafana_dashboard.node_exporter.uid
  timezone         = "UTC"
}

# Team
resource "grafana_team" "infrastructure" {
  name  = "Infrastructure Team"
  email = "infra@example.com"
}

# Service Account –¥–ª—è API access
resource "grafana_service_account" "automation" {
  name = "automation"
  role = "Admin"
}

resource "grafana_service_account_token" "automation" {
  name               = "automation-token"
  service_account_id = grafana_service_account.automation.id
}
```

`terraform/variables.tf`:

hcl

```hcl
variable "grafana_url" {
  description = "Grafana URL"
  type        = string
  default     = "http://localhost:3000"
}

variable "grafana_auth" {
  description = "Grafana auth (admin:password)"
  type        = string
  sensitive   = true
}

variable "prometheus_url" {
  description = "Prometheus URL"
  type        = string
  default     = "http://prometheus:9090"
}

variable "loki_url" {
  description = "Loki URL"
  type        = string
  default     = "http://loki:3100"
}

variable "environment" {
  description = "Environment name"
  type        = string
  default     = "development"
}

variable "slack_webhook_url" {
  description = "Slack webhook URL"
  type        = string
  sensitive   = true
}

variable "pagerduty_key" {
  description = "PagerDuty integration key"
  type        = string
  sensitive   = true
}
```

`terraform/outputs.tf`:

hcl

```hcl
output "prometheus_datasource_uid" {
  description = "Prometheus data source UID"
  value       = grafana_data_source.prometheus.uid
}

output "loki_datasource_uid" {
  description = "Loki data source UID"
  value       = grafana_data_source.loki.uid
}

output "automation_token" {
  description = "Automation service account token"
  value       = grafana_service_account_token.automation.key
  sensitive   = true
}

output "dashboard_urls" {
  description = "URLs of created dashboards"
  value = {
    node_exporter = "${var.grafana_url}/d/${grafana_dashboard.node_exporter.uid}"
    application   = "${var.grafana_url}/d/${grafana_dashboard.application.uid}"
  }
}
```

`terraform/dashboards/application.json.tpl`:

json

```json
{
  "dashboard": {
    "title": "Application Metrics - ${environment}",
    "tags": ["application", "${environment}"],
    "timezone": "browser",
    "panels": [
      {
        "id": 1,
        "title": "Request Rate",
        "type": "timeseries",
        "gridPos": {
          "h": 8,
          "w": 12,
          "x": 0,
          "y": 0
        },
        "targets": [
          {
            "datasource": "${datasource}",
            "expr": "sum(rate(http_requests_total{environment=\"${environment}\"}[5m])) by (service)",
            "legendFormat": "{{service}}"
          }
        ]
      },
      {
        "id": 2,
        "title": "Error Rate",
        "type": "stat",
        "gridPos": {
          "h": 8,
          "w": 12,
          "x": 12,
          "y": 0
        },
        "targets": [
          {
            "datasource": "${datasource}",
            "expr": "sum(rate(http_requests_total{environment=\"${environment}\",status=~\"5..\"}[5m])) / sum(rate(http_requests_total{environment=\"${environment}\"}[5m]))",
            "legendFormat": "Error Rate"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "percentunit",
            "thresholds": {
              "steps": [
                {"value": 0, "color": "green"},
                {"value": 0.01, "color": "yellow"},
                {"value": 0.05, "color": "red"}
              ]
            }
          }
        }
      }
    ]
  }
}
```

2. **–°–æ–∑–¥–∞–π Ansible playbook –¥–ª—è deployment**:

`ansible/inventory/hosts.ini`:

ini

```ini
[monitoring_servers]
monitoring-01 ansible_host=192.168.1.10 ansible_user=ubuntu
monitoring-02 ansible_host=192.168.1.11 ansible_user=ubuntu

[app_servers]
app-01 ansible_host=192.168.1.20 ansible_user=ubuntu
app-02 ansible_host=192.168.1.21 ansible_user=ubuntu
app-03 ansible_host=192.168.1.22 ansible_user=ubuntu

[all:vars]
ansible_python_interpreter=/usr/bin/python3
environment=production
```

`ansible/playbooks/deploy-monitoring-stack.yml`:

yaml

````yaml
---
- name: Deploy Monitoring Stack
  hosts: monitoring_servers
  become: yes
  vars:
    prometheus_version: "2.48.0"
    grafana_version: "10.2.3"
    alertmanager_version: "0.26.0"
    node_exporter_version: "1.7.0"
    
  tasks:
    - name: Update apt cache
      apt:
        update_cache: yes
        cache_valid_time: 3600
    
    - name: Install prerequisites
      apt:
        name:
          - apt-transport-https
          - software-properties-common
          - wget
          - curl
          - tar
        state: present
    
    - name: Create monitoring user
      user:
        name: monitoring
        system: yes
        shell: /bin/false
        create_home: no
    
    - name: Deploy Prometheus
      include_role:
        name: prometheus
      vars:
        prometheus_config_template: "{{ playbook_dir }}/../config/prometheus/prometheus.yml.j2"
        prometheus_alerts_dir: "{{ playbook_dir }}/../config/prometheus/alerts"
    
    - name: Deploy Alertmanager
      include_role:
        name: alertmanager
      vars:
        alertmanager_config_template: "{{ playbook_dir }}/../config/alertmanager/alertmanager.yml.j2"
    
    - name: Deploy Grafana
      include_role:
        name: grafana
      vars:
        grafana_provisioning_dir: "{{ playbook_dir }}/../config/grafana/provisioning"

- name: Install Node Exporter on all servers
  hosts: all
  become: yes
  tasks:
    - name: Deploy Node Exporter
      include_role:
        name: node_exporter

- name: Configure Application Monitoring
  hosts: app_servers
  become: yes
  tasks:
    - name: Install application exporters
      include_role:
        name: app_exporter
````

`ansible/roles/prometheus/tasks/main.yml`:

yaml

````yaml
---
- name: Create Prometheus directories
  file:
    path: "{{ item }}"
    state: directory
    owner: monitoring
    group: monitoring
    mode: '0755'
  loop:
    - /etc/prometheus
    - /etc/prometheus/rules
    - /var/lib/prometheus

- name: Download Prometheus
  get_url:
    url: "https://github.com/prometheus/prometheus/releases/download/v{{ prometheus_version }}/prometheus-{{ prometheus_version }}.linux-amd64.tar.gz"
    dest: "/tmp/prometheus-{{ prometheus_version }}.tar.gz"

- name: Extract Prometheus
  unarchive:
    src: "/tmp/prometheus-{{ prometheus_version }}.tar.gz"
    dest: /tmp
    remote_src: yes

- name: Install Prometheus binaries
  copy:
    src: "/tmp/prometheus-{{ prometheus_version }}.linux-amd64/{{ item }}"
    dest: "/usr/local/bin/{{ item }}"
    owner: monitoring
    group: monitoring
    mode: '0755'
    remote_src: yes
  loop:
    - prometheus
    - promtool

- name: Copy Prometheus configuration
  template:
    src: "{{ prometheus_config_template }}"
    dest: /etc/prometheus/prometheus.yml
    owner: monitoring
    group: monitoring
    mode: '0644'
  notify: reload prometheus

- name: Copy alert rules
  copy:
    src: "{{ prometheus_alerts_dir }}/"
    dest: /etc/prometheus/rules/
    owner: monitoring
    group: monitoring
    mode: '0644'
  notify: reload prometheus

- name: Create Prometheus systemd service
  template:
    src: prometheus.service.j2
    dest: /etc/systemd/system/prometheus.service
    owner: root
    group: root
    mode: '0644'
  notify: restart prometheus

- name: Enable and start Prometheus
  systemd:
    name: prometheus
    enabled: yes
    state: started
    daemon_reload: yes
````

`ansible/roles/prometheus/templates/prometheus.service.j2`:

ini

```ini
[Unit]
Description=Prometheus
Wants=network-online.target
After=network-online.target

[Service]
User=monitoring
Group=monitoring
Type=simple
ExecStart=/usr/local/bin/prometheus \
  --config.file=/etc/prometheus/prometheus.yml \
  --storage.tsdb.path=/var/lib/prometheus \
  --web.console.templates=/etc/prometheus/consoles \
  --web.console.libraries=/etc/prometheus/console_libraries \
  --storage.tsdb.retention.time={{ prometheus_retention | default('30d') }} \
  --web.enable-lifecycle \
  --web.enable-admin-api

Restart=always
RestartSec=5

[Install]
WantedBy=multi-user.target
```

`ansible/roles/prometheus/handlers/main.yml`:

yaml

````yaml
---
- name: reload prometheus
  uri:
    url: "http://localhost:9090/-/reload"
    method: POST
  listen: reload prometheus

- name: restart prometheus
  systemd:
    name: prometheus
    state: restarted
  listen: restart prometheus
````

3. **–°–æ–∑–¥–∞–π Helm chart –¥–ª—è Kubernetes**:

`helm/monitoring/Chart.yaml`:

yaml

```yaml
apiVersion: v2
name: monitoring-stack
description: Complete monitoring stack for Kubernetes
type: application
version: 1.0.0
appVersion: "1.0"

dependencies:
  - name: kube-prometheus-stack
    version: "55.0.0"
    repository: "https://prometheus-community.github.io/helm-charts"
    condition: prometheus.enabled
    
  - name: loki-stack
    version: "2.9.11"
    repository: "https://grafana.github.io/helm-charts"
    condition: loki.enabled
    
  - name: jaeger
    version: "0.71.11"
    repository: "https://jaegertracing.github.io/helm-charts"
    condition: jaeger.enabled
```

`helm/monitoring/values.yaml`:

yaml

```yaml
# Global settings
global:
  environment: production
  clusterName: main-cluster

# Prometheus configuration
prometheus:
  enabled: true
  
kube-prometheus-stack:
  prometheus:
    prometheusSpec:
      retention: 30d
      retentionSize: "50GB"
      replicas: 2
      storageSpec:
        volumeClaimTemplate:
          spec:
            accessModes: ["ReadWriteOnce"]
            resources:
              requests:
                storage: 100Gi
      
      # Additional scrape configs
      additionalScrapeConfigs:
        - job_name: 'custom-app'
          kubernetes_sd_configs:
            - role: pod
          relabel_configs:
            - source_labels: [__meta_kubernetes_pod_label_app]
              regex: my-app
              action: keep
      
      # Remote write (–¥–ª—è long-term storage)
      remoteWrite:
        - url: http://thanos-receiver:19291/api/v1/receive
          queueConfig:
            capacity: 10000
            maxShards: 50
  
  # Alert rules
  additionalPrometheusRulesMap:
    custom-alerts:
      groups:
        - name: custom_application_alerts
          interval: 30s
          rules:
            - alert: ApplicationDown
              expr: up{job="my-app"} == 0
              for: 2m
              labels:
                severity: critical
                team: backend
              annotations:
                summary: "Application {{ $labels.instance }} is down"
                description: "Application has been down for more than 2 minutes"
  
  # Alertmanager
  alertmanager:
    config:
      global:
        resolve_timeout: 5m
        slack_api_url: {{ .Values.slack.webhookUrl }}
      
      route:
        group_by: ['alertname', 'cluster', 'service']
        group_wait: 10s
        group_interval: 5m
        repeat_interval: 4h
        receiver: 'default'
        
        routes:
          - match:
              severity: critical
            receiver: pagerduty
            continue: true
          
          - match:
              severity: warning
            receiver: slack
      
      receivers:
        - name: 'default'
          webhook_configs:
            - url: 'http://webhook-receiver:8080/webhook'
        
        - name: 'slack'
          slack_configs:
            - channel: '#alerts'
              title: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
              text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
        
        - name: 'pagerduty'
          pagerduty_configs:
            - service_key: {{ .Values.pagerduty.serviceKey }}
  
  # Grafana
  grafana:
    enabled: true
    adminPassword: {{ .Values.grafana.adminPassword }}
    
    persistence:
      enabled: true
      size: 10Gi
    
    dashboardProviders:
      dashboardproviders.yaml:
        apiVersion: 1
        providers:
          - name: 'default'
            orgId: 1
            folder: ''
            type: file
            disableDeletion: false
            editable: true
            options:
              path: /var/lib/grafana/dashboards/default
    
    dashboards:
      default:
        node-exporter:
          gnetId: 1860
          revision: 31
          datasource: Prometheus
        
        kubernetes-cluster:
          gnetId: 7249
          revision: 1
          datasource: Prometheus

# Loki configuration
loki:
  enabled: true

loki-stack:
  loki:
    persistence:
      enabled: true
      size: 50Gi
    
    config:
      limits_config:
        retention_period: 168h  # 7 days
      
      compactor:
        retention_enabled: true
  
  promtail:
    config:
      clients:
        - url: http://loki:3100/loki/api/v1/push

# Jaeger configuration
jaeger:
  enabled: true

jaeger:
  storage:
    type: elasticsearch
  
  elasticsearch:
    replicas: 3
    minimumMasterNodes: 2
```

`helm/monitoring/values-dev.yaml`:
```yaml
global:
  environment: development
  clusterName: dev-cluster

kube-prometheus-stack:
  prometheus:
    prometheusSpec:
      retention: 7d
      replicas: 1
      storageSpec:
        volumeClaimTemplate:
          spec:
            resources:
              requests:
                storage: 20Gi

loki-stack:
  loki:
    config:
      limits_config:
        retention_period: 48h

jaeger:
  enabled: false  # Disable in dev
```

4. **–°–æ–∑–¥–∞–π GitOps configuration –¥–ª—è ArgoCD**:

`argocd/monitoring-app.yaml`:
```yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: monitoring-stack
  namespace: argocd
spec:
  project: default
  
  source:
    repoURL: https://github.com/your-org/monitoring-infrastructure.git
    targetRevision: main
    path: helm/monitoring
    helm:
      valueFiles:
        - values.yaml
        - values-{{ .Values.environment }}.yaml
  
  destination:
    server: https://kubernetes.default.svc
    namespace: monitoring
  
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
      allowEmpty: false
    
    syncOptions:
      - CreateNamespace=true
      - PrunePropagationPolicy=foreground
      - PruneLast=true
    
    retry:
      limit: 5
      backoff:
        duration: 5s
        factor: 2
        maxDuration: 3m
  
  ignoreDifferences:
    - group: apps
      kind: Deployment
      jsonPointers:
        - /spec/replicas  # Ignore HPA changes
```

5. **–°–æ–∑–¥–∞–π CI/CD pipeline**:

`.github/workflows/deploy-monitoring.yml`:
```yaml
name: Deploy Monitoring Stack

on:
  push:
    branches: [main, develop]
    paths:
      - 'helm/**'
      - 'config/**'
      - 'terraform/**'
  pull_request:
    branches: [main]

env:
  HELM_VERSION: v3.13.0
  TERRAFORM_VERSION: 1.6.0

jobs:
  validate:
    name: Validate Configurations
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Setup Helm
        uses: azure/setup-helm@v3
        with:
          version: ${{ env.HELM_VERSION }}
      
      - name: Lint Helm Charts
        run: |
          helm lint helm/monitoring
          helm lint helm/monitoring --values helm/monitoring/values-dev.yaml
          helm lint helm/monitoring --values helm/monitoring/values-prod.yaml
      
      - name: Validate Prometheus Config
        run: |
          docker run --rm -v $PWD/config/prometheus:/prometheus \
            prom/prometheus:latest \
            promtool check config /prometheus/prometheus.yml
      
      - name: Test Alert Rules
        run: |
          docker run --rm -v $PWD:/workspace \
            prom/prometheus:latest \
            promtool test rules /workspace/tests/alerts-test.yml
      
      - name: Validate Grafana Dashboards
        run: |
          for file in config/grafana/dashboards/*.json; do
            echo "Validating $file"
            jq empty "$file" || exit 1
          done
      
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}
      
      - name: Terraform Format Check
        run: terraform fmt -check -recursive terraform/
      
      - name: Terraform Validate
        run: |
          cd terraform
          terraform init -backend=false
          terraform validate

  deploy-dev:
    name: Deploy to Development
    needs: validate
    if: github.ref == 'refs/heads/develop'
    runs-on: ubuntu-latest
    environment: development
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Setup kubectl
        uses: azure/setup-kubectl@v3
      
      - name: Setup Helm
        uses: azure/setup-helm@v3
        with:
          version: ${{ env.HELM_VERSION }}
      
      - name: Configure kubeconfig
        run: |
          echo "${{ secrets.KUBECONFIG_DEV }}" | base64 -d > ~/.kube/config
      
      - name: Deploy Helm Chart
        run: |
          helm upgrade --install monitoring ./helm/monitoring \
            --namespace monitoring \
            --create-namespace \
            --values helm/monitoring/values-dev.yaml \
            --wait \
            --timeout 10m
      
      - name: Run Smoke Tests
        run: |
          kubectl wait --for=condition=ready pod \
            -l app.kubernetes.io/name=prometheus \
            -n monitoring \
            --timeout=300s
          
          kubectl wait --for=condition=ready pod \
            -l app.kubernetes.io/name=grafana \
            -n monitoring \
            --timeout=300s

  deploy-prod:
    name: Deploy to Production
    needs: validate
    if: github.ref == 'refs/heads/main'
    runs-on: ubuntu-latest
    environment: production
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Setup Helm
        uses: azure/setup-helm@v3
        with:
          version: ${{ env.HELM_VERSION }}
      
      - name: Configure kubeconfig
        run: |
          echo "${{ secrets.KUBECONFIG_PROD }}" | base64 -d > ~/.kube/config
      
      - name: Backup current state
        run: |
          helm get values monitoring -n monitoring > backup-values.yaml
          kubectl get configmap -n monitoring -o yaml > backup-configmaps.yaml
      
      - name: Deploy Helm Chart
        run: |
          helm upgrade --install monitoring ./helm/monitoring \
            --namespace monitoring \
            --create-namespace \
            --values helm/monitoring/values-prod.yaml \
            --wait \
            --timeout 15m
      
      - name: Verify Deployment
        run: |
          kubectl rollout status deployment/monitoring-grafana -n monitoring
          kubectl rollout status statefulset/prometheus-monitoring-kube-prometheus-prometheus -n monitoring
      
      - name: Notify Slack
        if: always()
        uses: 8398a7/action-slack@v3
        with:
          status: ${{ job.status }}
          text: 'Monitoring stack deployment to production: ${{ job.status }}'
          webhook_url: ${{ secrets.SLACK_WEBHOOK }}
```

6. **–°–æ–∑–¥–∞–π backup script**:

`scripts/backup-monitoring.sh`:
```bash
#!/bin/bash

set -e

# Configuration
BACKUP_DIR="/backup/monitoring"
RETENTION_DAYS=30
DATE=$(date +%Y%m%d-%H%M%S)
NAMESPACE="monitoring"

echo "Starting monitoring backup at $(date)"

# Create backup directory
mkdir -p "$BACKUP_DIR/$DATE"

# Backup Grafana
echo "Backing up Grafana..."
kubectl exec -n $NAMESPACE deployment/grafana -- \
  grafana-cli admin export-dashboard > "$BACKUP_DIR/$DATE/grafana-dashboards.json"

# Backup Grafana database
kubectl exec -n $NAMESPACE deployment/grafana -- \
  sqlite3 /var/lib/grafana/grafana.db .dump > "$BACKUP_DIR/$DATE/grafana-db.sql"

# Backup Prometheus config
echo "Backing up Prometheus configuration..."
kubectl get configmap -n $NAMESPACE prometheus-config -o yaml > \
  "$BACKUP_DIR/$DATE/prometheus-config.yaml"

# Backup Alert rules
kubectl get prometheusrule -n $NAMESPACE -o yaml > \
  "$BACKUP_DIR/$DATE/prometheus-rules.yaml"

# Backup Alertmanager config
kubectl get secret -n $NAMESPACE alertmanager-config -o yaml > \
  "$BACKUP_DIR/$DATE/alertmanager-config.yaml"

# Backup PVCs
echo "Backing up PVCs..."
kubectl get pvc -n $NAMESPACE -o yaml > "$BACKUP_DIR/$DATE/pvcs.yaml"

# Create tar archive
echo "Creating archive..."
tar -czf "$BACKUP_DIR/monitoring-backup-$DATE.tar.gz" -C "$BACKUP_DIR" "$DATE"

# Remove temporary directory
rm -rf "$BACKUP_DIR/$DATE"

# Cleanup old backups
echo "Cleaning up old backups..."
find "$BACKUP_DIR" -name "monitoring-backup-*.tar.gz" -mtime +$RETENTION_DAYS -delete

# Upload to S3 (optional)
if [ -n "$AWS_S3_BUCKET" ]; then
  echo "Uploading to S3..."
  aws s3 cp "$BACKUP_DIR/monitoring-backup-$DATE.tar.gz" \
    "s3://$AWS_S3_BUCKET/monitoring-backups/"
fi

echo "Backup completed successfully at $(date)"
echo "Backup location: $BACKUP_DIR/monitoring-backup-$DATE.tar.gz"
```

7. **–°–æ–∑–¥–∞–π validation tests**:

`tests/alerts-test.yml`:
```yaml
# Unit tests –¥–ª—è alert rules
rule_files:
  - ../config/prometheus/alerts/*.yml

evaluation_interval: 1m

tests:
  # Test HighCPUUsage alert
  - interval: 1m
    input_series:
      - series: 'node_cpu_seconds_total{mode="idle",instance="localhost:9100"}'
        values: '100+0x10'  # Idle CPU = 100 (–ø–æ—Å—Ç–æ—è–Ω–Ω–æ)
      
      - series: 'node_cpu_seconds_total{mode="system",instance="localhost:9100"}'
        values: '0+10x10'   # System CPU —Ä–∞—Å—Ç—ë—Ç
    
    alert_rule_test:
      - eval_time: 5m
        alertname: HighCPUUsage
        exp_alerts:
          - exp_labels:
              severity: warning
              instance: localhost:9100
            exp_annotations:
              summary: "High CPU usage on localhost:9100"
  
  # Test DiskSpaceCritical alert
  - interval: 1m
    input_series:
      - series: 'node_filesystem_size_bytes{mountpoint="/",instance="localhost:9100"}'
        values: '100000000000+0x10'  # 100GB total
      
      - series: 'node_filesystem_avail_bytes{mountpoint="/",instance="localhost:9100"}'
        values: '5000000000+0x10'    # 5GB available (95% used)
    
    alert_rule_test:
      - eval_time: 5m
        alertname: DiskSpaceCritical
        exp_alerts:
          - exp_labels:
              severity: critical
              instance: localhost:9100
              mountpoint: "/"
            exp_annotations:
              summary: "Critical disk space on localhost:9100"
  
  # Test no alert when metrics are normal
  - interval: 1m
    input_series:
      - series: 'node_cpu_seconds_total{mode="idle",instance="localhost:9100"}'
        values: '100+10x10'  # Normal idle CPU
    
    alert_rule_test:
      - eval_time: 10m
        alertname: HighCPUUsage
        exp_alerts: []  # No alerts expected
```

8. **–ó–∞–ø—É—Å–∫ –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ**:
```bash
# Terraform
cd terraform
terraform init
terraform plan -var="grafana_auth=admin:admin"
terraform apply -var="grafana_auth=admin:admin"

# Ansible
cd ansible
ansible-playbook -i inventory/hosts.ini playbooks/deploy-monitoring-stack.yml

# Helm (local test)
helm install monitoring ./helm/monitoring \
  --namespace monitoring \
  --create-namespace \
  --values helm/monitoring/values-dev.yaml \
  --dry-run --debug

# Real deployment
helm install monitoring ./helm/monitoring \
  --namespace monitoring \
  --create-namespace \
  --values helm/monitoring/values-prod.yaml

# Verify
kubectl get pods -n monitoring
helm list -n monitoring

# Run tests
promtool test rules tests/alerts-test.yml

# Backup
./scripts/backup-monitoring.sh
```

### üöÄ –ë–æ–Ω—É—Å (–Ω–æ–≤–æ–µ)

**1. Monitoring as Code with Jsonnet**:

`jsonnet/dashboards/application.jsonnet`:
```jsonnet
local grafana = import 'grafonnet/grafana.libsonnet';
local dashboard = grafana.dashboard;
local row = grafana.row;
local prometheus = grafana.prometheus;
local graphPanel = grafana.graphPanel;
local statPanel = grafana.statPanel;

dashboard.new(
  'Application Metrics',
  tags=['application', 'monitoring'],
  editable=true,
)
.addRow(
  row.new(title='Request Metrics')
  .addPanel(
    graphPanel.new(
      'Request Rate',
      datasource='Prometheus',
      format='reqps',
    )
    .addTarget(
      prometheus.target(
        'sum(rate(http_requests_total[5m])) by (service)',
        legendFormat='{{service}}',
      )
    )
  )
  .addPanel(
    statPanel.new(
      'Error Rate',
      datasource='Prometheus',
      unit='percentunit',
    )
    .addTarget(
      prometheus.target(
        'sum(rate(http_requests_total{status=~"5.."}[5m])) / sum(rate(http_requests_total[5m]))',
      )
    )
    .addThresholds([
      { value: 0, color: 'green' },
      { value: 0.01, color: 'yellow' },
      { value: 0.05, color: 'red' },
    ])
  )
)
```

–ö–æ–º–ø–∏–ª—è—Ü–∏—è:
```bash
jsonnet -J vendor dashboards/application.jsonnet > dashboards/application.json
```

**2. Monitoring Configuration Testing Framework**:

`tests/integration_test.py`:
```python
#!/usr/bin/env python3
"""
Integration tests –¥–ª—è monitoring stack
"""
import requests
import time
import pytest

PROMETHEUS_URL = "http://localhost:9090"
GRAFANA_URL = "http://localhost:3000"
ALERTMANAGER_URL = "http://localhost:9093"

class TestPrometheus:
    def test_prometheus_healthy(self):
        """Test Prometheus health"""
        response = requests.get(f"{PROMETHEUS_URL}/-/healthy")
        assert response.status_code == 200
    
    def test_prometheus_targets(self):
        """Test all targets are up"""
        response = requests.get(f"{PROMETHEUS_URL}/api/v1/targets")
        data = response.json()
        
        active_targets = data['data']['activeTargets']
        down_targets = [t for t in active_targets if t['health'] != 'up']
        
        assert len(down_targets) == 0, f"Down targets: {down_targets}"
    
    def test_prometheus_rules_loaded(self):
        """Test alert rules are loaded"""
        response = requests.get(f"{PROMETHEUS_URL}/api/v1/rules")
        data = response.json()
        
        groups = data['data']['groups']
        assert len(groups) > 0, "No alert rule groups found"
    
    def test_query_works(self):
        """Test PromQL queries work"""
        query = "up"
        response = requests.get(
            f"{PROMETHEUS_URL}/api/v1/query",
            params={'query': query}
        )
        data = response.json()
        
        assert data['status'] == 'success'
        assert len(data['data']['result']) > 0

class TestGrafana:
    def test_grafana_healthy(self):
        """Test Grafana health"""
        response = requests.get(f"{GRAFANA_URL}/api/health")
        assert response.status_code == 200
    
    def test_datasources_configured(self):
        """Test datasources are configured"""
        response = requests.get(
            f"{GRAFANA_URL}/api/datasources",
            auth=('admin', 'admin')
        )
        datasources = response.json()
        
        assert len(datasources) > 0, "No datasources configured"
        
        # Check Prometheus datasource
        prometheus_ds = [ds for ds in datasources if ds['type'] == 'prometheus']
        assert len(prometheus_ds) > 0, "Prometheus datasource not found"
    
    def test_dashboards_exist(self):
        """Test dashboards are provisioned"""
        response = requests.get(
            f"{GRAFANA_URL}/api/search",
            auth=('admin', 'admin')
        )
        dashboards = response.json()
        
        assert len(dashboards) > 0, "No dashboards found"

class TestAlertmanager:
    def test_alertmanager_healthy(self):
        """Test Alertmanager health"""
        response = requests.get(f"{ALERTMANAGER_URL}/-/healthy")
        assert response.status_code == 200
    
    def test_alertmanager_config(self):
        """Test Alertmanager configuration is valid"""
        response = requests.get(f"{ALERTMANAGER_URL}/api/v2/status")
        data = response.json()
        
        assert 'config' in data
        assert 'original' in data['config']

class TestEndToEnd:
    def test_alert_flow(self):
        """Test complete alert flow"""
        # 1. Trigger alert by sending bad metrics
        # 2. Wait for alert to fire
        # 3. Check alert in Alertmanager
        # 4. Verify notification sent
        
        # Wait for alert to evaluate
        time.sleep(60)
        
        # Check for firing alerts
        response = requests.get(f"{ALERTMANAGER_URL}/api/v2/alerts")
        alerts = response.json()
        
        # Should have some alerts
        assert isinstance(alerts, list)

if __name__ == "__main__":
    pytest.main([__file__, "-v"])
```

**3. Automated Dashboard Sync**:

`scripts/sync-dashboards.py`:
```python
#!/usr/bin/env python3
"""
Sync Grafana dashboards between environments
"""
import requests
import json
import os
from pathlib import Path

class GrafanaDashboardSync:
    def __init__(self, source_url, target_url, api_key):
        self.source_url = source_url
        self.target_url = target_url
        self.headers = {'Authorization': f'Bearer {api_key}'}
    
    def export_dashboards(self, output_dir):
        """Export all dashboards from source"""
        # Search all dashboards
        response = requests.get(
            f"{self.source_url}/api/search?type=dash-db",
            headers=self.headers
        )
        dashboards = response.json()
        
        Path(output_dir).mkdir(parents=True, exist_ok=True)
        
        for dash in dashboards:
            uid = dash['uid']
            title = dash['title']
            
            # Get dashboard JSON
            response = requests.get(
                f"{self.source_url}/api/dashboards/uid/{uid}",
                headers=self.headers
            )
            dashboard_json = response.json()
            
            # Save to file
            filename = f"{output_dir}/{uid}_{title.replace(' ', '_')}.json"
            with open(filename, 'w') as f:
                json.dump(dashboard_json, f, indent=2)
            
            print(f"Exported: {title}")
    
    def import_dashboards(self, input_dir):
        """Import dashboards to target"""
        for file_path in Path(input_dir).glob('*.json'):
            with open(file_path, 'r') as f:
                dashboard_data = json.load(f)
            
            # Prepare import payload
            payload = {
                'dashboard': dashboard_data['dashboard'],
                'folderId': 0,
                'overwrite': True
            }
            
            # Import dashboard
            response = requests.post(
                f"{self.target_url}/api/dashboards/db",
                headers=self.headers,
                json=payload
            )
            
            if response.status_code == 200:
                print(f"Imported: {file_path.name}")
            else:
                print(f"Failed to import {file_path.name}: {response.text}")

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser()
    parser.add_argument('--source', required=True)
    parser.add_argument('--target', required=True)
    parser.add_argument('--api-key', required=True)
    parser.add_argument('--export-dir', default='./dashboards')
    parser.add_argument('--action', choices=['export', 'import', 'sync'], required=True)
    
    args = parser.parse_args()
    
    sync = GrafanaDashboardSync(args.source, args.target, args.api_key)
    
    if args.action in ['export', 'sync']:
        sync.export_dashboards(args.export_dir)
    
    if args.action in ['import', 'sync']:
        sync.import_dashboards(args.export_dir)
```

**4. Cost Optimization –¥–ª—è Cloud Monitoring**:

`terraform/modules/cost-optimization/main.tf`:
```hcl
# Intelligent tiering –¥–ª—è Prometheus storage
resource "aws_s3_bucket" "prometheus_long_term" {
  bucket = "prometheus-long-term-storage"
  
  lifecycle_rule {
    enabled = true
    
    transition {
      days          = 30
      storage_class = "STANDARD_IA"
    }
    
    transition {
      days          = 90
      storage_class = "GLACIER"
    }
    
    expiration {
      days = 365
    }
  }
}

# Spot instances –¥–ª—è non-critical monitoring
resource "aws_autoscaling_group" "monitoring_workers" {
  name = "monitoring-workers"
  
  mixed_instances_policy {
    launch_template {
      launch_template_specification {
        launch_template_id = aws_launch_template.monitoring.id
      }
      
      override {
        instance_type = "t3.medium"
      }
      
      override {
        instance_type = "t3a.medium"
      }
    }
    
    instances_distribution {
      on_demand_base_capacity                  = 1
      on_demand_percentage_above_base_capacity = 0
      spot_allocation_strategy                 = "capacity-optimized"
    }
  }
}
```

---

## –ò—Ç–æ–≥–∏ –º–æ–¥—É–ª—è 7

–ü–æ—Å–ª–µ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏—è —ç—Ç–æ–≥–æ –º–æ–¥—É–ª—è —Ç—ã –¥–æ–ª–∂–µ–Ω —É–º–µ—Ç—å:

‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Terraform –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è Grafana
‚úÖ –°–æ–∑–¥–∞–≤–∞—Ç—å Ansible playbooks –¥–ª—è deployment monitoring
‚úÖ –†–∞–±–æ—Ç–∞—Ç—å —Å Helm charts –¥–ª—è Kubernetes
‚úÖ –ù–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å GitOps —Å ArgoCD
‚úÖ –ü–∏—Å–∞—Ç—å CI/CD pipelines –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
‚úÖ –°–æ–∑–¥–∞–≤–∞—Ç—å automated backups
‚úÖ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å monitoring –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
‚úÖ –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å dashboards –º–µ–∂–¥—É –æ–∫—Ä—É–∂–µ–Ω–∏—è–º–∏
‚úÖ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∑–∞—Ç—Ä–∞—Ç—ã –Ω–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
‚úÖ Version control –≤—Å–µ–π monitoring –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã

**–ö–ª—é—á–µ–≤—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã:**
1. –í—Å—è –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –≤ Git
2. –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è deployment
3. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–µ—Ä–µ–¥ production
4. –†–µ–≥—É–ª—è—Ä–Ω—ã–µ backups
5. Environment parity (dev/staging/prod –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ)
6. –î–æ–∫—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏–π
7. Cost optimization

**–°–ª–µ–¥—É—é—â–∏–π –º–æ–¥—É–ª—å:** SLI/SLO/SLA –∏ Error Budget - –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏
## –ú–æ–¥—É–ª—å 8: SLI/SLO/SLA –∏ Error Budget - Site Reliability Engineering (40 –º–∏–Ω—É—Ç)

### üéØ –ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞

**SRE –æ—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏:**

```
SLA (Service Level Agreement)
‚îú‚îÄ –Æ—Ä–∏–¥–∏—á–µ—Å–∫–æ–µ —Å–æ–≥–ª–∞—à–µ–Ω–∏–µ —Å –∫–ª–∏–µ–Ω—Ç–æ–º
‚îú‚îÄ –û–±—è–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ —É—Ä–æ–≤–Ω—è —Å–µ—Ä–≤–∏—Å–∞
‚îî‚îÄ –û–±—ã—á–Ω–æ: 99.9%, 99.95%, 99.99%

SLO (Service Level Objective)
‚îú‚îÄ –í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è —Ü–µ–ª—å –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏
‚îú‚îÄ –°—Ç—Ä–æ–∂–µ —á–µ–º SLA (–±—É—Ñ–µ—Ä –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏)
‚îî‚îÄ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π

SLI (Service Level Indicator)
‚îú‚îÄ –ú–µ—Ç—Ä–∏–∫–∞ –∏–∑–º–µ—Ä—è—é—â–∞—è –∫–∞—á–µ—Å—Ç–≤–æ —Å–µ—Ä–≤–∏—Å–∞
‚îú‚îÄ Availability, Latency, Error Rate
‚îî‚îÄ –û—Å–Ω–æ–≤–∞ –¥–ª—è SLO

Error Budget
‚îú‚îÄ –î–æ–ø—É—Å—Ç–∏–º–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—à–∏–±–æ–∫
‚îú‚îÄ 100% - SLO = Error Budget
‚îî‚îÄ –ë–∞–ª–∞–Ω—Å –º–µ–∂–¥—É –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å—é –∏ —Å–∫–æ—Ä–æ—Å—Ç—å—é
```

**–ü—Ä–∏–º–µ—Ä —Ä–∞—Å—á–µ—Ç–∞:**

```
SLA: 99.9% uptime –≤ –º–µ—Å—è—Ü
SLO: 99.95% uptime (–≤–Ω—É—Ç—Ä–µ–Ω–Ω—è—è —Ü–µ–ª—å, —Å—Ç—Ä–æ–∂–µ SLA)

Error Budget = 100% - 99.95% = 0.05%

–í –º–µ—Å—è—Ü (30 –¥–Ω–µ–π):
Total time: 30 * 24 * 60 = 43,200 –º–∏–Ω—É—Ç
Error Budget: 43,200 * 0.0005 = 21.6 –º–∏–Ω—É—Ç downtime –¥–æ–ø—É—Å—Ç–∏–º–æ

–ï—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ 10 –º–∏–Ω—É—Ç downtime:
Remaining Budget: 21.6 - 10 = 11.6 –º–∏–Ω—É—Ç
Budget consumed: 10/21.6 = 46.3%
```

**–¢–∏–ø—ã SLI:**

**1. Availability (–¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å):**

```
SLI = successful_requests / total_requests

Example:
- Successful: 999,500
- Failed: 500
- Total: 1,000,000
- Availability = 999,500 / 1,000,000 = 99.95%
```

**2. Latency (–∑–∞–¥–µ—Ä–∂–∫–∞):**

```
SLI = requests_under_threshold / total_requests

Example (threshold = 200ms):
- Under 200ms: 995,000
- Over 200ms: 5,000
- Total: 1,000,000
- Latency SLI = 995,000 / 1,000,000 = 99.5%
```

**3. Error Rate (—á–∞—Å—Ç–æ—Ç–∞ –æ—à–∏–±–æ–∫):**

```
SLI = (total_requests - error_requests) / total_requests

Example:
- Total: 1,000,000
- Errors (5xx): 300
- Error Rate SLI = (1,000,000 - 300) / 1,000,000 = 99.97%
```

**4. Throughput (–ø—Ä–æ–ø—É—Å–∫–Ω–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å):**

```
SLI = actual_throughput >= target_throughput

Example:
- Target: 1000 req/sec
- Actual: 1050 req/sec
- SLI = 100% (meets target)
```

**SLO Types (—Ç–∏–ø—ã —Ü–µ–ª–µ–π):**

**Request-based SLO:**

yaml

```yaml
# 99.9% requests –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —É—Å–ø–µ—à–Ω—ã–º–∏
slo:
  type: request_based
  target: 99.9
  sli:
    total_query: sum(rate(http_requests_total[5m]))
    good_query: sum(rate(http_requests_total{status!~"5.."}[5m]))
```

**Window-based SLO:**

yaml

````yaml
# 99.9% –≤—Ä–µ–º–µ–Ω–∏ —Å–µ—Ä–≤–∏—Å –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –¥–æ—Å—Ç—É–ø–µ–Ω
slo:
  type: window_based
  target: 99.9
  window: 30d
  sli:
    good_query: sum(up == 1)
    total_query: count(up)
```

**Multi-window SLO (Google SRE –ø–æ–¥—Ö–æ–¥):**
```
Short window (1 hour):
- Alert if burn rate > 14.4x (–∏—Å—á–µ—Ä–ø–∞–µ–º budget –∑–∞ 2 —á–∞—Å–∞)
- Severity: Critical

Medium window (6 hours):
- Alert if burn rate > 6x (–∏—Å—á–µ—Ä–ø–∞–µ–º budget –∑–∞ 5 –¥–Ω–µ–π)
- Severity: High

Long window (3 days):
- Alert if burn rate > 1x (–∏—Å—á–µ—Ä–ø–∞–µ–º budget –∑–∞ 30 –¥–Ω–µ–π)
- Severity: Warning
```

**Error Budget Policy:**
```
100% Error Budget –æ—Å—Ç–∞–µ—Ç—Å—è:
‚úÖ Ship –Ω–æ–≤—ã–µ features
‚úÖ –î–µ–ª–∞—Ç—å —Ä–∏—Å–∫–æ–≤–∞–Ω–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è
‚úÖ –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ on-call –¥–µ–∂—É—Ä—Å—Ç–≤–∞

< 50% Error Budget –æ—Å—Ç–∞–µ—Ç—Å—è:
‚ö†Ô∏è  –ó–∞–º–µ–¥–ª–∏—Ç—å releases
‚ö†Ô∏è  Code freeze –¥–ª—è non-critical features
‚ö†Ô∏è  –§–æ–∫—É—Å –Ω–∞ reliability

0% Error Budget –∏—Å—á–µ—Ä–ø–∞–Ω:
‚ùå –ü–æ–ª–Ω—ã–π code freeze
‚ùå –¢–æ–ª—å–∫–æ bug fixes –∏ reliability improvements
‚ùå Post-mortem –∞–Ω–∞–ª–∏–∑
‚ùå –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ on-call —Ä–µ—Å—É—Ä—Å—ã
````

**SLI/SLO –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤:**

**API Service:**

yaml

```yaml
slis:
  - name: availability
    description: "–ü—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—à–Ω—ã—Ö API –∑–∞–ø—Ä–æ—Å–æ–≤"
    query: |
      sum(rate(http_requests_total{status!~"5.."}[5m]))
      /
      sum(rate(http_requests_total[5m]))
    
  - name: latency
    description: "P95 latency –ø–æ–¥ 200ms"
    query: |
      histogram_quantile(0.95,
        rate(http_request_duration_seconds_bucket[5m])
      ) < 0.2

slos:
  - name: api_availability
    target: 99.9
    sli: availability
    window: 30d
    
  - name: api_latency
    target: 99.5
    sli: latency
    window: 30d
```

**Database:**

yaml

```yaml
slis:
  - name: query_success_rate
    query: |
      sum(rate(db_queries_total{status="success"}[5m]))
      /
      sum(rate(db_queries_total[5m]))
  
  - name: query_latency
    query: |
      histogram_quantile(0.99,
        rate(db_query_duration_seconds_bucket[5m])
      ) < 0.1

slos:
  - name: db_reliability
    target: 99.95
    sli: query_success_rate
    window: 30d
```

**Message Queue:**

yaml

```yaml
slis:
  - name: message_processing_success
    query: |
      sum(rate(queue_messages_processed_total{status="success"}[5m]))
      /
      sum(rate(queue_messages_processed_total[5m]))
  
  - name: queue_latency
    query: |
      queue_oldest_message_age_seconds < 300  # < 5 –º–∏–Ω—É—Ç

slos:
  - name: queue_reliability
    target: 99.9
    sli: message_processing_success
    window: 7d
```

**Monitoring SLO Compliance:**

promql

```promql
# –¢–µ–∫—É—â–∏–π SLO compliance
(
  sum(rate(http_requests_total{status!~"5.."}[30d]))
  /
  sum(rate(http_requests_total[30d]))
) * 100

# Error budget remaining (–≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö)
100 - (
  (1 - (sum(rate(http_requests_total{status!~"5.."}[30d])) / sum(rate(http_requests_total[30d]))))
  /
  (1 - 0.999)  # Target SLO
) * 100

# Error budget burn rate
(
  (1 - (sum(rate(http_requests_total{status!~"5.."}[1h])) / sum(rate(http_requests_total[1h]))))
  /
  (1 - 0.999)
)
```

**Alerting –Ω–∞ SLO –Ω–∞—Ä—É—à–µ–Ω–∏—è:**

yaml

````yaml
# Fast burn alert (2 —á–∞—Å–∞ –¥–æ –∏—Å—á–µ—Ä–ø–∞–Ω–∏—è)
- alert: ErrorBudgetBurnRateFast
  expr: |
    (
      (1 - (sum(rate(http_requests_total{status!~"5.."}[5m])) / sum(rate(http_requests_total[5m]))))
      /
      (1 - 0.999)  # SLO target
    ) > 14.4
  for: 2m
  labels:
    severity: critical
    slo: api_availability
  annotations:
    summary: "Fast error budget burn rate detected"
    description: "At current rate, error budget will be exhausted in 2 hours"

# Slow burn alert (5 –¥–Ω–µ–π –¥–æ –∏—Å—á–µ—Ä–ø–∞–Ω–∏—è)
- alert: ErrorBudgetBurnRateSlow
  expr: |
    (
      (1 - (sum(rate(http_requests_total{status!~"5.."}[1h])) / sum(rate(http_requests_total[1h]))))
      /
      (1 - 0.999)
    ) > 6
  for: 15m
  labels:
    severity: warning
    slo: api_availability
  annotations:
    summary: "Slow error budget burn rate detected"
    description: "At current rate, error budget will be exhausted in 5 days"

# SLO violation
- alert: SLOViolation
  expr: |
    (
      sum(rate(http_requests_total{status!~"5.."}[30d]))
      /
      sum(rate(http_requests_total[30d]))
    ) < 0.999
  for: 5m
  labels:
    severity: critical
    slo: api_availability
  annotations:
    summary: "SLO violation - 30 day window"
    description: "Current availability: {{ $value | humanizePercentage }}"
```

**SLO Dashboard –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:**
```
1. Current SLO Status
   - Gauge: —Ç–µ–∫—É—â–∏–π SLI
   - Target line: SLO
   - Color coding: green/yellow/red

2. Error Budget
   - Gauge: –æ—Å—Ç–∞–≤—à–∏–π—Å—è budget (%)
   - Graph: burn rate over time
   - Time to exhaustion

3. Burn Rate
   - Current burn rate (multiple windows)
   - Historical burn rate
   - Alerts status

4. Compliance History
   - 30-day rolling window
   - Daily compliance
   - Incidents impact

5. Budget Consumption
   - By incident
   - By service component
   - By time period
````

**User Journey SLO (—Å–∫–≤–æ–∑–Ω–æ–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥):**

yaml

```yaml
# –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π SLO –¥–ª—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–≥–æ user journey
user_journey:
  name: "Checkout Flow"
  steps:
    - name: "Add to Cart"
      slo: 99.9
      latency_target: 200ms
      
    - name: "View Cart"
      slo: 99.9
      latency_target: 300ms
      
    - name: "Payment"
      slo: 99.95  # –°—Ç—Ä–æ–∂–µ –¥–ª—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–π –æ–ø–µ—Ä–∞—Ü–∏–∏
      latency_target: 500ms
      
    - name: "Order Confirmation"
      slo: 99.9
      latency_target: 1000ms
  
  overall_slo: 99.7  # –ö–æ–º–ø–æ–∑–∏—Ç–Ω—ã–π SLO (99.9 * 99.9 * 99.95 * 99.9)
```

**SLO Report Template:**

markdown

````markdown
# SLO Report: API Service

**Period:** 2025-01-01 to 2025-01-31

## Summary
- **SLO Target:** 99.9%
- **Actual Availability:** 99.87%
- **Status:** ‚ö†Ô∏è Below Target
- **Error Budget:** 100% consumed + 30% over

## SLI Breakdown

### Availability
- Target: 99.9%
- Actual: 99.87%
- Total Requests: 100,000,000
- Failed Requests: 130,000
- Success Rate: 99.87%

### Latency (P95)
- Target: < 200ms
- Actual: 185ms
- Status: ‚úÖ Met

## Incidents

### Incident #1: Database Outage
- Date: 2025-01-15
- Duration: 45 minutes
- Impact: 100% unavailability
- Budget Consumed: 90%
- Root Cause: Primary DB failure, replication lag
- Action Items: Improve failover automation

### Incident #2: High Latency
- Date: 2025-01-22
- Duration: 2 hours
- Impact: 20% of requests over threshold
- Budget Consumed: 40%
- Root Cause: Memory leak in application
- Action Items: Add memory profiling

## Action Items
1. [ ] Implement automated database failover
2. [ ] Add continuous memory profiling
3. [ ] Increase monitoring sensitivity
4. [ ] Schedule reliability sprint

## Next Month Forecast
- If current trend continues: ‚ùå SLO at risk
- Recommended: Code freeze for non-critical features
```

**Tools –¥–ª—è SLO –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞:**
```
1. Sloth (SLO generator)
   - Generates Prometheus rules
   - Multi-window alerts
   - Dashboard generation

2. Pyrra
   - SLO management UI
   - Error budget visualization
   - Alert configuration

3. Grafana SLO Plugin
   - Native SLO support
   - Dashboard templates
   - Integration with Prometheus

4. Google Cloud SLO Monitoring
   - Managed service
   - Built-in SLI library
   - Automated reporting

5. Datadog SLO
   - SLO tracking
   - Budget alerts
   - Integration with incidents
```

**Cost of Downtime:**
```
–†–∞—Å—á–µ—Ç business impact:

E-commerce site:
- Revenue: $10M/month
- Monthly minutes: 43,200
- Revenue per minute: $231.48
- 1 hour downtime = $13,888 loss

SaaS application:
- Customers: 10,000
- Churn rate –ø—Ä–∏ downtime: 2%
- Average LTV: $5,000
- 1 hour downtime = 200 churned customers = $1M loss

Developer productivity:
- Developers: 50
- Hourly cost: $100/hour
- Blocked time per outage: 2 hours
- Cost: 50 * 100 * 2 = $10,000
````

### üíª –ó–∞–¥–∞–Ω–∏–µ

–ù–∞—Å—Ç—Ä–æ–π –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π SLO –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥:

1. **–°–æ–∑–¥–∞–π SLO –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é —Å Sloth**:

`slos/api-service.yaml`:

yaml

```yaml
version: "prometheus/v1"
service: "api-service"
labels:
  owner: "backend-team"
  tier: "critical"
slos:
  # Availability SLO
  - name: "requests-availability"
    objective: 99.9
    description: "API requests –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —É—Å–ø–µ—à–Ω—ã–º–∏"
    sli:
      events:
        error_query: sum(rate(http_requests_total{job="api",status=~"(5..|429)"}[{{.window}}]))
        total_query: sum(rate(http_requests_total{job="api"}[{{.window}}]))
    alerting:
      name: ApiHighErrorRate
      labels:
        category: "availability"
      annotations:
        summary: "High error rate on API service"
      page_alert:
        labels:
          severity: critical
      ticket_alert:
        labels:
          severity: warning
  
  # Latency SLO
  - name: "requests-latency"
    objective: 99.5
    description: "95% requests –¥–æ–ª–∂–Ω—ã –≤—ã–ø–æ–ª–Ω—è—Ç—å—Å—è –∑–∞ < 200ms"
    sli:
      events:
        error_query: |
          (
            sum(rate(http_request_duration_seconds_count{job="api"}[{{.window}}]))
            -
            sum(rate(http_request_duration_seconds_bucket{job="api",le="0.2"}[{{.window}}]))
          )
        total_query: sum(rate(http_request_duration_seconds_count{job="api"}[{{.window}}]))
    alerting:
      name: ApiHighLatency
      labels:
        category: "latency"
      annotations:
        summary: "High latency on API service"
      page_alert:
        labels:
          severity: critical
      ticket_alert:
        labels:
          severity: warning
```

`slos/database.yaml`:

yaml

```yaml
version: "prometheus/v1"
service: "postgresql"
labels:
  owner: "platform-team"
  tier: "critical"
slos:
  # Database availability
  - name: "connection-availability"
    objective: 99.95
    description: "Database –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –¥–æ—Å—Ç—É–ø–Ω–∞ –¥–ª—è –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–π"
    sli:
      events:
        error_query: sum(rate(pg_up{job="postgres"}[{{.window}}]) == 0)
        total_query: count(pg_up{job="postgres"})
    alerting:
      name: DatabaseUnavailable
      labels:
        category: "availability"
      page_alert:
        labels:
          severity: critical
  
  # Query performance
  - name: "query-performance"
    objective: 99.9
    description: "Queries –¥–æ–ª–∂–Ω—ã –≤—ã–ø–æ–ª–Ω—è—Ç—å—Å—è –∑–∞ < 100ms"
    sli:
      events:
        error_query: |
          sum(rate(pg_stat_statements_mean_exec_time{job="postgres"}[{{.window}}])) > 100
        total_query: sum(rate(pg_stat_statements_calls{job="postgres"}[{{.window}}]))
    alerting:
      name: DatabaseSlowQueries
      labels:
        category: "performance"
      ticket_alert:
        labels:
          severity: warning
```

2. **–ì–µ–Ω–µ—Ä–∞—Ü–∏—è Prometheus rules —Å Sloth**:

bash

```bash
# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ Sloth
go install github.com/slok/sloth/cmd/sloth@latest

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∞–≤–∏–ª
sloth generate -i slos/api-service.yaml -o prometheus/rules/api-slo.yaml
sloth generate -i slos/database.yaml -o prometheus/rules/database-slo.yaml

# –í–∞–ª–∏–¥–∞—Ü–∏—è
promtool check rules prometheus/rules/*.yaml
```

3. **–°–æ–∑–¥–∞–π SLO Dashboard –≤ Grafana**:

`dashboards/slo-overview.json`:

json

```json
{
  "dashboard": {
    "title": "SLO Overview",
    "tags": ["slo", "sre"],
    "timezone": "browser",
    "rows": [
      {
        "title": "SLO Status",
        "panels": [
          {
            "id": 1,
            "title": "API Availability SLO",
            "type": "gauge",
            "targets": [
              {
                "expr": "(\n  sum(rate(http_requests_total{job=\"api\",status!~\"(5..|429)\"}[30d]))\n  /\n  sum(rate(http_requests_total{job=\"api\"}[30d]))\n) * 100",
                "legendFormat": "Current"
              }
            ],
            "fieldConfig": {
              "defaults": {
                "unit": "percent",
                "min": 99,
                "max": 100,
                "thresholds": {
                  "steps": [
                    {"value": 99, "color": "red"},
                    {"value": 99.9, "color": "yellow"},
                    {"value": 99.95, "color": "green"}
                  ]
                }
              }
            }
          },
          {
            "id": 2,
            "title": "Error Budget Remaining",
            "type": "stat",
            "targets": [
              {
                "expr": "100 - (\n  (1 - (sum(rate(http_requests_total{job=\"api\",status!~\"(5..|429)\"}[30d])) / sum(rate(http_requests_total{job=\"api\"}[30d]))))\n  /\n  (1 - 0.999)\n) * 100",
                "legendFormat": "Budget Remaining"
              }
            ],
            "fieldConfig": {
              "defaults": {
                "unit": "percent",
                "thresholds": {
                  "steps": [
                    {"value": 0, "color": "red"},
                    {"value": 25, "color": "yellow"},
                    {"value": 50, "color": "green"}
                  ]
                }
              }
            }
          },
          {
            "id": 3,
            "title": "Burn Rate (1h window)",
            "type": "timeseries",
            "targets": [
              {
                "expr": "(\n  (1 - (sum(rate(http_requests_total{job=\"api\",status!~\"(5..|429)\"}[1h])) / sum(rate(http_requests_total{job=\"api\"}[1h]))))\n  /\n  (1 - 0.999)\n)",
                "legendFormat": "Burn Rate"
              },
              {
                "expr": "14.4",
                "legendFormat": "Critical Threshold (2h to exhaustion)"
              },
              {
                "expr": "6",
                "legendFormat": "Warning Threshold (5d to exhaustion)"
              }
            ]
          }
        ]
      },
      {
        "title": "SLO Compliance History",
        "panels": [
          {
            "id": 4,
            "title": "30-Day Rolling Availability",
            "type": "timeseries",
            "targets": [
              {
                "expr": "(\n  sum(rate(http_requests_total{job=\"api\",status!~\"(5..|429)\"}[30d]))\n  /\n  sum(rate(http_requests_total{job=\"api\"}[30d]))\n) * 100",
                "legendFormat": "Availability"
              },
              {
                "expr": "99.9",
                "legendFormat": "SLO Target"
              }
            ],
            "fieldConfig": {
              "defaults": {
                "unit": "percent",
                "min": 99,
                "max": 100
              }
            }
          },
          {
            "id": 5,
            "title": "Error Budget Consumption",
            "type": "bargauge",
            "targets": [
              {
                "expr": "(\n  (1 - (sum(rate(http_requests_total{job=\"api\",status!~\"(5..|429)\"}[30d])) / sum(rate(http_requests_total{job=\"api\"}[30d]))))\n  /\n  (1 - 0.999)\n) * 100",
                "legendFormat": "Budget Used"
              }
            ],
            "fieldConfig": {
              "defaults": {
                "unit": "percent",
                "max": 100,
                "thresholds": {
                  "steps": [
                    {"value": 0, "color": "green"},
                    {"value": 75, "color": "yellow"},
                    {"value": 100, "color": "red"}
                  ]
                }
              }
            }
          }
        ]
      }
    ]
  }
}
```

4. **–°–æ–∑–¥–∞–π Error Budget Policy –¥–æ–∫—É–º–µ–Ω—Ç**:

`docs/error-budget-policy.md`:

markdown

````markdown
# Error Budget Policy

## Overview
This document defines how we use error budgets to balance reliability and feature velocity.

## SLO Targets

| Service | SLO Target | Error Budget (30d) | Error Budget (minutes) |
|---------|------------|-------------------|----------------------|
| API Service | 99.9% | 0.1% | 43.2 minutes |
| Database | 99.95% | 0.05% | 21.6 minutes |
| Message Queue | 99.9% | 0.1% | 43.2 minutes |
| CDN | 99.99% | 0.01% | 4.3 minutes |

## Policy Levels

### üü¢ Level 1: Budget Healthy (> 50% remaining)

**Allowed Activities:**
- ‚úÖ Normal release cadence
- ‚úÖ Experimental features
- ‚úÖ Performance optimizations
- ‚úÖ Refactoring

**Requirements:**
- Standard change management process
- Automated testing
- Gradual rollouts

### üü° Level 2: Budget Warning (25-50% remaining)

**Allowed Activities:**
- ‚ö†Ô∏è Reduced release frequency
- ‚ö†Ô∏è Critical features only
- ‚ö†Ô∏è Additional testing required

**Requirements:**
- Senior engineer approval for changes
- Extended canary periods
- Increased monitoring
- Daily budget reviews

**Actions:**
- Conduct incident review
- Identify systemic issues
- Create reliability improvement tasks
- Schedule reliability sprint

### üî¥ Level 3: Budget Critical (< 25% remaining)

**Allowed Activities:**
- ‚ùå Feature freeze
- ‚úÖ Bug fixes only
- ‚úÖ Reliability improvements
- ‚úÖ Emergency security patches

**Requirements:**
- Director-level approval for any changes
- Mandatory post-mortems for all incidents
- 24/7 on-call rotation
- Hourly budget monitoring

**Actions:**
- Emergency reliability review
- All hands on stability
- External communication about delays
- Executive escalation

### ‚õî Level 4: Budget Exhausted (0% remaining)

**Allowed Activities:**
- ‚ùå Complete code freeze
- ‚úÖ Critical bug fixes only (with VP approval)
- ‚úÖ Incident response

**Requirements:**
- VP Engineering approval for ANY change
- Full post-mortem for budget exhaustion
- Recovery plan required before resuming features
- Daily executive updates

**Actions:**
- Immediate incident declared
- Full team mobilization
- Customer communication
- Systematic root cause analysis
- Multi-week recovery plan

## Escalation
```
Budget < 50% ‚Üí Team Lead notified
Budget < 25% ‚Üí Engineering Manager notified
Budget < 10% ‚Üí Director notified
Budget exhausted ‚Üí VP Engineering notified
```

## Review Process

- **Daily:** Automated budget reports
- **Weekly:** Team review of budget trends
- **Monthly:** SLO report to stakeholders
- **Quarterly:** Policy review and adjustment

## Exceptions

Exceptions to this policy require:
1. Written justification
2. Risk assessment
3. Approval from Director of Engineering
4. Documentation in incident log

## Contact

- **Policy Owner:** SRE Team
- **Questions:** #sre-team Slack channel
- **Escalations:** oncall-sre@company.com
````

5. **–°–æ–∑–¥–∞–π –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π SLO reporter**:

`scripts/slo-report.py`:

python

```python
#!/usr/bin/env python3
"""
Automated SLO Report Generator
"""
import requests
from datetime import datetime, timedelta
import json

class SLOReporter:
    def __init__(self, prometheus_url, period_days=30):
        self.prometheus_url = prometheus_url
        self.period_days = period_days
        self.slos = self.load_slo_config()
    
    def load_slo_config(self):
        """Load SLO configuration"""
        return {
            'api-availability': {
                'name': 'API Availability',
                'target': 99.9,
                'query': '''
                    (
                      sum(rate(http_requests_total{job="api",status!~"(5..|429)"}[30d]))
                      /
                      sum(rate(http_requests_total{job="api"}[30d]))
                    ) * 100
                '''
            },
            'api-latency': {
                'name': 'API Latency (P95 < 200ms)',
                'target': 99.5,
                'query': '''
                    (
                      sum(rate(http_request_duration_seconds_bucket{job="api",le="0.2"}[30d]))
                      /
                      sum(rate(http_request_duration_seconds_count{job="api"}[30d]))
                    ) * 100
                '''
            },
            'database-availability': {
                'name': 'Database Availability',
                'target': 99.95,
                'query': '''
                    (sum(up{job="postgres"}) / count(up{job="postgres"})) * 100
                '''
            }
        }
    
    def query_prometheus(self, query):
        """Execute Prometheus query"""
        response = requests.get(
            f"{self.prometheus_url}/api/v1/query",
            params={'query': query}
        )
        data = response.json()
        
        if data['status'] == 'success' and data['data']['result']:
            return float(data['data']['result'][0]['value'][1])
        return None
    
    def calculate_error_budget(self, actual, target):
        """Calculate error budget consumption"""
        total_budget = 100 - target
        consumed = target - actual
        
        if consumed < 0:
            return 0.0  # Over-performing
        
        budget_consumed_pct = (consumed / total_budget) * 100
        return min(budget_consumed_pct, 100.0)
    
    def get_downtime_minutes(self, availability_pct):
        """Calculate downtime in minutes"""
        total_minutes = self.period_days * 24 * 60
        uptime_minutes = total_minutes * (availability_pct / 100)
        downtime_minutes = total_minutes - uptime_minutes
        return downtime_minutes
    
    def determine_status(self, actual, target):
        """Determine SLO status"""
        if actual >= target:
            return "‚úÖ Met"
        elif actual >= target - 0.05:
            return "‚ö†Ô∏è At Risk"
        else:
            return "‚ùå Violated"
    
    def generate_report(self):
        """Generate complete SLO report"""
        report = {
            'period': f"{self.period_days} days",
            'generated_at': datetime.now().isoformat(),
            'slos': {}
        }
        
        for slo_id, slo_config in self.slos.items():
            actual = self.query_prometheus(slo_config['query'])
            
            if actual is None:
                continue
            
            target = slo_config['target']
            budget_consumed = self.calculate_error_budget(actual, target)
            status = self.determine_status(actual, target)
            downtime = self.get_downtime_minutes(actual)
            
            report['slos'][slo_id] = {
                'name': slo_config['name'],
                'target': target,
                'actual': round(actual, 4),
                'status': status,
                'error_budget_consumed': round(budget_consumed, 2),
                'error_budget_remaining': round(100 - budget_consumed, 2),
                'downtime_minutes': round(downtime, 2)
            }
        
        return report
    
    def format_markdown(self, report):
        """Format report as Markdown"""
        md = f"""# SLO Report

**Period:** {report['period']}  
**Generated:** {report['generated_at']}

## Summary

| SLO | Target | Actual | Status | Budget Used | Budget Remaining | Downtime |
|-----|--------|--------|--------|-------------|-----------------|----------|
"""
        
        for slo_id, slo_data in report['slos'].items():
            md += f"| {slo_data['name']} "
            md += f"| {slo_data['target']}% "
            md += f"| {slo_data['actual']}% " 
            md += f"| {slo_data['status']} "               
            md += f"| {slo_data['error_budget_consumed']}% " 
            md += f"| {slo_data['error_budget_remaining']}% " 
            md += f"| {slo_data['downtime_minutes']:.1f} min |\n"


    # Add recommendations
    md += "\n## Recommendations\n\n"
    
    for slo_id, slo_data in report['slos'].items():
        if slo_data['error_budget_remaining'] < 25:
            md += f"- ‚ö†Ô∏è **{slo_data['name']}**: Error budget critical. Implement code freeze.\n"
        elif slo_data['error_budget_remaining'] < 50:
            md += f"- ‚ö†Ô∏è **{slo_data['name']}**: Error budget warning. Reduce release frequency.\n"
    
    return md

def send_to_slack(self, report, webhook_url):
    """Send report to Slack"""
    # Implementation for Slack webhook
    pass


if **name** == "**main**": import argparse


parser = argparse.ArgumentParser()
parser.add_argument('--prometheus-url', default='http://localhost:9090')
parser.add_argument('--period-days', type=int, default=30)
parser.add_argument('--format', choices=['json', 'markdown'], default='markdown')
parser.add_argument('--output', help='Output file')

args = parser.parse_args()

reporter = SLOReporter(args.prometheus_url, args.period_days)
report = reporter.generate_report()

if args.format == 'json':
    output = json.dumps(report, indent=2)
else:
    output = reporter.format_markdown(report)

if args.output:
    with open(args.output, 'w') as f:
        f.write(output)
else:
    print(output)
```



–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
```bash
# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞
python scripts/slo-report.py --prometheus-url http://localhost:9090

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Ñ–∞–π–ª
python scripts/slo-report.py --output reports/slo-report-$(date +%Y%m%d).md

# JSON —Ñ–æ—Ä–º–∞—Ç
python scripts/slo-report.py --format json --output reports/slo-report.json
```

6. **–°–æ–∑–¥–∞–π –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—é –¥–ª—è Error Budget Policy**:

`scripts/error-budget-enforcer.py`:
```python
#!/usr/bin/env python3
"""
Error Budget Policy Enforcer
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–∏–º–µ–Ω—è–µ—Ç –ø–æ–ª–∏—Ç–∏–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ—Å—Ç–∞—Ç–∫–∞ error budget
"""
import requests
import sys

class ErrorBudgetEnforcer:
    def __init__(self, prometheus_url, github_token):
        self.prometheus_url = prometheus_url
        self.github_token = github_token
        self.thresholds = {
            'healthy': 50,
            'warning': 25,
            'critical': 10,
            'exhausted': 0
        }
    
    def get_error_budget_remaining(self):
        """Get current error budget remaining percentage"""
        query = '''
            100 - (
              (1 - (sum(rate(http_requests_total{job="api",status!~"(5..|429)"}[30d])) / sum(rate(http_requests_total{job="api"}[30d]))))
              /
              (1 - 0.999)
            ) * 100
        '''
        
        response = requests.get(
            f"{self.prometheus_url}/api/v1/query",
            params={'query': query}
        )
        data = response.json()
        
        if data['status'] == 'success' and data['data']['result']:
            return float(data['data']['result'][0]['value'][1])
        return None
    
    def determine_level(self, budget_remaining):
        """Determine current policy level"""
        if budget_remaining > self.thresholds['healthy']:
            return 'healthy', 'üü¢'
        elif budget_remaining > self.thresholds['warning']:
            return 'warning', 'üü°'
        elif budget_remaining > self.thresholds['critical']:
            return 'critical', 'üî¥'
        else:
            return 'exhausted', '‚õî'
    
    def enable_github_protections(self, level):
        """Enable GitHub branch protections based on level"""
        # Implementation for GitHub API
        protections = {
            'healthy': {
                'required_approving_review_count': 1,
                'dismiss_stale_reviews': False
            },
            'warning': {
                'required_approving_review_count': 2,
                'dismiss_stale_reviews': True
            },
            'critical': {
                'required_approving_review_count': 3,
                'dismiss_stale_reviews': True,
                'require_code_owner_reviews': True
            },
            'exhausted': {
                'required_approving_review_count': 4,
                'dismiss_stale_reviews': True,
                'require_code_owner_reviews': True,
                'required_status_checks': ['director-approval']
            }
        }
        
        return protections.get(level, protections['healthy'])
    
    def send_notification(self, level, budget_remaining, icon):
        """Send notification to team"""
        messages = {
            'healthy': f"{icon} Error budget healthy: {budget_remaining:.1f}% remaining",
            'warning': f"{icon} Error budget warning: {budget_remaining:.1f}% remaining. Reduce release frequency.",
            'critical': f"{icon} Error budget critical: {budget_remaining:.1f}% remaining. Feature freeze recommended.",
            'exhausted': f"{icon} Error budget exhausted! Complete code freeze in effect."
        }
        
        print(messages[level])
        # Implementation for Slack/Email notifications
    
    def enforce(self):
        """Enforce error budget policy"""
        budget_remaining = self.get_error_budget_remaining()
        
        if budget_remaining is None:
            print("‚ùå Could not retrieve error budget data")
            sys.exit(1)
        
        level, icon = self.determine_level(budget_remaining)
        
        print(f"\n{icon} Current Error Budget: {budget_remaining:.2f}%")
        print(f"Policy Level: {level.upper()}")
        
        # Apply protections
        protections = self.enable_github_protections(level)
        print(f"\nGitHub Protections: {protections}")
        
        # Send notifications
        self.send_notification(level, budget_remaining, icon)
        
        return level, budget_remaining

if __name__ == "__main__":
    import os
    
    prometheus_url = os.getenv('PROMETHEUS_URL', 'http://localhost:9090')
    github_token = os.getenv('GITHUB_TOKEN')
    
    enforcer = ErrorBudgetEnforcer(prometheus_url, github_token)
    level, budget = enforcer.enforce()
    
    # Exit code based on level
    exit_codes = {
        'healthy': 0,
        'warning': 1,
        'critical': 2,
        'exhausted': 3
    }
    
    sys.exit(exit_codes.get(level, 0))
```

–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ cron:
```bash
# /etc/cron.d/error-budget-enforcer
*/15 * * * * /usr/local/bin/error-budget-enforcer.py >> /var/log/error-budget.log 2>&1
```

### üöÄ –ë–æ–Ω—É—Å (–Ω–æ–≤–æ–µ)

**1. Composite SLO (–∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π SLO –¥–ª—è user journey)**:
```python
# composite_slo.py
class CompositeSLO:
    """
    –í—ã—á–∏—Å–ª–µ–Ω–∏–µ composite SLO –¥–ª—è multi-step user journey
    """
    def __init__(self, steps):
        self.steps = steps
    
    def calculate_composite_slo(self):
        """
        Composite SLO = –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ SLO –≤—Å–µ—Ö —à–∞–≥–æ–≤
        """
        composite = 1.0
        for step in self.steps:
            composite *= (step['slo'] / 100)
        return composite * 100
    
    def calculate_step_targets(self, target_composite_slo):
        """
        –í—ã—á–∏—Å–ª–∏—Ç—å –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ SLO –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —à–∞–≥–∞
        —á—Ç–æ–±—ã –¥–æ—Å—Ç–∏—á—å —Ü–µ–ª–µ–≤–æ–≥–æ composite SLO
        """
        num_steps = len(self.steps)
        # –†–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
        step_slo = (target_composite_slo / 100) ** (1 / num_steps) * 100
        return step_slo

# Example
checkout_flow = CompositeSLO([
    {'name': 'Add to Cart', 'slo': 99.9},
    {'name': 'View Cart', 'slo': 99.9},
    {'name': 'Payment', 'slo': 99.95},
    {'name': 'Confirmation', 'slo': 99.9}
])

composite = checkout_flow.calculate_composite_slo()
print(f"Composite SLO: {composite:.2f}%")
# Output: 99.65%

# –ï—Å–ª–∏ —Ö–æ—Ç–∏–º 99.9% composite, –∫–∞–∫–æ–π SLO –Ω—É–∂–µ–Ω –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ?
required_step_slo = checkout_flow.calculate_step_targets(99.9)
print(f"Required per-step SLO: {required_step_slo:.3f}%")
# Output: 99.975%
```

**2. SLO as Code —Å Terraform**:
```hcl
# terraform/slo/main.tf
resource "grafana_slo" "api_availability" {
  name        = "API Availability"
  description = "API requests –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —É—Å–ø–µ—à–Ω—ã–º–∏"
  
  query {
    type = "prometheus"
    
    # Success metric
    success {
      metric = "http_requests_total"
      filters = {
        job    = "api"
        status = "!~(5..|429)"
      }
    }
    
    # Total metric
    total {
      metric = "http_requests_total"
      filters = {
        job = "api"
      }
    }
  }
  
  objectives {
    value  = 99.9
    window = "30d"
  }
  
  alerting {
    fast_burn {
      enabled   = true
      threshold = 14.4
      window    = "1h"
    }
    
    slow_burn {
      enabled   = true
      threshold = 6
      window    = "6h"
    }
  }
  
  labels = {
    team     = "backend"
    service  = "api"
    tier     = "critical"
  }
}
```

**3. SLO Simulator –¥–ª—è testing**:
```python
# slo_simulator.py
import random
from datetime import datetime, timedelta

class SLOSimulator:
    """
    –°–∏–º—É–ª—è—Ç–æ—Ä –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è SLO policies
    """
    def __init__(self, target_slo, total_requests_per_day):
        self.target_slo = target_slo
        self.total_requests_per_day = total_requests_per_day
        self.error_budget = 100 - target_slo
    
    def simulate_month(self, incident_scenarios):
        """
        –°–∏–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –º–µ—Å—è—Ü —Ä–∞–±–æ—Ç—ã —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —Å—Ü–µ–Ω–∞—Ä–∏—è–º–∏ –∏–Ω—Ü–∏–¥–µ–Ω—Ç–æ–≤
        """
        days = 30
        total_requests = self.total_requests_per_day * days
        allowed_failures = total_requests * (self.error_budget / 100)
        
        print(f"Simulation Parameters:")
        print(f"  Target SLO: {self.target_slo}%")
        print(f"  Error Budget: {self.error_budget}%")
        print(f"  Total Requests (30d): {total_requests:,}")
        print(f"  Allowed Failures: {allowed_failures:,.0f}")
        print()
        
        actual_failures = 0
        
        for scenario in incident_scenarios:
            duration_minutes = scenario['duration_minutes']
            failure_rate = scenario['failure_rate']
            
            requests_during_incident = (duration_minutes / 1440) * self.total_requests_per_day
            failures = requests_during_incident * failure_rate
            
            actual_failures += failures
            
            print(f"Incident: {scenario['name']}")
            print(f"  Duration: {duration_minutes} minutes")
            print(f"  Failure Rate: {failure_rate * 100}%")
            print(f"  Requests Affected: {requests_during_incident:,.0f}")
            print(f"  Failures: {failures:,.0f}")
            print()
        
        actual_slo = ((total_requests - actual_failures) / total_requests) * 100
        budget_consumed = (actual_failures / allowed_failures) * 100
        
        print(f"Results:")
        print(f"  Actual SLO: {actual_slo:.4f}%")
        print(f"  Budget Consumed: {budget_consumed:.1f}%")
        print(f"  Status: {'‚úÖ Met' if actual_slo >= self.target_slo else '‚ùå Violated'}")
        
        return actual_slo, budget_consumed

# Example usage
simulator = SLOSimulator(target_slo=99.9, total_requests_per_day=10_000_000)

incidents = [
    {
        'name': 'Database Outage',
        'duration_minutes': 30,
        'failure_rate': 1.0  # 100% failure
    },
    {
        'name': 'High Latency Event',
        'duration_minutes': 120,
        'failure_rate': 0.2  # 20% failure
    },
    {
        'name': 'Partial Service Degradation',
        'duration_minutes': 60,
        'failure_rate': 0.5  # 50% failure
    }
]

simulator.simulate_month(incidents)
````

---

## –ò—Ç–æ–≥–∏ –º–æ–¥—É–ª—è 8

–ü–æ—Å–ª–µ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏—è —ç—Ç–æ–≥–æ –º–æ–¥—É–ª—è —Ç—ã –¥–æ–ª–∂–µ–Ω —É–º–µ—Ç—å:

‚úÖ –û–ø—Ä–µ–¥–µ–ª—è—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ SLI –¥–ª—è —Å–µ—Ä–≤–∏—Å–æ–≤
‚úÖ –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ SLO targets
‚úÖ –í—ã—á–∏—Å–ª—è—Ç—å –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å Error Budget
‚úÖ –ù–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å multi-window alerting
‚úÖ –°–æ–∑–¥–∞–≤–∞—Ç—å SLO dashboards
‚úÖ –ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ SLO reports
‚úÖ –ü—Ä–∏–º–µ–Ω—è—Ç—å Error Budget Policy
‚úÖ –í—ã—á–∏—Å–ª—è—Ç—å composite SLO
‚úÖ –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞—Ç—å reliability –∏ velocity
‚úÖ –ü—Ä–∏–Ω–∏–º–∞—Ç—å data-driven —Ä–µ—à–µ–Ω–∏—è –æ releases

**–ö–ª—é—á–µ–≤—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã SRE:**
1. SLO –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç target reliability
2. Error Budget –ø–æ–∑–≤–æ–ª—è–µ—Ç –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞—Ç—å —Ä–∏—Å–∫
3. –ò–∑–º–µ—Ä—è–π —Ç–æ, —á—Ç–æ –≤–∞–∂–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º
4. –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä—É–π enforcement policies
5. –ò—Å–ø–æ–ª—å–∑—É–π –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π
6. –†–µ–≥—É–ª—è—Ä–Ω–æ –ø–µ—Ä–µ—Å–º–∞—Ç—Ä–∏–≤–∞–π SLO targets
7. –î–æ–∫—É–º–µ–Ω—Ç–∏—Ä—É–π –∏ –∫–æ–º–º—É–Ω–∏—Ü–∏—Ä—É–π —Å—Ç–∞—Ç—É—Å

**–°–ª–µ–¥—É—é—â–∏–π –º–æ–¥—É–ª—å:** –§–∏–Ω–∞–ª—å–Ω—ã–π –ø—Ä–æ–µ–∫—Ç - –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ complete monitoring solution



## –§–∏–Ω–∞–ª—å–Ω—ã–π –ú–æ–¥—É–ª—å: Complete Monitoring Solution - Production-Ready Stack (90 –º–∏–Ω—É—Ç)

### üéØ –¶–µ–ª—å

–ü–æ—Å—Ç—Ä–æ–∏—Ç—å production-ready monitoring solution, –æ–±—ä–µ–¥–∏–Ω—è—é—â—É—é –≤—Å–µ –∑–Ω–∞–Ω–∏—è –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –º–æ–¥—É–ª–µ–π:

- Metrics (Prometheus)
- Logs (Loki)
- Traces (Jaeger/Tempo)
- Dashboards (Grafana)
- Alerting (Alertmanager)
- SLO Monitoring
- Infrastructure as Code
- Automation

### üìã –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞

```
Complete Monitoring Stack –¥–ª—è E-commerce –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã:

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Production Environment                    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ Frontend ‚îÇ  ‚îÇ   API    ‚îÇ  ‚îÇ Payment  ‚îÇ  ‚îÇ Inventory‚îÇ   ‚îÇ
‚îÇ  ‚îÇ Service  ‚îÇ‚Üí‚Üí‚îÇ Gateway  ‚îÇ‚Üí‚Üí‚îÇ Service  ‚îÇ  ‚îÇ Service  ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ       ‚îÇ             ‚îÇ              ‚îÇ              ‚îÇ          ‚îÇ
‚îÇ       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ
‚îÇ                         ‚îÇ                                     ‚îÇ
‚îÇ                         ‚Üì                                     ‚îÇ
‚îÇ              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                        ‚îÇ
‚îÇ              ‚îÇ   PostgreSQL + Redis ‚îÇ                        ‚îÇ
‚îÇ              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Monitoring Stack                          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
‚îÇ  ‚îÇ  Prometheus  ‚îÇ  ‚îÇ     Loki     ‚îÇ  ‚îÇ    Tempo     ‚îÇ     ‚îÇ
‚îÇ  ‚îÇ  + Thanos    ‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ              ‚îÇ     ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
‚îÇ         ‚îÇ                  ‚îÇ                  ‚îÇ              ‚îÇ
‚îÇ         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ
‚îÇ                         ‚îÇ                                     ‚îÇ
‚îÇ                         ‚Üì                                     ‚îÇ
‚îÇ              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                        ‚îÇ
‚îÇ              ‚îÇ      Grafana         ‚îÇ                        ‚îÇ
‚îÇ              ‚îÇ  + SLO Dashboards    ‚îÇ                        ‚îÇ
‚îÇ              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                        ‚îÇ
‚îÇ                                                               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê               ‚îÇ
‚îÇ  ‚îÇ Alertmanager ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí‚îÇ Notifications‚îÇ               ‚îÇ
‚îÇ  ‚îÇ              ‚îÇ           ‚îÇ Slack/PagerDuty              ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### üíª –ü—Ä–æ–µ–∫—Ç: E-commerce Monitoring

–°–æ–∑–¥–∞–¥–∏–º –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π monitoring –¥–ª—è e-commerce –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã —Å:

- 4 –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å–∞ (Frontend, API, Payment, Inventory)
- –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö (PostgreSQL) –∏ –∫—ç—à (Redis)
- Full observability (metrics, logs, traces)
- SLO monitoring
- Automated alerting
- GitOps deployment

---

## –ß–∞—Å—Ç—å 1: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã (20 –º–∏–Ω—É—Ç)

### 1.1 –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞

bash

```bash
ecommerce-monitoring/
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ docker-compose.yml
‚îú‚îÄ‚îÄ .env.example
‚îÇ
‚îú‚îÄ‚îÄ applications/
‚îÇ   ‚îú‚îÄ‚îÄ frontend/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ app.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ requirements.txt
‚îÇ   ‚îú‚îÄ‚îÄ api-gateway/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ app.js
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ package.json
‚îÇ   ‚îú‚îÄ‚îÄ payment-service/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.go
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ go.mod
‚îÇ   ‚îî‚îÄ‚îÄ inventory-service/
‚îÇ       ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ       ‚îú‚îÄ‚îÄ app.py
‚îÇ       ‚îî‚îÄ‚îÄ requirements.txt
‚îÇ
‚îú‚îÄ‚îÄ monitoring/
‚îÇ   ‚îú‚îÄ‚îÄ prometheus/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prometheus.yml
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ alerts/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ infrastructure.yml
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ application.yml
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ slo.yml
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ recording-rules/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ slo-rules.yml
‚îÇ   ‚îú‚îÄ‚îÄ loki/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ loki-config.yml
‚îÇ   ‚îú‚îÄ‚îÄ tempo/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ tempo-config.yml
‚îÇ   ‚îú‚îÄ‚îÄ grafana/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ datasources/
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ datasources.yml
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dashboards/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ overview.json
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ slo-dashboard.json
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ service-mesh.json
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ business-metrics.json
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ provisioning/
‚îÇ   ‚îú‚îÄ‚îÄ alertmanager/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ alertmanager.yml
‚îÇ   ‚îî‚îÄ‚îÄ otel-collector/
‚îÇ       ‚îî‚îÄ‚îÄ otel-config.yml
‚îÇ
‚îú‚îÄ‚îÄ slo/
‚îÇ   ‚îú‚îÄ‚îÄ api-gateway.yaml
‚îÇ   ‚îú‚îÄ‚îÄ payment-service.yaml
‚îÇ   ‚îî‚îÄ‚îÄ inventory-service.yaml
‚îÇ
‚îú‚îÄ‚îÄ terraform/
‚îÇ   ‚îú‚îÄ‚îÄ main.tf
‚îÇ   ‚îú‚îÄ‚îÄ variables.tf
‚îÇ   ‚îú‚îÄ‚îÄ grafana.tf
‚îÇ   ‚îî‚îÄ‚îÄ alerts.tf
‚îÇ
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ deploy.sh
‚îÇ   ‚îú‚îÄ‚îÄ generate-load.sh
‚îÇ   ‚îú‚îÄ‚îÄ backup.sh
‚îÇ   ‚îú‚îÄ‚îÄ slo-report.py
‚îÇ   ‚îî‚îÄ‚îÄ chaos-test.sh
‚îÇ
‚îî‚îÄ‚îÄ docs/
    ‚îú‚îÄ‚îÄ architecture.md
    ‚îú‚îÄ‚îÄ runbooks/
    ‚îÇ   ‚îú‚îÄ‚îÄ high-error-rate.md
    ‚îÇ   ‚îú‚îÄ‚îÄ database-down.md
    ‚îÇ   ‚îî‚îÄ‚îÄ slo-violation.md
    ‚îî‚îÄ‚îÄ slo-policy.md
```

### 1.2 Docker Compose - Complete Stack

`docker-compose.yml`:

yaml

```yaml
version: '3.8'

networks:
  monitoring:
    driver: bridge
  app:
    driver: bridge

volumes:
  prometheus-data:
  grafana-data:
  loki-data:
  tempo-data:
  postgres-data:
  redis-data:

services:
  # ============================================
  # Application Services
  # ============================================
  
  frontend:
    build: ./applications/frontend
    container_name: frontend
    ports:
      - "8080:8080"
    environment:
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4317
      - OTEL_SERVICE_NAME=frontend
      - API_GATEWAY_URL=http://api-gateway:8081
    networks:
      - app
      - monitoring
    depends_on:
      - api-gateway
    restart: unless-stopped

  api-gateway:
    build: ./applications/api-gateway
    container_name: api-gateway
    ports:
      - "8081:8081"
    environment:
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4317
      - OTEL_SERVICE_NAME=api-gateway
      - PAYMENT_SERVICE_URL=http://payment-service:8082
      - INVENTORY_SERVICE_URL=http://inventory-service:8083
    networks:
      - app
      - monitoring
    depends_on:
      - payment-service
      - inventory-service
    restart: unless-stopped

  payment-service:
    build: ./applications/payment-service
    container_name: payment-service
    ports:
      - "8082:8082"
    environment:
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4317
      - OTEL_SERVICE_NAME=payment-service
      - DATABASE_URL=postgres://user:password@postgres:5432/ecommerce
      - REDIS_URL=redis://redis:6379
    networks:
      - app
      - monitoring
    depends_on:
      - postgres
      - redis
    restart: unless-stopped

  inventory-service:
    build: ./applications/inventory-service
    container_name: inventory-service
    ports:
      - "8083:8083"
    environment:
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4317
      - OTEL_SERVICE_NAME=inventory-service
      - DATABASE_URL=postgres://user:password@postgres:5432/ecommerce
      - REDIS_URL=redis://redis:6379
    networks:
      - app
      - monitoring
    depends_on:
      - postgres
      - redis
    restart: unless-stopped

  # ============================================
  # Data Stores
  # ============================================

  postgres:
    image: postgres:16-alpine
    container_name: postgres
    environment:
      - POSTGRES_USER=user
      - POSTGRES_PASSWORD=password
      - POSTGRES_DB=ecommerce
    ports:
      - "5432:5432"
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./init-db.sql:/docker-entrypoint-initdb.d/init.sql
    networks:
      - app
      - monitoring
    restart: unless-stopped

  redis:
    image: redis:7-alpine
    container_name: redis
    ports:
      - "6379:6379"
    networks:
      - app
      - monitoring
    restart: unless-stopped

  # ============================================
  # Monitoring Stack
  # ============================================

  prometheus:
    image: prom/prometheus:v2.48.0
    container_name: prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=30d'
      - '--web.enable-lifecycle'
      - '--web.enable-admin-api'
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - ./monitoring/prometheus/alerts:/etc/prometheus/alerts
      - ./monitoring/prometheus/recording-rules:/etc/prometheus/recording-rules
      - prometheus-data:/prometheus
    networks:
      - monitoring
    restart: unless-stopped

  loki:
    image: grafana/loki:2.9.3
    container_name: loki
    command: -config.file=/etc/loki/config.yml
    ports:
      - "3100:3100"
    volumes:
      - ./monitoring/loki/loki-config.yml:/etc/loki/config.yml
      - loki-data:/loki
    networks:
      - monitoring
    restart: unless-stopped

  promtail:
    image: grafana/promtail:2.9.3
    container_name: promtail
    command: -config.file=/etc/promtail/config.yml
    volumes:
      - ./monitoring/promtail/promtail-config.yml:/etc/promtail/config.yml
      - /var/log:/var/log:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
    networks:
      - monitoring
    depends_on:
      - loki
    restart: unless-stopped

  tempo:
    image: grafana/tempo:latest
    container_name: tempo
    command: [ "-config.file=/etc/tempo.yml" ]
    ports:
      - "3200:3200"   # Tempo HTTP
      - "4317:4317"   # OTLP gRPC
      - "4318:4318"   # OTLP HTTP
    volumes:
      - ./monitoring/tempo/tempo-config.yml:/etc/tempo.yml
      - tempo-data:/tmp/tempo
    networks:
      - monitoring
    restart: unless-stopped

  otel-collector:
    image: otel/opentelemetry-collector-contrib:0.91.0
    container_name: otel-collector
    command: ["--config=/etc/otel-collector-config.yml"]
    ports:
      - "4317:4317"   # OTLP gRPC
      - "4318:4318"   # OTLP HTTP
      - "8888:8888"   # Prometheus metrics
    volumes:
      - ./monitoring/otel-collector/otel-config.yml:/etc/otel-collector-config.yml
    networks:
      - monitoring
    depends_on:
      - prometheus
      - loki
      - tempo
    restart: unless-stopped

  grafana:
    image: grafana/grafana:10.2.3
    container_name: grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_FEATURE_TOGGLES_ENABLE=traceqlEditor,correlations
      - GF_UNIFIED_ALERTING_ENABLED=true
      - GF_INSTALL_PLUGINS=grafana-piechart-panel,grafana-worldmap-panel
    volumes:
      - grafana-data:/var/lib/grafana
      - ./monitoring/grafana/datasources:/etc/grafana/provisioning/datasources
      - ./monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning
    networks:
      - monitoring
    depends_on:
      - prometheus
      - loki
      - tempo
    restart: unless-stopped

  alertmanager:
    image: prom/alertmanager:v0.26.0
    container_name: alertmanager
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
    ports:
      - "9093:9093"
    volumes:
      - ./monitoring/alertmanager/alertmanager.yml:/etc/alertmanager/alertmanager.yml
    networks:
      - monitoring
    restart: unless-stopped

  # ============================================
  # Exporters
  # ============================================

  node-exporter:
    image: prom/node-exporter:latest
    container_name: node-exporter
    command:
      - '--path.rootfs=/host'
    ports:
      - "9100:9100"
    volumes:
      - '/:/host:ro,rslave'
    pid: host
    networks:
      - monitoring
    restart: unless-stopped

  postgres-exporter:
    image: prometheuscommunity/postgres-exporter:latest
    container_name: postgres-exporter
    environment:
      - DATA_SOURCE_NAME=postgresql://user:password@postgres:5432/ecommerce?sslmode=disable
    ports:
      - "9187:9187"
    networks:
      - monitoring
    depends_on:
      - postgres
    restart: unless-stopped

  redis-exporter:
    image: oliver006/redis_exporter:latest
    container_name: redis-exporter
    environment:
      - REDIS_ADDR=redis://redis:6379
    ports:
      - "9121:9121"
    networks:
      - monitoring
    depends_on:
      - redis
    restart: unless-stopped

  # ============================================
  # Load Generator (–¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)
  # ============================================

  load-generator:
    image: grafana/k6:latest
    container_name: load-generator
    volumes:
      - ./scripts/load-test.js:/scripts/load-test.js
    command: run --vus 10 --duration 24h /scripts/load-test.js
    networks:
      - app
    depends_on:
      - frontend
    restart: unless-stopped
```

---

## –ß–∞—Å—Ç—å 2: Application Services —Å Instrumentation (25 –º–∏–Ω—É—Ç)

### 2.1 Frontend Service (Python Flask)

`applications/frontend/app.py`:

python

```python
from flask import Flask, render_template_string, jsonify, request
from opentelemetry import trace, metrics
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter
from opentelemetry.sdk.metrics import MeterProvider
from opentelemetry.sdk.metrics.export import PeriodicExportingMetricReader
from opentelemetry.exporter.otlp.proto.grpc.metric_exporter import OTLPMetricExporter
from opentelemetry.instrumentation.flask import FlaskInstrumentor
from opentelemetry.instrumentation.requests import RequestsInstrumentor
from opentelemetry.sdk.resources import Resource
from prometheus_client import Counter, Histogram, Gauge, generate_latest
import requests
import logging
import os
import time
import random

# Configure OpenTelemetry
resource = Resource.create({
    "service.name": os.getenv("OTEL_SERVICE_NAME", "frontend"),
    "service.version": "1.0.0"
})

# Tracing
trace_provider = TracerProvider(resource=resource)
trace_processor = BatchSpanProcessor(
    OTLPSpanExporter(endpoint=os.getenv("OTEL_EXPORTER_OTLP_ENDPOINT"))
)
trace_provider.add_span_processor(trace_processor)
trace.set_tracer_provider(trace_provider)
tracer = trace.get_tracer(__name__)

# Metrics
metric_reader = PeriodicExportingMetricReader(
    OTLPMetricExporter(endpoint=os.getenv("OTEL_EXPORTER_OTLP_ENDPOINT"))
)
meter_provider = MeterProvider(resource=resource, metric_readers=[metric_reader])
metrics.set_meter_provider(meter_provider)
meter = metrics.get_meter(__name__)

# Prometheus metrics
REQUEST_COUNT = Counter(
    'http_requests_total',
    'Total HTTP requests',
    ['method', 'endpoint', 'status']
)
REQUEST_DURATION = Histogram(
    'http_request_duration_seconds',
    'HTTP request duration',
    ['method', 'endpoint']
)
ACTIVE_REQUESTS = Gauge(
    'http_requests_inprogress',
    'Active HTTP requests',
    ['method', 'endpoint']
)

# Business metrics
ORDERS_CREATED = Counter('orders_created_total', 'Total orders created')
REVENUE = Counter('revenue_total_cents', 'Total revenue in cents')

app = Flask(__name__)
FlaskInstrumentor().instrument_app(app)
RequestsInstrumentor().instrument()

API_GATEWAY_URL = os.getenv("API_GATEWAY_URL", "http://api-gateway:8081")

# Structured logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - [trace_id=%(otelTraceID)s] - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# HTML Template
HTML_TEMPLATE = """
<!DOCTYPE html>
<html>
<head>
    <title>E-Commerce Demo</title>
    <style>
        body { font-family: Arial; max-width: 1200px; margin: 50px auto; padding: 20px; }
        .card { border: 1px solid #ddd; padding: 20px; margin: 10px 0; border-radius: 5px; }
        button { background: #007bff; color: white; padding: 10px 20px; border: none; border-radius: 5px; cursor: pointer; }
        button:hover { background: #0056b3; }
        .metrics { background: #f8f9fa; padding: 15px; margin-top: 20px; }
    </style>
</head>
<body>
    <h1>üõí E-Commerce Monitoring Demo</h1>
    
    <div class="card">
        <h2>Create Order</h2>
        <button onclick="createOrder()">Place Order</button>
        <button onclick="createSlowOrder()">Place Slow Order (Simulate Latency)</button>
        <button onclick="createFailingOrder()">Place Failing Order (Simulate Error)</button>
        <div id="result"></div>
    </div>
    
    <div class="card">
        <h2>Check Inventory</h2>
        <button onclick="checkInventory()">Check Stock</button>
        <div id="inventory"></div>
    </div>
    
    <div class="metrics">
        <h3>üìä Monitoring Links</h3>
        <ul>
            <li><a href="http://localhost:3000" target="_blank">Grafana Dashboards</a></li>
            <li><a href="http://localhost:9090" target="_blank">Prometheus</a></li>
            <li><a href="http://localhost:3200" target="_blank">Tempo (Traces)</a></li>
            <li><a href="http://localhost:9093" target="_blank">Alertmanager</a></li>
        </ul>
    </div>
    
    <script>
        async function createOrder() {
            const result = await fetch('/api/order', { method: 'POST' });
            const data = await result.json();
            document.getElementById('result').innerHTML = `<pre>${JSON.stringify(data, null, 2)}</pre>`;
        }
        
        async function createSlowOrder() {
            const result = await fetch('/api/order?slow=true', { method: 'POST' });
            const data = await result.json();
            document.getElementById('result').innerHTML = `<pre>${JSON.stringify(data, null, 2)}</pre>`;
        }
        
        async function createFailingOrder() {
            try {
                const result = await fetch('/api/order?fail=true', { method: 'POST' });
                const data = await result.json();
                document.getElementById('result').innerHTML = `<pre>${JSON.stringify(data, null, 2)}</pre>`;
            } catch (e) {
                document.getElementById('result').innerHTML = `<pre style="color:red">Error: ${e}</pre>`;
            }
        }
        
        async function checkInventory() {
            const result = await fetch('/api/inventory');
            const data = await result.json();
            document.getElementById('inventory').innerHTML = `<pre>${JSON.stringify(data, null, 2)}</pre>`;
        }
    </script>
</body>
</html>
"""

@app.route('/')
def index():
    """Homepage"""
    return render_template_string(HTML_TEMPLATE)

@app.route('/api/order', methods=['POST'])
def create_order():
    """Create order - calls API Gateway"""
    method = request.method
    endpoint = '/api/order'
    
    ACTIVE_REQUESTS.labels(method=method, endpoint=endpoint).inc()
    start_time = time.time()
    
    try:
        with tracer.start_as_current_span("create_order") as span:
            # Add span attributes
            span.set_attribute("http.method", method)
            span.set_attribute("http.route", endpoint)
            
            # Check for test parameters
            slow = request.args.get('slow') == 'true'
            fail = request.args.get('fail') == 'true'
            
            if slow:
                span.add_event("Simulating slow request")
            if fail:
                span.add_event("Simulating failing request")
            
            # Call API Gateway
            params = {}
            if slow:
                params['slow'] = 'true'
            if fail:
                params['fail'] = 'true'
            
            response = requests.post(
                f"{API_GATEWAY_URL}/order",
                json={"product": "Widget", "quantity": random.randint(1, 5)},
                params=params,
                timeout=10
            )
            
            # Business metrics
            if response.status_code == 200:
                data = response.json()
                ORDERS_CREATED.inc()
                REVENUE.inc(data.get('amount', 0))
                span.set_attribute("order.id", data.get('order_id'))
                span.set_attribute("order.amount", data.get('amount'))
            
            REQUEST_COUNT.labels(
                method=method,
                endpoint=endpoint,
                status=response.status_code
            ).inc()
            
            return jsonify(response.json()), response.status_code
            
    except Exception as e:
        span = trace.get_current_span()
        span.record_exception(e)
        span.set_status(trace.Status(trace.StatusCode.ERROR, str(e)))
        
        REQUEST_COUNT.labels(
            method=method,
            endpoint=endpoint,
            status=500
        ).inc()
        
        logger.error(f"Error creating order: {e}")
        return jsonify({"error": str(e)}), 500
        
    finally:
        duration = time.time() - start_time
        REQUEST_DURATION.labels(method=method, endpoint=endpoint).observe(duration)
        ACTIVE_REQUESTS.labels(method=method, endpoint=endpoint).dec()

@app.route('/api/inventory')
def get_inventory():
    """Get inventory"""
    with tracer.start_as_current_span("get_inventory"):
        response = requests.get(f"{API_GATEWAY_URL}/inventory")
        return jsonify(response.json()), response.status_code

@app.route('/health')
def health():
    """Health check"""
    return jsonify({"status": "healthy"}), 200

@app.route('/metrics')
def metrics():
    """Prometheus metrics endpoint"""
    return generate_latest()

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=8080, debug=False)
```

`applications/frontend/Dockerfile`:

dockerfile

````dockerfile
FROM python:3.11-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD ["python", "app.py"]
```

`applications/frontend/requirements.txt`:
```
flask==3.0.0
requests==2.31.0
opentelemetry-api==1.21.0
opentelemetry-sdk==1.21.0
opentelemetry-instrumentation-flask==0.42b0
opentelemetry-instrumentation-requests==0.42b0
opentelemetry-exporter-otlp==1.21.0
prometheus-client==0.19.0
````

### 2.2 API Gateway (Node.js)

`applications/api-gateway/app.js`:

javascript

```javascript
const express = require('express');
const axios = require('axios');
const { NodeSDK } = require('@opentelemetry/sdk-node');
const { getNodeAutoInstrumentations } = require('@opentelemetry/auto-instrumentations-node');
const { OTLPTraceExporter } = require('@opentelemetry/exporter-trace-otlp-grpc');
const { Resource } = require('@opentelemetry/resources');
const { SemanticResourceAttributes } = require('@opentelemetry/semantic-conventions');
const { PrometheusExporter } = require('@opentelemetry/exporter-prometheus');
const client = require('prom-client');

// OpenTelemetry SDK
const sdk = new NodeSDK({
  resource: new Resource({
    [SemanticResourceAttributes.SERVICE_NAME]: process.env.OTEL_SERVICE_NAME || 'api-gateway',
    [SemanticResourceAttributes.SERVICE_VERSION]: '1.0.0',
  }),
  traceExporter: new OTLPTraceExporter({
    url: process.env.OTEL_EXPORTER_OTLP_ENDPOINT || 'http://localhost:4317',
  }),
  instrumentations: [getNodeAutoInstrumentations()],
});

sdk.start();

// Prometheus metrics
const register = new client.Registry();
client.collectDefaultMetrics({ register });

const httpRequestDuration = new client.Histogram({
  name: 'http_request_duration_seconds',
  help: 'Duration of HTTP requests in seconds',
  labelNames: ['method', 'route', 'status'],
  registers: [register]
});

const httpRequestsTotal = new client.Counter({
  name: 'http_requests_total',
  help: 'Total number of HTTP requests',
  labelNames: ['method', 'route', 'status'],
  registers: [register]
});

const app = express();
app.use(express.json());

const PAYMENT_SERVICE_URL = process.env.PAYMENT_SERVICE_URL || 'http://payment-service:8082';
const INVENTORY_SERVICE_URL = process.env.INVENTORY_SERVICE_URL || 'http://inventory-service:8083';

// Middleware for metrics
app.use((req, res, next) => {
  const start = Date.now();
  
  res.on('finish', () => {
    const duration = (Date.now() - start) / 1000;
    httpRequestDuration.labels(req.method, req.route?.path || req.path, res.statusCode).observe(duration);
    httpRequestsTotal.labels(req.method, req.route?.path || req.path, res.statusCode).inc();
  });
  
  next();
});

// Create Order
app.post('/order', async (req, res) => {
  try {
    const { product, quantity } = req.body;
    const slow = req.query.slow === 'true';
    const fail = req.query.fail === 'true';
    
    console.log(`Creating order: product=${product}, quantity=${quantity}, slow=${slow}, fail=${fail}`);
    
    // Check inventory
    const inventoryResponse = await axios.get(`${INVENTORY_SERVICE_URL}/check`, {
      params: { product, quantity }
    });
    
    if (!inventoryResponse.data.available) {
      return res.status(400).json({ error: 'Product not available' });
    }
    
    // Process payment
    const amount = quantity * 1000; // $10 per item in cents
    
    const paymentResponse = await axios.post(`${PAYMENT_SERVICE_URL}/process`, {
      amount,
      slow,
      fail
    });
    
    if (paymentResponse.data.status !== 'success') {
      return res.status(400).json({ error: 'Payment failed' });
    }
    
    // Update inventory
    await axios.post(`${INVENTORY_SERVICE_URL}/reserve`, {
      product,
      quantity
    });
    
    const order = {
      order_id: `ORD-${Date.now()}`,
      product,
      quantity,
      amount,
      payment_id: paymentResponse.data.payment_id,
      status: 'completed'
    };
    
    console.log('Order created:', order);
    res.json(order);
    
  } catch (error) {
    console.error('Error creating order:', error.message);
    res.status(500).json({ error: error.message });
  }
});

// Get Inventory
app.get('/inventory', async (req, res) => {try { const response = await axios.get(`${INVENTORY_SERVICE_URL}/list`); res.json(response.data); } 
catch (error) { console.error('Error fetching inventory:', error.message); res.status(500).json({ error: error.message }); } });

// Health check app.get('/health', (req, res) => { res.json({ status: 'healthy' }); });

// Metrics endpoint app.get('/metrics', async (req, res) => { res.set('Content-Type', register.contentType); res.end(await register.metrics()); });

const PORT = 8081; app.listen(PORT, () => { console.log(`API Gateway listening on port ${PORT}`); });

````

`applications/api-gateway/package.json`:
```json
{
  "name": "api-gateway",
  "version": "1.0.0",
  "dependencies": {
    "express": "^4.18.2",
    "axios": "^1.6.2",
    "@opentelemetry/sdk-node": "^0.45.1",
    "@opentelemetry/auto-instrumentations-node": "^0.40.1",
    "@opentelemetry/exporter-trace-otlp-grpc": "^0.45.1",
    "@opentelemetry/resources": "^1.19.0",
    "@opentelemetry/semantic-conventions": "^1.19.0",
    "@opentelemetry/exporter-prometheus": "^0.45.1",
    "prom-client": "^15.0.0"
  }
}
```

`applications/api-gateway/Dockerfile`:
```dockerfile
FROM node:20-alpine

WORKDIR /app

COPY package*.json ./
RUN npm install

COPY . .

CMD ["node", "app.js"]
```

–°–æ–∑–¥–∞–µ–º Payment –∏ Inventory services, –∏ –ø–µ—Ä–µ—Ö–æ–¥–∏–º –∫ –Ω–∞—Å—Ç—Ä–æ–π–∫–µ monitoring stack

### 2.3 Payment Service (Go)

`applications/payment-service/main.go`:

go

```go
package main

import (
	"context"
	"encoding/json"
	"fmt"
	"log"
	"math/rand"
	"net/http"
	"os"
	"time"

	"github.com/prometheus/client_golang/prometheus"
	"github.com/prometheus/client_golang/prometheus/promhttp"
	"go.opentelemetry.io/otel"
	"go.opentelemetry.io/otel/attribute"
	"go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracegrpc"
	"go.opentelemetry.io/otel/sdk/resource"
	sdktrace "go.opentelemetry.io/otel/sdk/trace"
	semconv "go.opentelemetry.io/otel/semconv/v1.17.0"
	"go.opentelemetry.io/otel/trace"
)

var (
	tracer trace.Tracer
	
	// Prometheus metrics
	requestDuration = prometheus.NewHistogramVec(
		prometheus.HistogramOpts{
			Name:    "http_request_duration_seconds",
			Help:    "HTTP request duration in seconds",
			Buckets: []float64{.005, .01, .025, .05, .1, .25, .5, 1, 2.5, 5, 10},
		},
		[]string{"method", "endpoint", "status"},
	)
	
	requestsTotal = prometheus.NewCounterVec(
		prometheus.CounterOpts{
			Name: "http_requests_total",
			Help: "Total number of HTTP requests",
		},
		[]string{"method", "endpoint", "status"},
	)
	
	paymentsProcessed = prometheus.NewCounterVec(
		prometheus.CounterOpts{
			Name: "payments_processed_total",
			Help: "Total number of payments processed",
		},
		[]string{"status"},
	)
	
	paymentAmount = prometheus.NewHistogram(
		prometheus.HistogramOpts{
			Name:    "payment_amount_cents",
			Help:    "Payment amount in cents",
			Buckets: prometheus.ExponentialBuckets(100, 2, 10),
		},
	)
)

func init() {
	prometheus.MustRegister(requestDuration)
	prometheus.MustRegister(requestsTotal)
	prometheus.MustRegister(paymentsProcessed)
	prometheus.MustRegister(paymentAmount)
}

type PaymentRequest struct {
	Amount int  `json:"amount"`
	Slow   bool `json:"slow"`
	Fail   bool `json:"fail"`
}

type PaymentResponse struct {
	PaymentID string `json:"payment_id"`
	Status    string `json:"status"`
	Amount    int    `json:"amount"`
}

func initTracer() func(context.Context) error {
	ctx := context.Background()

	exporter, err := otlptracegrpc.New(ctx,
		otlptracegrpc.WithEndpoint(getEnv("OTEL_EXPORTER_OTLP_ENDPOINT", "localhost:4317")),
		otlptracegrpc.WithInsecure(),
	)
	if err != nil {
		log.Fatalf("Failed to create exporter: %v", err)
	}

	res, err := resource.New(ctx,
		resource.WithAttributes(
			semconv.ServiceName(getEnv("OTEL_SERVICE_NAME", "payment-service")),
			semconv.ServiceVersion("1.0.0"),
		),
	)
	if err != nil {
		log.Fatalf("Failed to create resource: %v", err)
	}

	tp := sdktrace.NewTracerProvider(
		sdktrace.WithBatcher(exporter),
		sdktrace.WithResource(res),
	)

	otel.SetTracerProvider(tp)
	tracer = tp.Tracer("payment-service")

	return tp.Shutdown
}

func processPaymentHandler(w http.ResponseWriter, r *http.Request) {
	start := time.Now()
	ctx := r.Context()
	
	ctx, span := tracer.Start(ctx, "process_payment")
	defer span.End()

	var req PaymentRequest
	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		span.RecordError(err)
		http.Error(w, err.Error(), http.StatusBadRequest)
		recordMetrics(r.Method, "/process", "400", start)
		return
	}

	span.SetAttributes(
		attribute.Int("payment.amount", req.Amount),
		attribute.Bool("payment.slow", req.Slow),
		attribute.Bool("payment.fail", req.Fail),
	)

	log.Printf("Processing payment: amount=%d, slow=%v, fail=%v", req.Amount, req.Slow, req.Fail)

	// Simulate payment gateway call
	ctx, gatewaySpan := tracer.Start(ctx, "call_payment_gateway")
	
	if req.Slow {
		gatewaySpan.AddEvent("Simulating slow payment gateway")
		time.Sleep(time.Duration(2000+rand.Intn(3000)) * time.Millisecond)
	} else {
		time.Sleep(time.Duration(50+rand.Intn(150)) * time.Millisecond)
	}

	// Simulate random failures
	if req.Fail || rand.Float64() < 0.05 {
		gatewaySpan.AddEvent("Payment gateway failed")
		gatewaySpan.RecordError(fmt.Errorf("payment gateway error"))
		gatewaySpan.End()
		
		paymentsProcessed.WithLabelValues("failed").Inc()
		
		response := PaymentResponse{
			PaymentID: fmt.Sprintf("PAY-FAIL-%d", time.Now().Unix()),
			Status:    "failed",
			Amount:    req.Amount,
		}
		
		w.WriteHeader(http.StatusInternalServerError)
		json.NewEncoder(w).Encode(response)
		recordMetrics(r.Method, "/process", "500", start)
		return
	}
	
	gatewaySpan.AddEvent("Payment successful")
	gatewaySpan.End()

	// Record business metrics
	paymentsProcessed.WithLabelValues("success").Inc()
	paymentAmount.Observe(float64(req.Amount))

	response := PaymentResponse{
		PaymentID: fmt.Sprintf("PAY-%d", time.Now().Unix()),
		Status:    "success",
		Amount:    req.Amount,
	}

	span.SetAttributes(attribute.String("payment.id", response.PaymentID))
	span.AddEvent("Payment processed successfully")

	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(response)
	recordMetrics(r.Method, "/process", "200", start)
}

func healthHandler(w http.ResponseWriter, r *http.Request) {
	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]string{"status": "healthy"})
}

func recordMetrics(method, endpoint, status string, start time.Time) {
	duration := time.Since(start).Seconds()
	requestDuration.WithLabelValues(method, endpoint, status).Observe(duration)
	requestsTotal.WithLabelValues(method, endpoint, status).Inc()
}

func getEnv(key, fallback string) string {
	if value := os.Getenv(key); value != "" {
		return value
	}
	return fallback
}

func main() {
	shutdown := initTracer()
	defer shutdown(context.Background())

	http.HandleFunc("/process", processPaymentHandler)
	http.HandleFunc("/health", healthHandler)
	http.Handle("/metrics", promhttp.Handler())

	log.Println("Payment Service listening on :8082")
	log.Fatal(http.ListenAndServe(":8082", nil))
}
```

`applications/payment-service/go.mod`:

go

```go
module payment-service

go 1.21

require (
	github.com/prometheus/client_golang v1.17.0
	go.opentelemetry.io/otel v1.21.0
	go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracegrpc v1.21.0
	go.opentelemetry.io/otel/sdk v1.21.0
)
```

`applications/payment-service/Dockerfile`:

dockerfile

```dockerfile
FROM golang:1.21-alpine AS builder

WORKDIR /app
COPY go.mod go.sum ./
RUN go mod download

COPY . .
RUN CGO_ENABLED=0 GOOS=linux go build -o payment-service .

FROM alpine:latest
RUN apk --no-cache add ca-certificates
WORKDIR /root/
COPY --from=builder /app/payment-service .

EXPOSE 8082
CMD ["./payment-service"]
```

### 2.4 Inventory Service (Python)

`applications/inventory-service/app.py`:

python

````python
from flask import Flask, jsonify, request
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter
from opentelemetry.instrumentation.flask import FlaskInstrumentor
from opentelemetry.sdk.resources import Resource
from prometheus_client import Counter, Gauge, Histogram, generate_latest
import psycopg2
import redis
import os
import time
import logging
import json

# OpenTelemetry setup
resource = Resource.create({
    "service.name": os.getenv("OTEL_SERVICE_NAME", "inventory-service"),
    "service.version": "1.0.0"
})

trace_provider = TracerProvider(resource=resource)
trace_processor = BatchSpanProcessor(
    OTLPSpanExporter(endpoint=os.getenv("OTEL_EXPORTER_OTLP_ENDPOINT"))
)
trace_provider.add_span_processor(trace_processor)
trace.set_tracer_provider(trace_provider)
tracer = trace.get_tracer(__name__)

# Prometheus metrics
REQUEST_COUNT = Counter('http_requests_total', 'Total requests', ['method', 'endpoint', 'status'])
REQUEST_DURATION = Histogram('http_request_duration_seconds', 'Request duration', ['method', 'endpoint'])
INVENTORY_LEVEL = Gauge('inventory_level', 'Current inventory level', ['product'])
RESERVATIONS = Counter('inventory_reservations_total', 'Total reservations', ['product'])

app = Flask(__name__)
FlaskInstrumentor().instrument_app(app)

DATABASE_URL = os.getenv("DATABASE_URL")
REDIS_URL = os.getenv("REDIS_URL")

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Mock inventory data
INVENTORY = {
    "Widget": {"stock": 1000, "price": 1000},
    "Gadget": {"stock": 500, "price": 2000},
    "Doohickey": {"stock": 750, "price": 1500}
}

def get_redis():
    """Get Redis connection"""
    return redis.from_url(REDIS_URL)

def get_db():
    """Get database connection"""
    return psycopg2.connect(DATABASE_URL)

@app.route('/list')
def list_inventory():
    """List all inventory"""
    start = time.time()
    
    with tracer.start_as_current_span("list_inventory") as span:
        try:
            # Check cache
            r = get_redis()
            cached = r.get("inventory:list")
            
            if cached:
                span.add_event("Cache hit")
                span.set_attribute("cache.hit", True)
                inventory = json.loads(cached)
            else:
                span.add_event("Cache miss")
                span.set_attribute("cache.hit", False)
                
                # Query database (simulated with mock data)
                inventory = INVENTORY
                
                # Update Prometheus gauges
                for product, data in inventory.items():
                    INVENTORY_LEVEL.labels(product=product).set(data['stock'])
                
                # Cache result
                r.setex("inventory:list", 60, json.dumps(inventory))
            
            REQUEST_COUNT.labels(method='GET', endpoint='/list', status='200').inc()
            REQUEST_DURATION.labels(method='GET', endpoint='/list').observe(time.time() - start)
            
            return jsonify(inventory), 200
            
        except Exception as e:
            span.record_exception(e)
            logger.error(f"Error listing inventory: {e}")
            REQUEST_COUNT.labels(method='GET', endpoint='/list', status='500').inc()
            return jsonify({"error": str(e)}), 500

@app.route('/check')
def check_availability():
    """Check product availability"""
    start = time.time()
    
    with tracer.start_as_current_span("check_availability") as span:
        product = request.args.get('product')
        quantity = int(request.args.get('quantity', 1))
        
        span.set_attribute("product", product)
        span.set_attribute("quantity", quantity)
        
        try:
            if product not in INVENTORY:
                span.set_attribute("available", False)
                return jsonify({"available": False, "reason": "Product not found"}), 200
            
            available = INVENTORY[product]['stock'] >= quantity
            span.set_attribute("available", available)
            span.set_attribute("current_stock", INVENTORY[product]['stock'])
            
            REQUEST_COUNT.labels(method='GET', endpoint='/check', status='200').inc()
            REQUEST_DURATION.labels(method='GET', endpoint='/check').observe(time.time() - start)
            
            return jsonify({
                "available": available,
                "product": product,
                "requested": quantity,
                "in_stock": INVENTORY[product]['stock']
            }), 200
            
        except Exception as e:
            span.record_exception(e)
            logger.error(f"Error checking availability: {e}")
            REQUEST_COUNT.labels(method='GET', endpoint='/check', status='500').inc()
            return jsonify({"error": str(e)}), 500

@app.route('/reserve', methods=['POST'])
def reserve_inventory():
    """Reserve inventory"""
    start = time.time()
    
    with tracer.start_as_current_span("reserve_inventory") as span:
        data = request.json
        product = data.get('product')
        quantity = data.get('quantity')
        
        span.set_attribute("product", product)
        span.set_attribute("quantity", quantity)
        
        try:
            if product not in INVENTORY:
                return jsonify({"error": "Product not found"}), 404
            
            if INVENTORY[product]['stock'] < quantity:
                span.add_event("Insufficient stock")
                return jsonify({"error": "Insufficient stock"}), 400
            
            # Reserve inventory
            INVENTORY[product]['stock'] -= quantity
            INVENTORY_LEVEL.labels(product=product).set(INVENTORY[product]['stock'])
            RESERVATIONS.labels(product=product).inc()
            
            span.add_event("Inventory reserved")
            logger.info(f"Reserved {quantity} units of {product}")
            
            # Invalidate cache
            r = get_redis()
            r.delete("inventory:list")
            
            REQUEST_COUNT.labels(method='POST', endpoint='/reserve', status='200').inc()
            REQUEST_DURATION.labels(method='POST', endpoint='/reserve').observe(time.time() - start)
            
            return jsonify({
                "reserved": quantity,
                "product": product,
                "remaining_stock": INVENTORY[product]['stock']
            }), 200
            
        except Exception as e:
            span.record_exception(e)
            logger.error(f"Error reserving inventory: {e}")
            REQUEST_COUNT.labels(method='POST', endpoint='/reserve', status='500').inc()
            return jsonify({"error": str(e)}), 500

@app.route('/health')
def health():
    """Health check"""
    return jsonify({"status": "healthy"}), 200

@app.route('/metrics')
def metrics():
    """Prometheus metrics"""
    return generate_latest()

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=8083, debug=False)
```

`applications/inventory-service/requirements.txt`:
```
flask==3.0.0
psycopg2-binary==2.9.9
redis==5.0.1
opentelemetry-api==1.21.0
opentelemetry-sdk==1.21.0
opentelemetry-instrumentation-flask==0.42b0
opentelemetry-exporter-otlp==1.21.0
prometheus-client==0.19.0
````

`applications/inventory-service/Dockerfile`:

dockerfile

```dockerfile
FROM python:3.11-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD ["python", "app.py"]
```

---

## –ß–∞—Å—Ç—å 3: Monitoring Configuration (20 –º–∏–Ω—É—Ç)

### 3.1 Prometheus Configuration

`monitoring/prometheus/prometheus.yml`:

yaml

```yaml
global:
  scrape_interval: 15s
  evaluation_interval: 15s
  external_labels:
    cluster: 'ecommerce-demo'
    environment: 'production'

# Alertmanager configuration
alerting:
  alertmanagers:
    - static_configs:
        - targets:
            - alertmanager:9093

# Load rules
rule_files:
  - '/etc/prometheus/alerts/*.yml'
  - '/etc/prometheus/recording-rules/*.yml'

scrape_configs:
  # Prometheus itself
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  # Application services
  - job_name: 'frontend'
    static_configs:
      - targets: ['frontend:8080']
    relabel_configs:
      - source_labels: [__address__]
        target_label: instance
      - target_label: service
        replacement: 'frontend'

  - job_name: 'api-gateway'
    static_configs:
      - targets: ['api-gateway:8081']
    relabel_configs:
      - target_label: service
        replacement: 'api-gateway'

  - job_name: 'payment-service'
    static_configs:
      - targets: ['payment-service:8082']
    relabel_configs:
      - target_label: service
        replacement: 'payment-service'

  - job_name: 'inventory-service'
    static_configs:
      - targets: ['inventory-service:8083']
    relabel_configs:
      - target_label: service
        replacement: 'inventory-service'

  # Infrastructure
  - job_name: 'node-exporter'
    static_configs:
      - targets: ['node-exporter:9100']

  - job_name: 'postgres-exporter'
    static_configs:
      - targets: ['postgres-exporter:9187']

  - job_name: 'redis-exporter'
    static_configs:
      - targets: ['redis-exporter:9121']

  # OpenTelemetry Collector
  - job_name: 'otel-collector'
    static_configs:
      - targets: ['otel-collector:8888']
```

### 3.2 Alert Rules

`monitoring/prometheus/alerts/application.yml`:

yaml

```yaml
groups:
  - name: application_alerts
    interval: 30s
    rules:
      # High error rate
      - alert: HighErrorRate
        expr: |
          (
            sum(rate(http_requests_total{status=~"5.."}[5m])) by (service)
            /
            sum(rate(http_requests_total[5m])) by (service)
          ) > 0.05
        for: 2m
        labels:
          severity: critical
          team: backend
        annotations:
          summary: "High error rate on {{ $labels.service }}"
          description: "Error rate is {{ $value | humanizePercentage }} on {{ $labels.service }}"
          dashboard: "http://localhost:3000/d/app-overview"
          runbook: "https://runbooks.example.com/high-error-rate"

      # High latency
      - alert: HighLatency
        expr: |
          histogram_quantile(0.95,
            sum(rate(http_request_duration_seconds_bucket[5m])) by (service, le)
          ) > 1
        for: 5m
        labels:
          severity: warning
          team: backend
        annotations:
          summary: "High latency on {{ $labels.service }}"
          description: "P95 latency is {{ $value }}s on {{ $labels.service }}"

      # Low throughput
      - alert: LowThroughput
        expr: |
          sum(rate(http_requests_total[5m])) by (service) < 1
        for: 10m
        labels:
          severity: warning
          team: backend
        annotations:
          summary: "Low throughput on {{ $labels.service }}"
          description: "Request rate is {{ $value }} req/s"

      # Payment failures
      - alert: PaymentFailureSpike
        expr: |
          (
            sum(rate(payments_processed_total{status="failed"}[5m]))
            /
            sum(rate(payments_processed_total[5m]))
          ) > 0.1
        for: 3m
        labels:
          severity: critical
          team: payments
        annotations:
          summary: "High payment failure rate"
          description: "Payment failure rate is {{ $value | humanizePercentage }}"

      # Inventory low
      - alert: InventoryLow
        expr: inventory_level < 100
        for: 5m
        labels:
          severity: warning
          team: inventory
        annotations:
          summary: "Low inventory for {{ $labels.product }}"
          description: "Only {{ $value }} units remaining"
```

`monitoring/prometheus/alerts/infrastructure.yml`:

yaml

```yaml
groups:
  - name: infrastructure_alerts
    interval: 30s
    rules:
      # Service down
      - alert: ServiceDown
        expr: up == 0
        for: 1m
        labels:
          severity: critical
          team: sre
        annotations:
          summary: "Service {{ $labels.job }} is down"
          description: "{{ $labels.instance }} has been down for more than 1 minute"

      # High CPU
      - alert: HighCPU
        expr: |
          100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
        for: 5m
        labels:
          severity: warning
          team: infrastructure
        annotations:
          summary: "High CPU on {{ $labels.instance }}"
          description: "CPU usage is {{ $value }}%"

      # High memory
      - alert: HighMemory
        expr: |
          (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 85
        for: 5m
        labels:
          severity: warning
          team: infrastructure
        annotations:
          summary: "High memory on {{ $labels.instance }}"
          description: "Memory usage is {{ $value }}%"

      # Database connection pool
      - alert: DatabaseConnectionPoolHigh
        expr: |
          pg_stat_activity_count / pg_settings_max_connections > 0.8
        for: 5m
        labels:
          severity: warning
          team: database
        annotations:
          summary: "Database connection pool near limit"
          description: "Using {{ $value | humanizePercentage }} of connections"

      # Redis memory
      - alert: RedisMemoryHigh
        expr: |
          redis_memory_used_bytes / redis_memory_max_bytes > 0.9
        for: 5m
        labels:
          severity: warning
          team: cache
        annotations:
          summary: "Redis memory usage high"
          description: "Redis using {{ $value | humanizePercentage }} of max memory"
```

### 3.3 SLO Configuration

`monitoring/prometheus/alerts/slo.yml`:

yaml

```yaml
groups:
  - name: slo_alerts
    interval: 30s
    rules:
      # Fast burn rate (2h to exhaustion)
      - alert: ErrorBudgetBurnRateFast
        expr: |
          (
            (1 - (sum(rate(http_requests_total{service="api-gateway",status!~"5.."}[5m])) / sum(rate(http_requests_total{service="api-gateway"}[5m]))))
            /
            (1 - 0.999)
          ) > 14.4
        for: 2m
        labels:
          severity: critical
          slo: api_availability
          burn_rate: fast
        annotations:
          summary: "Fast error budget burn on API Gateway"
          description: "Error budget will be exhausted in 2 hours at current rate"

      # Slow burn rate (5d to exhaustion)
      - alert: ErrorBudgetBurnRateSlow
        expr: |
          (
            (1 - (sum(rate(http_requests_total{service="api-gateway",status!~"5.."}[1h])) / sum(rate(http_requests_total{service="api-gateway"}[1h]))))
            /
            (1 - 0.999)
          ) > 6
        for: 15m
        labels:
          severity: warning
          slo: api_availability
          burn_rate: slow
        annotations:
          summary: "Slow error budget burn on API Gateway"
          description: "Error budget will be exhausted in 5 days at current rate"

      # SLO violation
      - alert: SLOViolation
        expr: |
          (
            sum(rate(http_requests_total{service="api-gateway",status!~"5.."}[30d]))
            /
            sum(rate(http_requests_total{service="api-gateway"}[30d]))
          ) < 0.999
        for: 5m
        labels:
          severity: critical
          slo: api_availability
        annotations:
          summary: "API Gateway SLO violated"
          description: "30-day availability: {{ $value | humanizePercentage }}"
```

`monitoring/prometheus/recording-rules/slo-rules.yml`:

yaml

```yaml
groups:
  - name: slo_recording_rules
    interval: 30s
    rules:
      # API Gateway availability
      - record: slo:api_gateway:availability:ratio
        expr: |
          sum(rate(http_requests_total{service="api-gateway",status!~"5.."}[5m]))
          /
          sum(rate(http_requests_total{service="api-gateway"}[5m]))

      # API Gateway error budget remaining
      - record: slo:api_gateway:error_budget:ratio
        expr: |
          1 - (
            (1 - slo:api_gateway:availability:ratio)
            /
            (1 - 0.999)
          )

      # Payment service availability
      - record: slo:payment:availability:ratio
        expr: |
          sum(rate(payments_processed_total{status="success"}[5m]))
          /
          sum(rate(payments_processed_total[5m]))
```

---

## –ß–∞—Å—Ç—å 4: Grafana Dashboards (15 –º–∏–Ω—É—Ç)

### 4.1 Datasources Configuration

`monitoring/grafana/datasources/datasources.yml`:

yaml

```yaml
apiVersion: 1

datasources:
  - name: Prometheus
    type: prometheus
    access: proxy
    url: http://prometheus:9090
    isDefault: true
    editable: true
    jsonData:
      httpMethod: POST
      timeInterval: 30s
      exemplarTraceIdDestinations:
        - name: trace_id
          datasourceUid: tempo

  - name: Loki
    type: loki
    access: proxy
    url: http://loki:3100
    editable: true
    jsonData:
      maxLines: 1000
      derivedFields:
        - datasourceUid: tempo
          matcherRegex: "trace_id=(\\w+)"
          name: TraceID
          url: "$${__value.raw}"

  - name: Tempo
    type: tempo
    access: proxy
    url: http://tempo:3200
    editable: true
    jsonData:
      tracesToLogs:
        datasourceUid: 'loki'
        tags: ['service', 'trace_id']
      serviceMap:
        datasourceUid: 'prometheus'
```

### 4.2 Dashboard Provisioning

`monitoring/grafana/provisioning/dashboards.yml`:

yaml

```yaml
apiVersion: 1

providers:
  - name: 'Default'
    orgId: 1
    folder: ''
    type: file
    disableDeletion: false
    updateIntervalSeconds: 10
    allowUiUpdates: true
    options:
      path: /etc/grafana/provisioning/dashboards
```

### 4.3 Main Overview Dashboard

`monitoring/grafana/dashboards/overview.json`:

json

```json
{
  "dashboard": {
    "title": "E-Commerce Overview",
    "tags": ["overview", "production"],
    "timezone": "browser",
    "panels": [
      {
        "id": 1,
        "title": "Request Rate",
        "type": "timeseries",
        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 0},
        "targets": [
          {
            "datasource": "Prometheus",
            "expr": "sum(rate(http_requests_total[5m])) by (service)",
            "legendFormat": "{{service}}"
          }
        ]
      },
      {
        "id": 2,
        "title": "Error Rate",
        "type": "timeseries",
        "gridPos": {"h": 8, "w": 12, "x": 12, "y": 0},
        "targets": [
          {
            "datasource": "Prometheus",
            "expr": "sum(rate(http_requests_total{status=~\"5..\"}[5m])) by (service) / sum(rate(http_requests_total[5m])) by (service)",
            "legendFormat": "{{service}}"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "percentunit"
          }
        }
      },
      {
        "id": 3,
        "title": "P95 Latency",
        "type": "timeseries",
        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 8},
        "targets": [
          {
            "datasource": "Prometheus",
            "expr": "histogram_quantile(0.95, sum(rate(http_request_duration_seconds_bucket[5m])) by (service, le))",
            "legendFormat": "{{service}}"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "s"
          }
        }
      },
      {
        "id": 4,
        "title": "Active Services",
        "type": "stat",
        "gridPos": {"h": 4, "w": 6, "x": 12, "y": 8},
        "targets": [
          {
            "datasource": "Prometheus", "expr": "count(up == 1)" } ] }, { "id": 5, "title": "Total Orders", "type": "stat", "gridPos": {"h": 4, "w": 6, "x": 18, "y": 8}, "targets": [ { "datasource": "Prometheus", "expr": "sum(orders_created_total)" } ] }, { "id": 6, "title": "Revenue (last hour)", "type": "stat", "gridPos": {"h": 4, "w": 6, "x": 12, "y": 12}, "targets": [ { "datasource": "Prometheus", "expr": "sum(increase(revenue_total_cents[1h])) / 100" } ], "fieldConfig": { "defaults": { "unit": "currencyUSD" } } }, { "id": 7, "title": "Payment Success Rate", "type": "gauge", "gridPos": {"h": 4, "w": 6, "x": 18, "y": 12}, "targets": [ { "datasource": "Prometheus", "expr": "sum(rate(payments_processed_total{status="success"}[5m])) / sum(rate(payments_processed_total[5m]))" } ], "fieldConfig": { "defaults": { "unit": "percentunit", "min": 0, "max": 1, "thresholds": { "steps": [ {"value": 0, "color": "red"}, {"value": 0.95, "color": "yellow"}, {"value": 0.99, "color": "green"} ] } } } }, { "id": 8, "title": "Service Map", "type": "nodeGraph", "gridPos": {"h": 12, "w": 24, "x": 0, "y": 16}, "targets": [ { "datasource": "Tempo", "queryType": "serviceMap" } ] } ] } }
            
            
```

### 4.4 SLO Dashboard

`monitoring/grafana/dashboards/slo-dashboard.json`:

json

```json
{
  "dashboard": {
    "title": "SLO Dashboard",
    "tags": ["slo", "sre"],
    "timezone": "browser",
    "refresh": "30s",
    "panels": [
      {
        "id": 1,
        "title": "API Gateway SLO Status",
        "type": "gauge",
        "gridPos": {"h": 8, "w": 8, "x": 0, "y": 0},
        "targets": [
          {
            "datasource": "Prometheus",
            "expr": "slo:api_gateway:availability:ratio * 100",
            "legendFormat": "Availability"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "percent",
            "min": 99,
            "max": 100,
            "thresholds": {
              "steps": [
                {"value": 99, "color": "red"},
                {"value": 99.9, "color": "yellow"},
                {"value": 99.95, "color": "green"}
              ]
            }
          }
        },
        "options": {
          "showThresholdLabels": true,
          "showThresholdMarkers": true
        }
      },
      {
        "id": 2,
        "title": "Error Budget Remaining",
        "type": "stat",
        "gridPos": {"h": 8, "w": 8, "x": 8, "y": 0},
        "targets": [
          {
            "datasource": "Prometheus",
            "expr": "slo:api_gateway:error_budget:ratio * 100"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "percent",
            "thresholds": {
              "steps": [
                {"value": 0, "color": "red"},
                {"value": 25, "color": "orange"},
                {"value": 50, "color": "yellow"},
                {"value": 75, "color": "green"}
              ]
            }
          }
        }
      },
      {
        "id": 3,
        "title": "Burn Rate (1h window)",
        "type": "timeseries",
        "gridPos": {"h": 8, "w": 8, "x": 16, "y": 0},
        "targets": [
          {
            "datasource": "Prometheus",
            "expr": "(1 - (sum(rate(http_requests_total{service=\"api-gateway\",status!~\"5..\"}[1h])) / sum(rate(http_requests_total{service=\"api-gateway\"}[1h])))) / (1 - 0.999)",
            "legendFormat": "Burn Rate"
          },
          {
            "datasource": "Prometheus",
            "expr": "14.4",
            "legendFormat": "Critical (2h)"
          },
          {
            "datasource": "Prometheus",
            "expr": "6",
            "legendFormat": "Warning (5d)"
          }
        ]
      },
      {
        "id": 4,
        "title": "30-Day Availability Trend",
        "type": "timeseries",
        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 8},
        "targets": [
          {
            "datasource": "Prometheus",
            "expr": "(sum(rate(http_requests_total{service=\"api-gateway\",status!~\"5..\"}[30d])) / sum(rate(http_requests_total{service=\"api-gateway\"}[30d]))) * 100"
          },
          {
            "datasource": "Prometheus",
            "expr": "99.9",
            "legendFormat": "SLO Target"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "percent",
            "min": 99,
            "max": 100
          }
        }
      },
      {
        "id": 5,
        "title": "Error Budget Consumption by Day",
        "type": "bargauge",
        "gridPos": {"h": 8, "w": 12, "x": 12, "y": 8},
        "targets": [
          {
            "datasource": "Prometheus",
            "expr": "(1 - slo:api_gateway:error_budget:ratio) * 100"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "percent",
            "max": 100
          }
        },
        "options": {
          "orientation": "horizontal",
          "displayMode": "gradient"
        }
      },
      {
        "id": 6,
        "title": "SLI Components",
        "type": "table",
        "gridPos": {"h": 8, "w": 24, "x": 0, "y": 16},
        "targets": [
          {
            "datasource": "Prometheus",
            "expr": "slo:api_gateway:availability:ratio",
            "format": "table",
            "instant": true
          }
        ]
      }
    ]
  }
}
```

### 4.5 Business Metrics Dashboard

`monitoring/grafana/dashboards/business-metrics.json`:

json

```json
{
  "dashboard": {
    "title": "Business Metrics",
    "tags": ["business", "kpi"],
    "panels": [
      {
        "id": 1,
        "title": "Orders per Minute",
        "type": "timeseries",
        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 0},
        "targets": [
          {
            "expr": "sum(rate(orders_created_total[5m])) * 60"
          }
        ]
      },
      {
        "id": 2,
        "title": "Revenue per Hour",
        "type": "timeseries",
        "gridPos": {"h": 8, "w": 12, "x": 12, "y": 0},
        "targets": [
          {
            "expr": "sum(rate(revenue_total_cents[5m])) * 3600 / 100"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "currencyUSD"
          }
        }
      },
      {
        "id": 3,
        "title": "Payment Success Rate",
        "type": "stat",
        "gridPos": {"h": 6, "w": 6, "x": 0, "y": 8},
        "targets": [
          {
            "expr": "sum(rate(payments_processed_total{status=\"success\"}[5m])) / sum(rate(payments_processed_total[5m]))"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "percentunit"
          }
        }
      },
      {
        "id": 4,
        "title": "Average Order Value",
        "type": "stat",
        "gridPos": {"h": 6, "w": 6, "x": 6, "y": 8},
        "targets": [
          {
            "expr": "sum(rate(revenue_total_cents[5m])) / sum(rate(orders_created_total[5m])) / 100"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "currencyUSD"
          }
        }
      },
      {
        "id": 5,
        "title": "Inventory by Product",
        "type": "bargauge",
        "gridPos": {"h": 6, "w": 12, "x": 12, "y": 8},
        "targets": [
          {
            "expr": "inventory_level"
          }
        ],
        "options": {
          "orientation": "horizontal"
        }
      },
      {
        "id": 6,
        "title": "Top Products by Reservations",
        "type": "piechart",
        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 14},
        "targets": [
          {
            "expr": "sum(rate(inventory_reservations_total[1h])) by (product)"
          }
        ]
      },
      {
        "id": 7,
        "title": "Payment Amount Distribution",
        "type": "histogram",
        "gridPos": {"h": 8, "w": 12, "x": 12, "y": 14},
        "targets": [
          {
            "expr": "payment_amount_cents"
          }
        ]
      }
    ]
  }
}
```

---

## –ß–∞—Å—Ç—å 5: OpenTelemetry Collector Configuration (10 –º–∏–Ω—É—Ç)

`monitoring/otel-collector/otel-config.yml`:

yaml

```yaml
receivers:
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317
      http:
        endpoint: 0.0.0.0:4318

processors:
  batch:
    timeout: 10s
    send_batch_size: 1024

  memory_limiter:
    check_interval: 1s
    limit_mib: 512

  # Tail sampling - save errors and slow requests
  tail_sampling:
    decision_wait: 10s
    num_traces: 100
    expected_new_traces_per_sec: 10
    policies:
      - name: error-traces
        type: status_code
        status_code:
          status_codes: [ERROR]
      
      - name: slow-traces
        type: latency
        latency:
          threshold_ms: 1000
      
      - name: probabilistic-sample
        type: probabilistic
        probabilistic:
          sampling_percentage: 10

  resource:
    attributes:
      - key: environment
        value: production
        action: insert

  # Add span metrics
  spanmetrics:
    metrics_exporter: prometheus
    latency_histogram_buckets: [100ms, 250ms, 500ms, 1s, 2s, 5s, 10s]
    dimensions:
      - name: http.method
      - name: http.status_code

exporters:
  # Tempo for traces
  otlp/tempo:
    endpoint: tempo:4317
    tls:
      insecure: true

  # Prometheus for metrics
  prometheus:
    endpoint: "0.0.0.0:8889"
    namespace: otel
    const_labels:
      environment: production

  # Loki for logs
  loki:
    endpoint: http://loki:3100/loki/api/v1/push
    labels:
      attributes:
        service.name: "service"
        severity: "severity"

  # Logging for debugging
  logging:
    loglevel: info
    sampling_initial: 5
    sampling_thereafter: 200

service:
  pipelines:
    traces:
      receivers: [otlp]
      processors: [memory_limiter, tail_sampling, batch, resource, spanmetrics]
      exporters: [otlp/tempo, logging]
    
    metrics:
      receivers: [otlp]
      processors: [memory_limiter, batch, resource]
      exporters: [prometheus]
    
    logs:
      receivers: [otlp]
      processors: [memory_limiter, batch, resource]
      exporters: [loki, logging]
```

---

## –ß–∞—Å—Ç—å 6: Alertmanager Configuration (5 –º–∏–Ω—É—Ç)

`monitoring/alertmanager/alertmanager.yml`:

yaml

```yaml
global:
  resolve_timeout: 5m
  slack_api_url: 'https://hooks.slack.com/services/YOUR/WEBHOOK/URL'

templates:
  - '/etc/alertmanager/templates/*.tmpl'

route:
  receiver: 'default'
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 10s
  group_interval: 5m
  repeat_interval: 4h
  
  routes:
    # Critical alerts
    - match:
        severity: critical
      receiver: critical-alerts
      group_wait: 10s
      repeat_interval: 1h
      continue: true

    # SLO alerts
    - match:
        slo: api_availability
      receiver: slo-alerts
      group_wait: 30s
      repeat_interval: 2h
    
    # Business alerts
    - match_re:
        alertname: (PaymentFailureSpike|InventoryLow)
      receiver: business-alerts

    # Infrastructure alerts
    - match:
        team: infrastructure
      receiver: infrastructure-alerts

# Inhibition rules
inhibit_rules:
  # If service is down, don't alert on high error rate
  - source_match:
      alertname: ServiceDown
    target_match:
      severity: warning
    equal: ['service']

  # If fast burn rate is firing, don't alert on slow burn
  - source_match:
      burn_rate: fast
    target_match:
      burn_rate: slow
    equal: ['slo']

receivers:
  - name: 'default'
    slack_configs:
      - channel: '#monitoring'
        title: 'Alert: {{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'

  - name: 'critical-alerts'
    slack_configs:
      - channel: '#critical-alerts'
        title: 'üö® CRITICAL: {{ .GroupLabels.alertname }}'
        text: |
          {{ range .Alerts }}
          *Summary:* {{ .Annotations.summary }}
          *Description:* {{ .Annotations.description }}
          *Runbook:* {{ .Annotations.runbook }}
          {{ end }}
        send_resolved: true
        color: '{{ if eq .Status "firing" }}danger{{ else }}good{{ end }}'

  - name: 'slo-alerts'
    slack_configs:
      - channel: '#slo-alerts'
        title: 'üìä SLO Alert: {{ .GroupLabels.alertname }}'
        text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'

  - name: 'business-alerts'
    slack_configs:
      - channel: '#business-alerts'
        title: 'üíº Business Alert: {{ .GroupLabels.alertname }}'

  - name: 'infrastructure-alerts'
    slack_configs:
      - channel: '#infrastructure'
```

---

## –ß–∞—Å—Ç—å 7: Scripts & Automation (10 –º–∏–Ω—É—Ç)

### 7.1 Deployment Script

`scripts/deploy.sh`:

bash

```bash
#!/bin/bash
set -e

echo "üöÄ Deploying E-Commerce Monitoring Stack"

# Colors
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
NC='\033[0m'

# Check prerequisites
check_prerequisites() {
    echo "Checking prerequisites..."
    
    if ! command -v docker &> /dev/null; then
        echo -e "${RED}Docker is not installed${NC}"
        exit 1
    fi
    
    if ! command -v docker-compose &> /dev/null; then
        echo -e "${RED}Docker Compose is not installed${NC}"
        exit 1
    fi
    
    echo -e "${GREEN}‚úì Prerequisites check passed${NC}"
}

# Initialize database
init_database() {
    echo "Initializing database..."
    cat > init-db.sql <<EOF
CREATE TABLE IF NOT EXISTS orders (
    id SERIAL PRIMARY KEY,
    product VARCHAR(100),
    quantity INT,
    amount INT,
    status VARCHAR(20),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

CREATE TABLE IF NOT EXISTS inventory (
    id SERIAL PRIMARY KEY,
    product VARCHAR(100),
    stock INT,
    price INT
);

INSERT INTO inventory (product, stock, price) VALUES
    ('Widget', 1000, 1000),
    ('Gadget', 500, 2000),
    ('Doohickey', 750, 1500)
ON CONFLICT DO NOTHING;
EOF
    echo -e "${GREEN}‚úì Database initialized${NC}"
}

# Start services
start_services() {
    echo "Starting services..."
    docker-compose up -d
    echo -e "${GREEN}‚úì Services started${NC}"
}

# Wait for services to be healthy
wait_for_services() {
    echo "Waiting for services to be healthy..."
    
    services=("prometheus:9090" "grafana:3000" "loki:3100" "tempo:3200")
    
    for service in "${services[@]}"; do
        IFS=':' read -r name port <<< "$service"
        echo -n "Waiting for $name..."
        
        max_attempts=30
        attempt=0
        
        while [ $attempt -lt $max_attempts ]; do
            if curl -s "http://localhost:$port/health" > /dev/null 2>&1 || \
               curl -s "http://localhost:$port/ready" > /dev/null 2>&1 || \
               curl -s "http://localhost:$port/" > /dev/null 2>&1; then
                echo -e " ${GREEN}‚úì${NC}"
                break
            fi
            
            attempt=$((attempt + 1))
            sleep 2
            echo -n "."
        done
        
        if [ $attempt -eq $max_attempts ]; then
            echo -e " ${RED}‚úó${NC}"
            echo -e "${RED}Failed to start $name${NC}"
        fi
    done
}

# Display access information
show_info() {
    echo ""
    echo -e "${GREEN}========================================${NC}"
    echo -e "${GREEN}Deployment Complete!${NC}"
    echo -e "${GREEN}========================================${NC}"
    echo ""
    echo "üìä Access URLs:"
    echo "  Frontend:       http://localhost:8080"
    echo "  Grafana:        http://localhost:3000 (admin/admin)"
    echo "  Prometheus:     http://localhost:9090"
    echo "  Alertmanager:   http://localhost:9093"
    echo "  Tempo:          http://localhost:3200"
    echo ""
    echo "üìà Monitoring Features:"
    echo "  - Full observability (metrics, logs, traces)"
    echo "  - SLO monitoring with error budgets"
    echo "  - Business metrics tracking"
    echo "  - Automated alerting"
    echo ""
    echo "üß™ Test the application:"
    echo "  1. Open http://localhost:8080"
    echo "  2. Click 'Place Order' to generate traffic"
    echo "  3. View metrics in Grafana"
    echo ""
}

# Main execution
main() {
    check_prerequisites
    init_database
    start_services
    wait_for_services
    show_info
}

main
```

### 7.2 Load Generator

`scripts/load-test.js`:

javascript

```javascript
import http from 'k6/http';
import { sleep, check } from 'k6';
import { Rate, Trend } from 'k6/metrics';

// Custom metrics
const errorRate = new Rate('errors');
const orderDuration = new Trend('order_duration');

export const options = {
  stages: [
    { duration: '2m', target: 10 },   // Ramp up
    { duration: '5m', target: 10 },   // Steady state
    { duration: '2m', target: 20 },   // Spike
    { duration: '5m', target: 20 },   // Steady spike
    { duration: '2m', target: 0 },    // Ramp down
  ],
  thresholds: {
    'http_req_duration': ['p(95)<2000'], // 95% of requests under 2s
    'errors': ['rate<0.05'],              // Error rate < 5%
  },
};

const BASE_URL = 'http://frontend:8080';

export default function () {
  // Normal order (80%)
  if (Math.random() < 0.8) {
    const res = http.post(`${BASE_URL}/api/order`, JSON.stringify({
      product: 'Widget',
      quantity: Math.floor(Math.random() * 5) + 1
    }), {
      headers: { 'Content-Type': 'application/json' },
    });
    
    const success = check(res, {
      'order created': (r) => r.status === 200,
    });
    
    errorRate.add(!success);
    orderDuration.add(res.timings.duration);
  }
  
  // Slow order (10%)
  else if (Math.random() < 0.1) {
    http.post(`${BASE_URL}/api/order?slow=true`);
  }
  
  // Failing order (5%)
  else if (Math.random() < 0.05) {
    http.post(`${BASE_URL}/api/order?fail=true`);
  }
  
  // Check inventory (5%)
  else {
    http.get(`${BASE_URL}/api/inventory`);
  }
  
  sleep(Math.random() * 3 + 1); // Random sleep 1-4s
}
```

### 7.3 Backup Script

`scripts/backup.sh`:

bash

```bash
#!/bin/bash

BACKUP_DIR="/backup/monitoring"
DATE=$(date +%Y%m%d-%H%M%S)
RETENTION_DAYS=7

echo "Starting backup at $(date)"

mkdir -p "$BACKUP_DIR"

# Backup Grafana
echo "Backing up Grafana..."
docker exec grafana tar czf - /var/lib/grafana > "$BACKUP_DIR/grafana-$DATE.tar.gz"

# Backup Prometheus config
echo "Backing up Prometheus config..."
tar czf "$BACKUP_DIR/prometheus-config-$DATE.tar.gz" monitoring/prometheus/

# Backup alert configuration
tar czf "$BACKUP_DIR/alertmanager-config-$DATE.tar.gz" monitoring/alertmanager/

# Cleanup old backups
echo "Cleaning up old backups..."
find "$BACKUP_DIR" -name "*.tar.gz" -mtime +$RETENTION_DAYS -delete

echo "Backup completed at $(date)"
echo "Backup location: $BACKUP_DIR"
```

### 7.4 SLO Report Generator

`scripts/generate-slo-report.sh`:

bash

```bash
#!/bin/bash

PROMETHEUS_URL="http://localhost:9090"
OUTPUT_FILE="slo-report-$(date +%Y%m%d).md"

echo "Generating SLO Report..."

python3 - <<'EOF'
import requests
from datetime import datetime

PROMETHEUS_URL = "http://localhost:9090"

def query_prometheus(query):
    response = requests.get(
        f"{PROMETHEUS_URL}/api/v1/query",
        params={'query': query}
    )
    data = response.json()
    if data['status'] == 'success' and data['data']['result']:
        return float(data['data']['result'][0]['value'][1])
    return None

# Get metrics
availability = query_prometheus('slo:api_gateway:availability:ratio * 100')
error_budget = query_prometheus('slo:api_gateway:error_budget:ratio * 100')

print(f"# SLO Report - {datetime.now().strftime('%Y-%m-%d')}")
print()
print("## API Gateway")
print(f"- **Availability**: {availability:.4f}%")
print(f"- **Target**: 99.9%")
print(f"- **Status**: {'‚úÖ Met' if availability >= 99.9 else '‚ùå Violated'}")
print(f"- **Error Budget Remaining**: {error_budget:.2f}%")
print()

if error_budget < 25:
    print("‚ö†Ô∏è **ACTION REQUIRED**: Error budget critical. Implement code freeze.")
elif error_budget < 50:
    print("‚ö†Ô∏è **WARNING**: Error budget low. Reduce release frequency.")
else:
    print("‚úÖ Error budget healthy. Normal operations.")
EOF
```

### 7.5 Chaos Testing Script

`scripts/chaos-test.sh`:

bash

```bash
#!/bin/bash

echo "üî• Starting Chaos Engineering Tests"

# Test 1: Kill random service
chaos_kill_service() {
    services=("payment-service" "inventory-service" "api-gateway")
    service=${services[$RANDOM % ${#services[@]}]}
    
    echo "Test: Killing $service"
    docker kill $service
    sleep 30
    docker start $service
    echo "‚úì Service recovered"
}

# Test 2: Network latency
chaos_network_latency() {
    echo "Test: Adding network latency"
    docker exec payment-service tc qdisc add dev eth0 root netem delay 2000ms
    sleep 60
    docker exec payment-service tc qdisc del dev eth0 root
    echo "‚úì Network restored"
}

# Test 3: High CPU load
chaos_cpu_load() {
    echo "Test: Generating CPU load"
    docker exec api-gateway sh -c "yes > /dev/null &"
    PID=$!
    sleep 60
    docker exec api-gateway kill $PID
    echo "‚úì CPU load removed"
}

# Run tests
chaos_kill_service
sleep 60
chaos_network_latency
sleep 60
chaos_cpu_load

echo "üéâ Chaos tests completed"
```

---

## –ß–∞—Å—Ç—å 8: Documentation (5 –º–∏–Ω—É—Ç)

### 8.1 Main README

`README.md`:

markdown

````markdown
# E-Commerce Monitoring Solution

Complete production-ready monitoring stack for e-commerce platform with full observability.

## üèóÔ∏è Architecture

- **Applications**: 4 microservices (Frontend, API Gateway, Payment, Inventory)
- **Data Stores**: PostgreSQL, Redis
- **Observability**: Prometheus, Loki, Tempo, Grafana
- **Alerting**: Alertmanager with multi-channel notifications
- **SLO Monitoring**: Automated error budget tracking

## üöÄ Quick Start
```bash
# Clone repository
git clone 
cd ecommerce-monitoring

# Deploy stack
chmod +x scripts/deploy.sh
./scripts/deploy.sh

# Access services
# Frontend: http://localhost:8080
# Grafana: http://localhost:3000 (admin/admin)
```

## üìä Features

### Observability
- ‚úÖ Metrics collection (Prometheus)
- ‚úÖ Distributed tracing (Tempo)
- ‚úÖ Log aggregation (Loki)
- ‚úÖ Unified visualization (Grafana)

### Monitoring
- ‚úÖ Infrastructure metrics
- ‚úÖ Application metrics  
- ‚úÖ Business metrics (orders, revenue)
- ‚úÖ SLO tracking with error budgets

### Alerting
- ‚úÖ Multi-window burn rate alerts
- ‚úÖ Service health monitoring
- ‚úÖ Performance degradation detection
- ‚úÖ Business KPI alerts

## üìà Dashboards

1. **Overview Dashboard**: High-level system health
2. **SLO Dashboard**: Error budget and compliance
3. **Business Metrics**: Revenue, orders, conversions
4. **Service Mesh**: Dependency visualization

## üéØ SLO Targets

| Service | SLO | Error Budget |
|---------|-----|--------------|
| API Gateway | 99.9% | 43.2 min/month |
| Payment Service | 99.95% | 21.6 min/month |
| Inventory Service | 99.9% | 43.2 min/month |

## üß™ Testing
```bash
# Generate load
./scripts/load-test.sh

# Run chaos tests
./scripts/chaos-test.sh

# Generate SLO report
./scripts/generate-slo-report.sh
```

## üìö Documentation

- [Architecture](docs/architecture.md)
- [Runbooks](docs/runbooks/)
- [SLO Policy](docs/slo-policy.md)

## üîß Maintenance
```bash
# Backup
./scripts/backup.sh

# View logs
docker-compose logs -f

# Restart services
docker-compose restart

# Stop everything
docker-compose down
```

## üìû Support

- Slack: #monitoring-support
- On-call: oncall@example.com
````

### 8.2 Runbook Example

`docs/runbooks/high-error-rate.md`:

markdown

````markdown
# Runbook: High Error Rate

## Alert: HighErrorRate

**Severity**: Critical  
**Team**: Backend

## Symptoms

- Error rate > 5% for 2 minutes
- Users experiencing 5xx errors
- SLO at risk

## Investigation Steps

### 1. Identify affected service
```bash
# Check error rate by service
curl -s 'http://localhost:9090/api/v1/query?query=rate(http_requests_total{status=~"5.."}[5m])' | jq
```

### 2. Check recent deployments
```bash
# View recent changes
git log --since="1 hour ago"

# Check deployment times
kubectl get events --sort-by='.lastTimestamp'
```

### 3. Review traces

- Open Grafana ‚Üí Explore ‚Üí Tempo
- Filter by: `status=error` 
- Look for common patterns

### 4. Check dependencies
```promql
# Database errors
rate(pg_stat_database_errors[5m])

# Redis errors  
rate(redis_commands_failed_total[5m])

# External API errors
rate(http_requests_total{endpoint="external"}[5m])
```

## Resolution

### If deployment related:
```bash
# Rollback
kubectl rollout undo deployment/
```

### If database related:
```bash
# Check connections
SELECT count(*) FROM pg_stat_activity;

# Check slow queries
SELECT * FROM pg_stat_statements ORDER BY mean_exec_time DESC LIMIT 10;
```

### If external dependency:
- Enable circuit breaker
- Switch to fallback service
- Contact vendor

## Prevention

- [ ] Add integration tests
- [ ] Improve monitoring
- [ ] Update runbook
- [ ] Schedule post-mortem
````

---

## –ß–∞—Å—Ç—å 9: –§–∏–Ω–∞–ª—å–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ (5 –º–∏–Ω—É—Ç)

### 9.1 Verification Checklist

`VERIFICATION.md`:

markdown

```markdown
# Verification Checklist

## Services Health

- [ ] All containers running: `docker-compose ps`
- [ ] Frontend accessible: http://localhost:8080
- [ ] Grafana accessible: http://localhost:3000
- [ ] Prometheus accessible: http://localhost:9090
- [ ] Alertmanager accessible: http://localhost:9093

## Metrics Collection

- [ ] Prometheus targets all UP
- [ ] Metrics flowing from all services
- [ ] Node exporter metrics visible
- [ ] Application metrics visible

## Logging

- [ ] Logs visible in Loki
- [ ] Log correlation with traces working
- [ ] Log levels correct

## Tracing

- [ ] Traces visible in Tempo
- [ ] Service map populated
- [ ] Trace‚ÜíLog correlation working
- [ ] Span attributes correct

## Dashboards

- [ ] Overview dashboard loading
- [ ] SLO dashboard showing data
- [ ] Business metrics dashboard working
- [ ] All panels displaying correctly

## Alerting

- [ ] Alertmanager config loaded
- [ ] Test alert fires correctly
- [ ] Notifications working
- [ ] Inhibition rules working

## SLO Monitoring

- [ ] SLO metrics calculating correctly
- [ ] Error budget visible
- [ ] Burn rate alerts configured
- [ ] Recording rules working

## End-to-End Test

1. Place order through frontend
2. Verify trace appears in Tempo
3. Check logs in Loki
4. Verify metrics in Prometheus
5. See order count increase in
````markdown
1. Place order through frontend
2. Verify trace appears in Tempo
3. Check logs in Loki
4. Verify metrics in Prometheus
5. See order count increase in Grafana
6. Verify alert doesn't fire (healthy state)
7. Trigger slow order, verify latency alert
8. Trigger failing order, verify error alert

## Load Testing

- [ ] Load generator running
- [ ] Request rate stable
- [ ] Error rate within SLO
- [ ] Latency within targets
- [ ] No memory leaks over time

## SLO Compliance

- [ ] Availability > 99.9%
- [ ] Error budget > 50%
- [ ] Burn rate < warning threshold
- [ ] 30-day trend positive

## Documentation

- [ ] All runbooks accessible
- [ ] Architecture diagram accurate
- [ ] SLO policy documented
- [ ] Backup procedures tested

## Troubleshooting Common Issues

### Issue: Service not showing metrics
```bash
# Check service is running
docker ps | grep 

# Check metrics endpoint
curl http://localhost:/metrics

# Check Prometheus targets
curl http://localhost:9090/api/v1/targets | jq '.data.activeTargets[] | select(.health != "up")'
```

### Issue: Traces not appearing
```bash
# Check OTLP endpoint
curl http://localhost:4317

# Verify collector config
docker logs otel-collector | grep -i error

# Check trace in Tempo
curl http://localhost:3200/api/search
```

### Issue: Dashboards not loading
```bash
# Check Grafana logs
docker logs grafana

# Verify datasource connection
curl -u admin:admin http://localhost:3000/api/datasources

# Re-provision dashboards
docker restart grafana
```

### Issue: Alerts not firing
```bash
# Check Prometheus rules
curl http://localhost:9090/api/v1/rules

# Verify Alertmanager config
curl http://localhost:9093/api/v2/status

# Test alert manually
curl -X POST http://localhost:9093/api/v2/alerts
```

### 9.2 Integration Test Script

`scripts/integration-test.sh`:

bash

```bash
#!/bin/bash
set -e

# Colors
GREEN='\033[0;32m'
RED='\033[0;31m'
YELLOW='\033[1;33m'
NC='\033[0m'

FAILED=0

test_service() {
    local name=$1
    local url=$2
    local expected=$3
    
    echo -n "Testing $name... "
    
    response=$(curl -s -o /dev/null -w "%{http_code}" "$url")
    
    if [ "$response" -eq "$expected" ]; then
        echo -e "${GREEN}‚úì${NC}"
        return 0
    else
        echo -e "${RED}‚úó (got $response, expected $expected)${NC}"
        FAILED=$((FAILED + 1))
        return 1
    fi
}

test_metrics() {
    local name=$1
    local url=$2
    
    echo -n "Testing $name metrics... "
    
    if curl -s "$url" | grep -q "^# HELP"; then
        echo -e "${GREEN}‚úì${NC}"
        return 0
    else
        echo -e "${RED}‚úó${NC}"
        FAILED=$((FAILED + 1))
        return 1
    fi
}

test_prometheus_target() {
    local job=$1
    
    echo -n "Testing Prometheus target $job... "
    
    result=$(curl -s "http://localhost:9090/api/v1/targets" | \
        jq -r ".data.activeTargets[] | select(.labels.job==\"$job\") | .health")
    
    if [ "$result" = "up" ]; then
        echo -e "${GREEN}‚úì${NC}"
        return 0
    else
        echo -e "${RED}‚úó (status: $result)${NC}"
        FAILED=$((FAILED + 1))
        return 1
    fi
}

test_grafana_datasource() {
    local name=$1
    
    echo -n "Testing Grafana datasource $name... "
    
    result=$(curl -s -u admin:admin "http://localhost:3000/api/datasources/name/$name" | \
        jq -r '.basicAuth')
    
    if [ "$result" != "null" ] || [ -n "$result" ]; then
        echo -e "${GREEN}‚úì${NC}"
        return 0
    else
        echo -e "${RED}‚úó${NC}"
        FAILED=$((FAILED + 1))
        return 1
    fi
}

echo "========================================="
echo "Integration Tests"
echo "========================================="
echo ""

echo "## Service Health Checks"
test_service "Frontend" "http://localhost:8080/health" 200
test_service "API Gateway" "http://localhost:8081/health" 200
test_service "Payment Service" "http://localhost:8082/health" 200
test_service "Inventory Service" "http://localhost:8083/health" 200
echo ""

echo "## Metrics Endpoints"
test_metrics "Frontend" "http://localhost:8080/metrics"
test_metrics "API Gateway" "http://localhost:8081/metrics"
test_metrics "Payment Service" "http://localhost:8082/metrics"
test_metrics "Inventory Service" "http://localhost:8083/metrics"
echo ""

echo "## Monitoring Stack"
test_service "Prometheus" "http://localhost:9090/-/healthy" 200
test_service "Grafana" "http://localhost:3000/api/health" 200
test_service "Alertmanager" "http://localhost:9093/-/healthy" 200
test_service "Loki" "http://localhost:3100/ready" 200
test_service "Tempo" "http://localhost:3200/ready" 200
echo ""

echo "## Prometheus Targets"
test_prometheus_target "frontend"
test_prometheus_target "api-gateway"
test_prometheus_target "payment-service"
test_prometheus_target "inventory-service"
echo ""

echo "## Grafana Datasources"
test_grafana_datasource "Prometheus"
test_grafana_datasource "Loki"
test_grafana_datasource "Tempo"
echo ""

echo "## End-to-End Test"
echo -n "Creating test order... "
response=$(curl -s -X POST "http://localhost:8080/api/order" \
    -H "Content-Type: application/json" \
    -d '{"product":"Widget","quantity":1}')

if echo "$response" | jq -e '.order_id' > /dev/null 2>&1; then
    echo -e "${GREEN}‚úì${NC}"
    order_id=$(echo "$response" | jq -r '.order_id')
    echo "  Order ID: $order_id"
else
    echo -e "${RED}‚úó${NC}"
    FAILED=$((FAILED + 1))
fi
echo ""

# Wait for metrics to be scraped
echo "Waiting for metrics propagation (15s)..."
sleep 15

echo -n "Verifying order metrics... "
order_count=$(curl -s "http://localhost:9090/api/v1/query?query=orders_created_total" | \
    jq -r '.data.result[0].value[1]')

if [ -n "$order_count" ] && [ "$order_count" != "null" ]; then
    echo -e "${GREEN}‚úì${NC}"
    echo "  Total orders: $order_count"
else
    echo -e "${RED}‚úó${NC}"
    FAILED=$((FAILED + 1))
fi
echo ""

echo "========================================="
if [ $FAILED -eq 0 ]; then
    echo -e "${GREEN}All tests passed!${NC}"
    exit 0
else
    echo -e "${RED}$FAILED test(s) failed${NC}"
    exit 1
fi
```

---

## –ß–∞—Å—Ç—å 10: Production Deployment Guide (5 –º–∏–Ω—É—Ç)

### 10.1 Production Checklist

`docs/PRODUCTION.md`:

markdown

````markdown
# Production Deployment Guide

## Pre-Deployment Checklist

### Infrastructure
- [ ] Sufficient resources allocated (CPU, Memory, Disk)
- [ ] High availability configured (3+ replicas)
- [ ] Backup strategy in place
- [ ] Disaster recovery plan documented
- [ ] Network policies configured
- [ ] SSL/TLS certificates installed

### Security
- [ ] Default passwords changed
- [ ] Access control configured
- [ ] Secrets stored securely (Vault/AWS Secrets Manager)
- [ ] API keys rotated
- [ ] Network segmentation implemented
- [ ] Security scanning completed

### Monitoring
- [ ] All exporters configured
- [ ] Alert rules validated
- [ ] Notification channels tested
- [ ] SLO targets defined
- [ ] Dashboards reviewed
- [ ] Runbooks updated

### Data Retention
- [ ] Prometheus retention: 30 days
- [ ] Loki retention: 7 days
- [ ] Tempo retention: 7 days
- [ ] Long-term storage configured (Thanos/Mimir)
- [ ] Backup retention: 30 days

## Deployment Steps

### 1. Environment Preparation
```bash
# Set production environment
export ENVIRONMENT=production
export CLUSTER_NAME=prod-cluster

# Update configurations
sed -i 's/development/production/g' monitoring/*/config.yml
```

### 2. Resource Sizing

**Prometheus**
```yaml
resources:
  requests:
    memory: 8Gi
    cpu: 2
  limits:
    memory: 16Gi
    cpu: 4
```

**Grafana**
```yaml
resources:
  requests:
    memory: 2Gi
    cpu: 1
  limits:
    memory: 4Gi
    cpu: 2
```

**Loki**
```yaml
resources:
  requests:
    memory: 4Gi
    cpu: 2
  limits:
    memory: 8Gi
    cpu: 4
```

### 3. High Availability Setup

**Prometheus HA**
```yaml
# Use Thanos for HA
services:
  prometheus-1:
    image: prom/prometheus
    command:
      - --web.enable-lifecycle
      - --storage.tsdb.min-block-duration=2h
      - --storage.tsdb.max-block-duration=2h
    
  prometheus-2:
    image: prom/prometheus
    command:
      - --web.enable-lifecycle
      - --storage.tsdb.min-block-duration=2h
      - --storage.tsdb.max-block-duration=2h
    
  thanos-sidecar-1:
    image: thanosio/thanos
    command:
      - sidecar
      - --prometheus.url=http://prometheus-1:9090
      - --tsdb.path=/prometheus
      - --objstore.config-file=/etc/thanos/bucket.yml
    
  thanos-query:
    image: thanosio/thanos
    command:
      - query
      - --store=thanos-sidecar-1:10901
      - --store=thanos-sidecar-2:10901
```

### 4. Secrets Management

**Using Kubernetes Secrets**
```bash
# Create secrets
kubectl create secret generic grafana-admin \
  --from-literal=username=admin \
  --from-literal=password=$(openssl rand -base64 32)

kubectl create secret generic slack-webhook \
  --from-literal=url=$SLACK_WEBHOOK_URL

kubectl create secret generic pagerduty-key \
  --from-literal=key=$PAGERDUTY_INTEGRATION_KEY
```

**Using AWS Secrets Manager**
```bash
# Store secret
aws secretsmanager create-secret \
  --name monitoring/grafana-admin \
  --secret-string '{"username":"admin","password":"xxx"}'

# Retrieve in application
aws secretsmanager get-secret-value \
  --secret-id monitoring/grafana-admin \
  --query SecretString \
  --output text
```

### 5. TLS Configuration
```yaml
# Grafana with TLS
services:
  grafana:
    environment:
      - GF_SERVER_PROTOCOL=https
      - GF_SERVER_CERT_FILE=/etc/grafana/ssl/cert.pem
      - GF_SERVER_CERT_KEY=/etc/grafana/ssl/key.pem
    volumes:
      - ./ssl:/etc/grafana/ssl:ro
```

### 6. Deploy to Production
```bash
# Deploy with production values
./scripts/deploy.sh --env production

# Verify deployment
./scripts/integration-test.sh

# Enable monitoring
./scripts/enable-monitoring.sh
```

## Post-Deployment

### 1. Smoke Tests
```bash
# Run comprehensive tests
./scripts/smoke-test.sh

# Verify SLOs
./scripts/verify-slo.sh

# Check alert rules
./scripts/test-alerts.sh
```

### 2. Initial Monitoring
- Monitor for 24 hours
- Review all dashboards
- Verify alerts fire correctly
- Check resource usage
- Validate backup jobs

### 3. Team Handoff
- [ ] Documentation shared
- [ ] Runbooks reviewed
- [ ] On-call rotation set up
- [ ] Access granted
- [ ] Training completed

## Scaling Guidelines

### Horizontal Scaling
```yaml
# Prometheus
replicas: 3
sharding:
  enabled: true
  shards: 3

# Loki
replicas: 3
querier:
  replicas: 3
ingester:
  replicas: 3
```

### Vertical Scaling
Monitor these metrics:
- Prometheus: `prometheus_tsdb_head_series`
- Grafana: `grafana_api_response_time_seconds`
- Loki: `loki_ingester_memory_chunks`

Scale when:
- Memory > 80%
- CPU > 70%
- Disk > 85%
- Query latency > 5s

## Troubleshooting Production Issues

### High Memory Usage
```bash
# Check series cardinality
curl http://prometheus:9090/api/v1/status/tsdb | jq '.data.seriesCountByMetricName'

# Reduce cardinality
# - Add relabel_configs to drop high-cardinality labels
# - Increase scrape_interval
# - Use recording rules
```

### Slow Queries
```bash
# Enable query logging
--query.log-file=/var/log/prometheus/query.log

# Analyze slow queries
cat query.log | jq 'select(.duration > 5)'

# Optimize with recording rules
```

### Disk Space Issues
```bash
# Check disk usage
df -h /prometheus

# Reduce retention
--storage.tsdb.retention.time=15d
--storage.tsdb.retention.size=50GB

# Setup long-term storage
# Use Thanos/Mimir for historical data
```

## Maintenance Windows

### Planned Maintenance
```bash
# 1. Create silence in Alertmanager
amtool silence add --duration=2h --comment="Planned maintenance"

# 2. Notify stakeholders
./scripts/notify-maintenance.sh

# 3. Perform maintenance
./scripts/maintenance.sh

# 4. Verify system
./scripts/integration-test.sh

# 5. Remove silence
amtool silence expire 
```

### Emergency Rollback
```bash
# Quick rollback
docker-compose down
docker-compose -f docker-compose.yml -f docker-compose.backup.yml up -d

# Or use Kubernetes
kubectl rollout undo deployment/prometheus
```

## Support Contacts

- **On-Call**: oncall@example.com
- **Slack**: #monitoring-support
- **Escalation**: sre-lead@example.com
- **Emergency**: +1-555-0100
````

---

## –ß–∞—Å—Ç—å 11: Graduation & Next Steps (5 –º–∏–Ω—É—Ç)

### 11.1 Course Completion Certificate

`docs/COMPLETION.md`:

markdown

````markdown
# üéì Course Completion

Congratulations! You've completed the **Complete Monitoring for DevOps** course.

## What You've Learned

### Core Concepts ‚úÖ
- Four Golden Signals (Latency, Traffic, Errors, Saturation)
- USE and RED methods
- Observability pillars (Metrics, Logs, Traces)
- SLI/SLO/SLA and Error Budgets

### Technical Skills ‚úÖ
- Prometheus configuration and PromQL
- Grafana dashboards and alerting
- Loki for log aggregation
- Jaeger/Tempo for distributed tracing
- Alertmanager for notifications
- OpenTelemetry instrumentation

### Advanced Topics ‚úÖ
- Multi-window multi-burn-rate alerting
- Error Budget policies
- Infrastructure as Code for monitoring
- GitOps workflows
- SLO-based alerting
- Composite SLOs

### Production Readiness ‚úÖ
- High availability setup
- Security best practices
- Scaling strategies
- Disaster recovery
- Runbook creation
- Incident management

## Your Monitoring Stack

You now have a production-ready monitoring solution featuring:
- üìä Complete observability (metrics, logs, traces)
- üéØ SLO monitoring with error budgets
- üö® Intelligent alerting with multiple channels
- üìà Business metrics tracking
- üîÑ Automated deployment via IaC
- üìö Comprehensive documentation

## Next Steps

### Immediate (This Week)
1. **Deploy to staging**: Test in your staging environment
2. **Customize dashboards**: Adapt to your specific needs
3. **Define SLOs**: Set realistic targets for your services
4. **Write runbooks**: Document procedures for common issues

### Short-term (This Month)
1. **Production deployment**: Roll out with proper planning
2. **Team training**: Educate your team on the stack
3. **Alert tuning**: Reduce noise, improve signal
4. **Integration**: Connect with existing tools (Slack, PagerDuty)

### Long-term (This Quarter)
1. **Capacity planning**: Use metrics for resource forecasting
2. **Cost optimization**: Identify and reduce waste
3. **Advanced features**: Implement anomaly detection, AI-powered insights
4. **Continuous improvement**: Regular review and enhancement

## Advanced Learning Paths

### Path 1: Cloud-Native Monitoring
- Kubernetes monitoring with kube-prometheus-stack
- Service mesh observability (Istio, Linkerd)
- Cloud provider monitoring (AWS CloudWatch, GCP Monitoring)
- Multi-cluster monitoring with Thanos

**Resources**:
- Kubernetes Monitoring Guide: https://kubernetes.io/docs/tasks/debug/
- Istio Observability: https://istio.io/latest/docs/tasks/observability/
- Thanos Documentation: https://thanos.io/

### Path 2: Advanced Observability
- OpenTelemetry advanced features
- Continuous profiling (Pyroscope, Parca)
- Real User Monitoring (RUM)
- Synthetic monitoring
- Chaos engineering with monitoring

**Resources**:
- OpenTelemetry docs: https://opentelemetry.io/docs/
- Grafana Tempo deep dive: https://grafana.com/docs/tempo/
- Chaos Engineering book: "Chaos Engineering" by Casey Rosenthal

### Path 3: SRE Mastery
- SLO engineering at scale
- Error budget policies
- Incident management
- Post-mortem culture
- Toil reduction

**Resources**:
- "Site Reliability Engineering" (Google SRE book)
- "The Site Reliability Workbook"
- SLO workshop materials: https://sre.google/workbook/

### Path 4: AI/ML in Monitoring
- Anomaly detection
- Predictive alerting
- Intelligent incident management
- Auto-remediation
- AIOps platforms

**Resources**:
- Prometheus ML: https://github.com/grafana/cortex-tools
- Anomaly detection patterns
- AIOps platforms evaluation

## Community & Resources

### Official Documentation
- **Prometheus**: https://prometheus.io/docs/
- **Grafana**: https://grafana.com/docs/
- **OpenTelemetry**: https://opentelemetry.io/docs/

### Communities
- **CNCF Slack**: cloud-native.slack.com
- **Prometheus Users**: groups.google.com/forum/#!forum/prometheus-users
- **r/devops**: reddit.com/r/devops
- **SRE Weekly**: sreweekly.com

### Conferences
- **KubeCon + CloudNativeCon**
- **Observability Meetups**
- **SREcon**
- **Monitorama**

### Certifications
- **Certified Kubernetes Administrator (CKA)**
- **Prometheus Certified Associate**
- **AWS/GCP/Azure Monitoring Certifications**
- **Site Reliability Engineering Certification**

## Your Monitoring Journey
```
Where you started               Where you are now              Where you're going
     ‚ùì                              ‚úÖ                            üöÄ
                                    
Basic monitoring           Complete observability        Industry expert
Manual alerts              Intelligent alerting          Thought leader
Reactive approach          SLO-based decisions          Proactive optimization
Limited visibility         Full stack observability      Predictive systems
```

## Final Checklist

Before considering yourself "done", ensure:
- [ ] Complete monitoring stack running
- [ ] All tests passing
- [ ] Documentation complete
- [ ] Team trained
- [ ] Production deployed
- [ ] SLOs defined and tracked
- [ ] Runbooks written
- [ ] Alerts tuned and tested
- [ ] Backup and DR tested
- [ ] On-call rotation established

## Staying Current

Monitoring is an evolving field. Stay updated:
- Subscribe to monitoring blogs
- Follow industry leaders on Twitter/LinkedIn
- Participate in online communities
- Attend webinars and conferences
- Read new releases and changelogs
- Experiment with emerging tools

## Give Back

Help others on their monitoring journey:
- Write blog posts about your experience
- Contribute to open-source projects
- Share dashboards on grafana.com
- Answer questions in forums
- Mentor junior engineers
- Speak at meetups

## Thank You!

Thank you for completing this course. Remember:

> "Monitoring is not about the tools, it's about the insights and actions they enable."

Now go build amazing, reliable systems! üöÄ

---

**Course Author**: [Your Name]  
**Version**: 1.0  
**Last Updated**: 2025-01-05

**Feedback**: We'd love to hear about your experience!  
- Email: monitoring-course@example.com
- GitHub: github.com/your-org/monitoring-course
````

### 11.2 Final Exercise

`docs/FINAL_EXERCISE.md`:

markdown

```markdown
# Final Exercise: Build Your Own Monitoring Solution

## Objective
Apply everything you've learned to monitor a real application of your choice.

## Requirements

### 1. Choose Your Application
Select one of:
- Your existing production application
- Open-source project (e.g., WordPress, GitLab, Jenkins)
- Sample e-commerce/blog platform
- Your side project

### 2. Implement Complete Observability

#### Metrics (Required)
- [ ] Install Prometheus
- [ ] Add at least 3 exporters
- [ ] Create 5+ custom metrics
- [ ] Set up recording rules
- [ ] Configure scrape targets

#### Logs (Required)
- [ ] Set up Loki
- [ ] Configure log collection
- [ ] Parse structured logs
- [ ] Create log-based alerts
- [ ] Set up retention policies

#### Traces (Required)
- [ ] Implement distributed tracing
- [ ] Instrument your application
- [ ] Set up Tempo/Jaeger
- [ ] Create service dependency map
- [ ] Link traces with logs

#### Dashboards (Required)
- [ ] Create overview dashboard
- [ ] Create service-specific dashboards
- [ ] Add business metrics dashboard
- [ ] Include SLO dashboard
- [ ] Make them production-ready

#### Alerting (Required)
- [ ] Define 10+ alert rules
- [ ] Set up Alertmanager
- [ ] Configure notification channels
- [ ] Create inhibition rules
- [ ] Test all alerts

#### SLO Monitoring (Required)
- [ ] Define 3+ SLOs
- [ ] Set error budgets
- [ ] Create burn rate alerts
- [ ] Build SLO dashboard
- [ ] Document SLO policy

### 3. Documentation

Create:
- Architecture diagram
- Monitoring strategy document
- 5+ runbooks for common issues
- On-call procedures
- Disaster recovery plan

### 4. Infrastructure as Code

Use:
- Terraform for Grafana configuration
- Ansible for deployment, OR
- Helm charts for Kubernetes, OR
- Docker Compose for simple setups

### 5. Advanced Features (Choose 2)

- [ ] Implement continuous profiling
- [ ] Add synthetic monitoring
- [ ] Set up anomaly detection
- [ ] Create custom exporters
- [ ] Implement chaos testing
- [ ] Build cost tracking dashboard
- [ ] Add security monitoring
- [ ] Implement capacity planning

## Deliverables

Submit:
1. **GitHub repository** with:
   - All configuration files
   - Application code with instrumentation
   - Documentation
   - IaC scripts
   
2. **Video demo** (5-10 minutes) showing:
   - System overview
   - Key dashboards
   - Alert demonstration
   - Trace walkthrough
   
3. **Written report** including:
   - Design decisions
   - Challenges faced
   - SLO definitions
   - Future improvements

## Evaluation Criteria

### Technical Implementation (40%)
- Completeness of observability
- Quality of instrumentation
- Proper use of tools
- Security considerations

### Design & Architecture (20%)
- Scalability
- High availability
- Maintainability
- Best practices

### Documentation (20%)
- Clarity and completeness
- Runbooks quality
- Architecture diagrams
- Code comments

### Operational Readiness (20%)
- Production-ready setup
- Disaster recovery plan
- Alert quality
- SLO definitions

## Timeline

Recommended schedule:
- **Week 1**: Planning and setup
- **Week 2**: Implementation
- **Week 3**: Testing and tuning
- **Week 4**: Documentation and demo

## Getting Help

- Review course materials
- Check documentation
- Ask in community forums
- Consult with peers

## Bonus Challenges

Go above and beyond:
- Multi-region monitoring
- Custom AI/ML models
- Integration with CI/CD
- Automated remediation
- Cost optimization dashboard
- Mobile app monitoring
- User experience monitoring

## Success Stories

Share your project:
- Blog about your experience
- Present at a meetup
- Submit to CNCF showcase
- Add to your portfolio

Good luck! üöÄ
```

---

## üéâ –ü–æ–∑–¥—Ä–∞–≤–ª—è–µ–º —Å –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ–º –∫—É—Ä—Å–∞!

–í—ã –ø—Ä–æ—à–ª–∏ –ø–æ–ª–Ω—ã–π –ø—É—Ç—å –æ—Ç –±–∞–∑–æ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫ –¥–æ production-ready monitoring solution. –¢–µ–ø–µ—Ä—å —É –≤–∞—Å –µ—Å—Ç—å:

1. ‚úÖ **–ü–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π monitoring stack** —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏, –ª–æ–≥–∞–º–∏ –∏ —Ç—Ä–µ–π—Å–∞–º–∏
2. ‚úÖ **SLO –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥** —Å error budget tracking
3. ‚úÖ **–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π alerting** —Å multi-window burn rates
4. ‚úÖ **Production-ready –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞** —Å HA –∏ DR
5. ‚úÖ **–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è** —á–µ—Ä–µ–∑ IaC –∏ GitOps
6. ‚úÖ **–ü–æ–ª–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è** —Å runbooks –∏ procedures

**–°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:**

1. –ü—Ä–∏–º–µ–Ω–∏—Ç–µ –∑–Ω–∞–Ω–∏—è –∫ –≤–∞—à–∏–º —Ä–µ–∞–ª—å–Ω—ã–º –ø—Ä–æ–µ–∫—Ç–∞–º
2. –ü–æ–¥–µ–ª–∏—Ç–µ—Å—å –æ–ø—ã—Ç–æ–º —Å –∫–æ–º–∞–Ω–¥–æ–π
3. –ü—Ä–æ–¥–æ–ª–∂–∞–π—Ç–µ —É—á–∏—Ç—å—Å—è –∏ —Å–æ–≤–µ—Ä—à–µ–Ω—Å—Ç–≤–æ–≤–∞—Ç—å—Å—è
4. –ü–æ–º–æ–≥–∞–π—Ç–µ –¥—Ä—É–≥–∏–º –≤ –∏—Ö monitoring journey

**–ü–æ–º–Ω–∏—Ç–µ:** –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ - —ç—Ç–æ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å —É–ª—É—á—à–µ–Ω–∏—è. –£–¥–∞—á–∏! üöÄ
