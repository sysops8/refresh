# RabbitMQ Refresh: –ï–∂–µ–≥–æ–¥–Ω—ã–π/–ü–æ–ª—É–≥–æ–¥–æ–≤–æ–π –∫—É—Ä—Å –¥–ª—è DevOps

**–¶–µ–ª—å:** –û—Å–≤–µ–∂–∏—Ç—å –≤ –ø–∞–º—è—Ç–∏ –∫–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ RabbitMQ –∑–∞ 2-3 —á–∞—Å–∞ –ø—Ä–∞–∫—Ç–∏–∫–∏ –∏ —É–∑–Ω–∞—Ç—å 1-2 –Ω–æ–≤—ã–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏.

**–§–æ—Ä–º–∞—Ç:** –ö–∞–∂–¥—ã–π —Ä–∞–∑–¥–µ–ª —Å–æ—Å—Ç–æ–∏—Ç –∏–∑:
- **–ö—Ä–∞—Ç–∫–æ–π —Ç–µ–æ—Ä–∏–∏ (–ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞):** –°–∞–º–æ–µ –≥–ª–∞–≤–Ω–æ–µ, —á—Ç–æ –≤—ã –º–æ–≥–ª–∏ –∑–∞–±—ã—Ç—å
- **–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–≥–æ –∑–∞–¥–∞–Ω–∏—è:** –†–µ–∞–ª—å–Ω–∞—è –∑–∞–¥–∞—á–∞, –∫–æ—Ç–æ—Ä—É—é –Ω—É–∂–Ω–æ —Ä–µ—à–∏—Ç—å
- **–ë–æ–Ω—É—Å–Ω–æ–≥–æ –∑–∞–¥–∞–Ω–∏—è (–¥–ª—è —Ä–æ—Å—Ç–∞):** –ó–∞–¥–∞—á–∞ –ø–æ—Å–ª–æ–∂–Ω–µ–µ –∏–ª–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –Ω–æ–≤–æ–π —Ñ–∏—á–∏

## –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è:

- Docker —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω
- Python 3.8+ –∏–ª–∏ –¥—Ä—É–≥–æ–π —è–∑—ã–∫ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è
- –ë–∞–∑–æ–≤—ã–µ –∑–Ω–∞–Ω–∏—è messaging —Å–∏—Å—Ç–µ–º
- –î–æ—Å—Ç—É–ø –∫ —Ç–µ—Ä–º–∏–Ω–∞–ª—É

---

## –ú–æ–¥—É–ª—å 1: –û—Å–Ω–æ–≤—ã RabbitMQ –∏ –±—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç (20 –º–∏–Ω—É—Ç)

### üéØ –ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞

**–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ RabbitMQ:**

```
Producer ‚Üí Exchange ‚Üí Queue ‚Üí Consumer
           ‚Üì
       Bindings (routing rules)
```

**–û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Producer   ‚îÇ - –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏—è
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Exchange   ‚îÇ - –ú–∞—Ä—à—Ä—É—Ç–∏–∑–∏—Ä—É–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏—è –ø–æ –ø—Ä–∞–≤–∏–ª–∞–º
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ (binding + routing key)
       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    Queue    ‚îÇ - –•—Ä–∞–Ω–∏—Ç —Å–æ–æ–±—â–µ–Ω–∏—è
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Consumer   ‚îÇ - –ü–æ–ª—É—á–∞–µ—Ç –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏—è
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**–¢–∏–ø—ã Exchange:**

1. **Direct** - —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ routing key
2. **Fanout** - broadcast –≤—Å–µ–º –ø—Ä–∏–≤—è–∑–∞–Ω–Ω—ã–º queue
3. **Topic** - pattern matching —Å wildcards (* –∏ #)
4. **Headers** - –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è –ø–æ headers (—Ä–µ–¥–∫–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è)

**–ó–∞–ø—É—Å–∫ RabbitMQ:**

```bash
# Docker
docker run -d --name rabbitmq \
  -p 5672:5672 \
  -p 15672:15672 \
  -e RABBITMQ_DEFAULT_USER=admin \
  -e RABBITMQ_DEFAULT_PASS=admin \
  rabbitmq:3-management

# –ü—Ä–æ–≤–µ—Ä–∫–∞
docker logs rabbitmq
docker exec rabbitmq rabbitmqctl status

# Web UI
# http://localhost:15672
# Login: admin / admin
```

**–û—Å–Ω–æ–≤–Ω—ã–µ –ø–æ—Ä—Ç—ã:**

- `5672` - AMQP protocol
- `15672` - Management UI
- `25672` - Erlang distribution (clustering)
- `4369` - EPMD (Erlang Port Mapper Daemon)

**CLI –∫–æ–º–∞–Ω–¥—ã (rabbitmqctl):**

```bash
# –°—Ç–∞—Ç—É—Å
docker exec rabbitmq rabbitmqctl status
docker exec rabbitmq rabbitmqctl cluster_status

# –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏
docker exec rabbitmq rabbitmqctl list_users
docker exec rabbitmq rabbitmqctl add_user myuser mypass
docker exec rabbitmq rabbitmqctl set_user_tags myuser administrator
docker exec rabbitmq rabbitmqctl set_permissions -p / myuser ".*" ".*" ".*"

# Virtual Hosts
docker exec rabbitmq rabbitmqctl list_vhosts
docker exec rabbitmq rabbitmqctl add_vhost /dev
docker exec rabbitmq rabbitmqctl delete_vhost /dev

# Queues
docker exec rabbitmq rabbitmqctl list_queues name messages consumers
docker exec rabbitmq rabbitmqctl list_queues name messages_ready messages_unacknowledged

# Exchanges
docker exec rabbitmq rabbitmqctl list_exchanges name type

# Bindings
docker exec rabbitmq rabbitmqctl list_bindings

# Connections
docker exec rabbitmq rabbitmqctl list_connections name peer_host peer_port state
docker exec rabbitmq rabbitmqctl close_connection "<connection_name>" "Reason"

# Channels
docker exec rabbitmq rabbitmqctl list_channels connection name number
```

**Python –∫–ª–∏–µ–Ω—Ç (pika):**

```bash
pip install pika
```

```python
import pika

# –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ
connection = pika.BlockingConnection(
    pika.ConnectionParameters('localhost')
)
channel = connection.channel()

# –°–æ–∑–¥–∞–Ω–∏–µ queue
channel.queue_declare(queue='hello')

# –û—Ç–ø—Ä–∞–≤–∫–∞
channel.basic_publish(
    exchange='',
    routing_key='hello',
    body='Hello World!'
)

# –ü–æ–ª—É—á–µ–Ω–∏–µ
def callback(ch, method, properties, body):
    print(f"Received {body}")
    ch.basic_ack(delivery_tag=method.delivery_tag)

channel.basic_consume(
    queue='hello',
    on_message_callback=callback
)

channel.start_consuming()
```

### üíª –ó–∞–¥–∞–Ω–∏–µ

**–ü–æ–¥–≥–æ—Ç–æ–≤—å —Ç–µ—Å—Ç–æ–≤–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ:**

1. **–ó–∞–ø—É—Å—Ç–∏ RabbitMQ:**
```bash
docker run -d --name rabbitmq \
  -p 5672:5672 \
  -p 15672:15672 \
  -e RABBITMQ_DEFAULT_USER=admin \
  -e RABBITMQ_DEFAULT_PASS=admin \
  rabbitmq:3-management

# –î–æ–∂–¥–∏—Å—å –∑–∞–ø—É—Å–∫–∞
docker logs -f rabbitmq
```

2. **–ü—Ä–æ–≤–µ—Ä—å Web UI:**
   - –û—Ç–∫—Ä–æ–π http://localhost:15672
   - –ó–∞–ª–æ–≥–∏–Ω—å—Å—è: admin / admin
   - –ò–∑—É—á–∏ –≤–∫–ª–∞–¥–∫–∏: Overview, Connections, Channels, Exchanges, Queues

3. **–°–æ–∑–¥–∞–π –ø—Ä–æ—Å—Ç–æ–≥–æ Producer:**
```python
# producer.py
import pika
import sys

connection = pika.BlockingConnection(
    pika.ConnectionParameters('localhost')
)
channel = connection.channel()

channel.queue_declare(queue='test_queue', durable=True)

message = ' '.join(sys.argv[1:]) or "Hello World!"
channel.basic_publish(
    exchange='',
    routing_key='test_queue',
    body=message,
    properties=pika.BasicProperties(
        delivery_mode=2,  # make message persistent
    ))

print(f" [x] Sent '{message}'")
connection.close()
```

4. **–°–æ–∑–¥–∞–π –ø—Ä–æ—Å—Ç–æ–≥–æ Consumer:**
```python
# consumer.py
import pika
import time

connection = pika.BlockingConnection(
    pika.ConnectionParameters('localhost')
)
channel = connection.channel()

channel.queue_declare(queue='test_queue', durable=True)

def callback(ch, method, properties, body):
    print(f" [x] Received {body.decode()}")
    time.sleep(body.count(b'.'))
    print(" [x] Done")
    ch.basic_ack(delivery_tag=method.delivery_tag)

channel.basic_qos(prefetch_count=1)
channel.basic_consume(queue='test_queue', on_message_callback=callback)

print(' [*] Waiting for messages. To exit press CTRL+C')
channel.start_consuming()
```

5. **–ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–π:**
```bash
# –¢–µ—Ä–º–∏–Ω–∞–ª 1
python consumer.py

# –¢–µ—Ä–º–∏–Ω–∞–ª 2
python producer.py "First message."
python producer.py "Second message.."
python producer.py "Third message..."

# –ü—Ä–æ–≤–µ—Ä—å –≤ Web UI –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–æ–æ–±—â–µ–Ω–∏–π
```

### üöÄ –ë–æ–Ω—É—Å (–Ω–æ–≤–æ–µ)

**1. –ò—Å–ø–æ–ª—å–∑—É–π rabbitmqadmin –¥–ª—è CLI —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è:**

```bash
# –°–∫–∞—á–∞–π rabbitmqadmin
docker exec rabbitmq cat /usr/local/bin/rabbitmqadmin > rabbitmqadmin
chmod +x rabbitmqadmin

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
./rabbitmqadmin -u admin -p admin list queues
./rabbitmqadmin -u admin -p admin list exchanges
./rabbitmqadmin -u admin -p admin declare queue name=my_queue durable=true
./rabbitmqadmin -u admin -p admin publish routing_key=my_queue payload="test message"
```

**2. –ù–∞—Å—Ç—Ä–æ–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Å Prometheus:**

```bash
# –í–∫–ª—é—á–∏ prometheus plugin
docker exec rabbitmq rabbitmq-plugins enable rabbitmq_prometheus

# –ü—Ä–æ–≤–µ—Ä—å –º–µ—Ç—Ä–∏–∫–∏
curl http://localhost:15692/metrics
```

**3. –ò—Å–ø–æ–ª—å–∑—É–π Management API:**

```bash
# –ü–æ–ª—É—á–∏—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ queues
curl -u admin:admin http://localhost:15672/api/queues

# –ü–æ–ª—É—á–∏—Ç—å —Å–æ–æ–±—â–µ–Ω–∏—è –∏–∑ queue (non-destructive)
curl -u admin:admin http://localhost:15672/api/queues/%2F/test_queue/get \
  -d '{"count":5,"ackmode":"ack_requeue_true","encoding":"auto"}' \
  -H "Content-Type: application/json"

# –°–æ–∑–¥–∞—Ç—å queue
curl -u admin:admin -X PUT http://localhost:15672/api/queues/%2F/my_new_queue \
  -d '{"durable":true,"auto_delete":false}' \
  -H "Content-Type: application/json"
```

---

## –ú–æ–¥—É–ª—å 2: Exchanges –∏ Routing (25 –º–∏–Ω—É—Ç)

### üéØ –ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞

**Direct Exchange:**

```python
# Producer
channel.exchange_declare(exchange='direct_logs', exchange_type='direct')

channel.basic_publish(
    exchange='direct_logs',
    routing_key='error',  # exact match
    body=message
)

# Consumer
channel.queue_bind(
    exchange='direct_logs',
    queue=queue_name,
    routing_key='error'
)
```

**Fanout Exchange (Broadcast):**

```python
# Producer
channel.exchange_declare(exchange='logs', exchange_type='fanout')

channel.basic_publish(
    exchange='logs',
    routing_key='',  # ignored for fanout
    body=message
)

# Consumer
channel.queue_bind(exchange='logs', queue=queue_name)
```

**Topic Exchange (Pattern Matching):**

```python
# Producer
channel.exchange_declare(exchange='topic_logs', exchange_type='topic')

channel.basic_publish(
    exchange='topic_logs',
    routing_key='kern.critical',  # pattern: <word>.<word>
    body=message
)

# Consumer
channel.queue_bind(
    exchange='topic_logs',
    queue=queue_name,
    routing_key='kern.*'      # * = –æ–¥–∏–Ω word
)

channel.queue_bind(
    exchange='topic_logs',
    queue=queue_name,
    routing_key='*.critical'  # # = –Ω–æ–ª—å –∏–ª–∏ –±–æ–ª—å—à–µ words
)
```

**Topic Wildcards:**

- `*` (star) - –∑–∞–º–µ–Ω—è–µ—Ç —Ä–æ–≤–Ω–æ –æ–¥–Ω–æ —Å–ª–æ–≤–æ
- `#` (hash) - –∑–∞–º–µ–Ω—è–µ—Ç –Ω–æ–ª—å –∏–ª–∏ –±–æ–ª—å—à–µ —Å–ª–æ–≤

**–ü—Ä–∏–º–µ—Ä—ã routing keys:**

```
"stock.usd.nyse"
"nyse.vmw"
"quick.orange.rabbit"
"lazy.orange.elephant"

Bindings:
"*.orange.*"     ‚Üí quick.orange.rabbit, lazy.orange.elephant
"*.*.rabbit"     ‚Üí quick.orange.rabbit
"lazy.#"         ‚Üí lazy.orange.elephant, lazy.pink.rabbit
"#"              ‚Üí –≤—Å–µ —Å–æ–æ–±—â–µ–Ω–∏—è
```

**Headers Exchange:**

```python
# Producer
channel.exchange_declare(exchange='headers_logs', exchange_type='headers')

channel.basic_publish(
    exchange='headers_logs',
    routing_key='',
    body=message,
    properties=pika.BasicProperties(
        headers={'format': 'pdf', 'type': 'report'}
    )
)

# Consumer
channel.queue_bind(
    exchange='headers_logs',
    queue=queue_name,
    arguments={
        'x-match': 'all',  # –∏–ª–∏ 'any'
        'format': 'pdf',
        'type': 'report'
    }
)
```

**–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π Exchange (fallback):**

```python
channel.exchange_declare(
    exchange='main_exchange',
    exchange_type='direct',
    arguments={
        'alternate-exchange': 'unrouted_exchange'
    }
)

# –°–æ–æ–±—â–µ–Ω–∏—è –±–µ–∑ matching binding –ø–æ–ø–∞–¥—É—Ç –≤ alternate exchange
```

### üíª –ó–∞–¥–∞–Ω–∏–µ

**–°–æ–∑–¥–∞–π —Å–∏—Å—Ç–µ–º—É –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —Å —Ä–∞–∑–Ω—ã–º–∏ —É—Ä–æ–≤–Ω—è–º–∏:**

**1. –°–æ–∑–¥–∞–π producer –¥–ª—è –ª–æ–≥–æ–≤:**

```python
# log_producer.py
import pika
import sys

connection = pika.BlockingConnection(
    pika.ConnectionParameters('localhost')
)
channel = connection.channel()

channel.exchange_declare(exchange='logs_direct', exchange_type='direct')

severity = sys.argv[1] if len(sys.argv) > 1 else 'info'
message = ' '.join(sys.argv[2:]) or 'Hello World!'

channel.basic_publish(
    exchange='logs_direct',
    routing_key=severity,
    body=message
)

print(f" [x] Sent {severity}: {message}")
connection.close()
```

**2. –°–æ–∑–¥–∞–π consumer –¥–ª—è –æ—à–∏–±–æ–∫:**

```python
# log_consumer_errors.py
import pika

connection = pika.BlockingConnection(
    pika.ConnectionParameters('localhost')
)
channel = connection.channel()

channel.exchange_declare(exchange='logs_direct', exchange_type='direct')

result = channel.queue_declare(queue='', exclusive=True)
queue_name = result.method.queue

severities = ['error', 'critical']

for severity in severities:
    channel.queue_bind(
        exchange='logs_direct',
        queue=queue_name,
        routing_key=severity
    )

print(' [*] Waiting for error logs. To exit press CTRL+C')

def callback(ch, method, properties, body):
    print(f" [x] {method.routing_key}: {body.decode()}")

channel.basic_consume(
    queue=queue_name,
    on_message_callback=callback,
    auto_ack=True
)

channel.start_consuming()
```

**3. –°–æ–∑–¥–∞–π Topic exchange –¥–ª—è —Å–ª–æ–∂–Ω–æ–π –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏–∏:**

```python
# log_producer_topic.py
import pika
import sys

connection = pika.BlockingConnection(
    pika.ConnectionParameters('localhost')
)
channel = connection.channel()

channel.exchange_declare(exchange='topic_logs', exchange_type='topic')

routing_key = sys.argv[1] if len(sys.argv) > 2 else 'anonymous.info'
message = ' '.join(sys.argv[2:]) or 'Hello World!'

channel.basic_publish(
    exchange='topic_logs',
    routing_key=routing_key,
    body=message
)

print(f" [x] Sent {routing_key}: {message}")
connection.close()
```

**4. –°–æ–∑–¥–∞–π consumer —Å pattern matching:**

```python
# log_consumer_topic.py
import pika
import sys

connection = pika.BlockingConnection(
    pika.ConnectionParameters('localhost')
)
channel = connection.channel()

channel.exchange_declare(exchange='topic_logs', exchange_type='topic')

result = channel.queue_declare(queue='', exclusive=True)
queue_name = result.method.queue

binding_keys = sys.argv[1:] if len(sys.argv) > 1 else ['#']

for binding_key in binding_keys:
    channel.queue_bind(
        exchange='topic_logs',
        queue=queue_name,
        routing_key=binding_key
    )

print(f' [*] Waiting for logs matching: {binding_keys}')

def callback(ch, method, properties, body):
    print(f" [x] {method.routing_key}: {body.decode()}")

channel.basic_consume(
    queue=queue_name,
    on_message_callback=callback,
    auto_ack=True
)

channel.start_consuming()
```

**5. –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–π:**

```bash
# –¢–µ—Ä–º–∏–Ω–∞–ª 1: —Å–ª—É—à–∞–µ–º –≤—Å–µ kern.* –ª–æ–≥–∏
python log_consumer_topic.py "kern.*"

# –¢–µ—Ä–º–∏–Ω–∞–ª 2: —Å–ª—É—à–∞–µ–º –∫—Ä–∏—Ç–∏—á–Ω—ã–µ –ª–æ–≥–∏
python log_consumer_topic.py "*.critical"

# –¢–µ—Ä–º–∏–Ω–∞–ª 3: –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º
python log_producer_topic.py "kern.critical" "A critical kernel error"
python log_producer_topic.py "kern.info" "Kernel information"
python log_producer_topic.py "app.critical" "Application crashed"

# –ü—Ä–æ–≤–µ—Ä—å, –∫—Ç–æ —á—Ç–æ –ø–æ–ª—É—á–∏–ª
```

### üöÄ –ë–æ–Ω—É—Å (–Ω–æ–≤–æ–µ)

**1. Consistent Hash Exchange (–¥–ª—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –Ω–∞–≥—Ä—É–∑–∫–∏):**

```bash
# –í–∫–ª—é—á–∏ plugin
docker exec rabbitmq rabbitmq-plugins enable rabbitmq_consistent_hash_exchange
```

```python
channel.exchange_declare(
    exchange='hash_exchange',
    exchange_type='x-consistent-hash'
)

# –ë–∏–Ω–¥–∏–Ω–≥ —Å –≤–µ—Å–∞–º–∏
channel.queue_bind(
    exchange='hash_exchange',
    queue='queue1',
    routing_key='1'  # –≤–µ—Å
)

channel.queue_bind(
    exchange='hash_exchange',
    queue='queue2',
    routing_key='2'  # –¥–≤–æ–π–Ω–æ–π –≤–µ—Å
)
```

**2. Exchange to Exchange Binding:**

```python
# –°–æ–∑–¥–∞—ë–º –∏–µ—Ä–∞—Ä—Ö–∏—é exchanges
channel.exchange_declare(exchange='main', exchange_type='topic')
channel.exchange_declare(exchange='errors', exchange_type='fanout')

# –ë–∏–Ω–¥–∏–º exchange –∫ exchange
channel.exchange_bind(
    destination='errors',
    source='main',
    routing_key='*.error'
)
```

**3. Dead Letter Exchange (DLX):**

```python
# Queue —Å DLX
channel.queue_declare(
    queue='main_queue',
    arguments={
        'x-dead-letter-exchange': 'dlx_exchange',
        'x-dead-letter-routing-key': 'dead_letters'
    }
)

channel.exchange_declare(exchange='dlx_exchange', exchange_type='direct')
channel.queue_declare(queue='dead_letters')
channel.queue_bind(
    exchange='dlx_exchange',
    queue='dead_letters',
    routing_key='dead_letters'
)
```

---

## –ú–æ–¥—É–ª—å 3: –ù–∞–¥–µ–∂–Ω–æ—Å—Ç—å –∏ Durability (30 –º–∏–Ω—É—Ç)

### üéØ –ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞

**–¢—Ä–∏ —É—Ä–æ–≤–Ω—è –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏:**

1. **Message Persistence** - —Å–æ–æ–±—â–µ–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –Ω–∞ –¥–∏—Å–∫
2. **Queue Durability** - queue –ø–µ—Ä–µ–∂–∏–≤–∞–µ—Ç —Ä–µ—Å—Ç–∞—Ä—Ç
3. **Publisher Confirms** - –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –¥–æ—Å—Ç–∞–≤–∫–∏

**Durable Queue:**

```python
channel.queue_declare(
    queue='durable_queue',
    durable=True  # –ø–µ—Ä–µ–∂–∏–≤–µ—Ç —Ä–µ—Å—Ç–∞—Ä—Ç RabbitMQ
)
```

**Persistent Messages:**

```python
channel.basic_publish(
    exchange='',
    routing_key='durable_queue',
    body=message,
    properties=pika.BasicProperties(
        delivery_mode=2,  # persistent
    )
)
```

**Manual Acknowledgments:**

```python
def callback(ch, method, properties, body):
    print(f"Processing {body}")
    # ... –æ–±—Ä–∞–±–æ—Ç–∫–∞ ...
    
    # Success
    ch.basic_ack(delivery_tag=method.delivery_tag)
    
    # Reject and requeue
    # ch.basic_nack(delivery_tag=method.delivery_tag, requeue=True)
    
    # Reject and don't requeue
    # ch.basic_reject(delivery_tag=method.delivery_tag, requeue=False)

channel.basic_consume(
    queue='tasks',
    on_message_callback=callback,
    auto_ack=False  # manual ack
)
```

**Prefetch (QoS):**

```python
# Consumer –ø–æ–ª—É—á–∏—Ç –º–∞–∫—Å–∏–º—É–º N unacked —Å–æ–æ–±—â–µ–Ω–∏–π
channel.basic_qos(prefetch_count=1)

# –ò–ª–∏ –≥–ª–æ–±–∞–ª—å–Ω–æ –Ω–∞ channel
channel.basic_qos(prefetch_count=10, global_qos=True)
```

**Publisher Confirms:**

```python
# –í–∫–ª—é—á–∞–µ–º confirms
channel.confirm_delivery()

try:
    channel.basic_publish(
        exchange='',
        routing_key='queue',
        body=message,
        mandatory=True  # –≤–µ—Ä–Ω–µ—Ç –æ—à–∏–±–∫—É –µ—Å–ª–∏ –Ω–µ—Ç route
    )
    print("Message confirmed")
except pika.exceptions.UnroutableError:
    print("Message was returned")
except pika.exceptions.NackError:
    print("Message was nacked")
```

**Transactions (–º–µ–¥–ª–µ–Ω–Ω–µ–µ, —á–µ–º confirms):**

```python
channel.tx_select()  # –Ω–∞—á–∞–ª–æ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏

try:
    channel.basic_publish(...)
    channel.basic_publish(...)
    channel.tx_commit()  # –∫–æ–º–º–∏—Ç
except Exception:
    channel.tx_rollback()  # –æ—Ç–∫–∞—Ç
```

**Message TTL:**

```python
# Per-message TTL
channel.basic_publish(
    exchange='',
    routing_key='queue',
    body=message,
    properties=pika.BasicProperties(
        expiration='60000'  # 60 —Å–µ–∫—É–Ω–¥ –≤ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥–∞—Ö
    )
)

# Per-queue TTL
channel.queue_declare(
    queue='ttl_queue',
    arguments={
        'x-message-ttl': 60000  # –≤—Å–µ —Å–æ–æ–±—â–µ–Ω–∏—è
    }
)
```

**Queue TTL:**

```python
# Queue —É–¥–∞–ª–∏—Ç—Å—è –µ—Å–ª–∏ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è
channel.queue_declare(
    queue='temp_queue',
    arguments={
        'x-expires': 300000  # 5 –º–∏–Ω—É—Ç
    }
)
```

**Queue Length Limit:**

```python
channel.queue_declare(
    queue='limited_queue',
    arguments={
        'x-max-length': 1000,  # –º–∞–∫—Å–∏–º—É–º —Å–æ–æ–±—â–µ–Ω–∏–π
        'x-overflow': 'reject-publish'  # –∏–ª–∏ 'drop-head'
    }
)
```

**Priority Queue:**

```python
channel.queue_declare(
    queue='priority_queue',
    arguments={
        'x-max-priority': 10  # 0-10
    }
)

channel.basic_publish(
    exchange='',
    routing_key='priority_queue',
    body=message,
    properties=pika.BasicProperties(
        priority=5
    )
)
```

### üíª –ó–∞–¥–∞–Ω–∏–µ

**–°–æ–∑–¥–∞–π –Ω–∞–¥–µ–∂–Ω—É—é —Å–∏—Å—Ç–µ–º—É –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–¥–∞—á:**

**1. Producer —Å –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è–º–∏:**

```python
# reliable_producer.py
import pika
import sys
import time

def main():
    connection = pika.BlockingConnection(
        pika.ConnectionParameters(
            'localhost',
            heartbeat=600,
            blocked_connection_timeout=300
        )
    )
    channel = connection.channel()
    
    # Durable queue
    channel.queue_declare(queue='task_queue', durable=True)
    
    # Enable confirms
    channel.confirm_delivery()
    
    for i in range(10):
        message = f'Task {i}'
        
        try:
            channel.basic_publish(
                exchange='',
                routing_key='task_queue',
                body=message,
                properties=pika.BasicProperties(
                    delivery_mode=2,  # persistent
                    priority=i % 10,  # priority 0-9
                ),
                mandatory=True
            )
            print(f" [‚úì] Sent and confirmed: {message}")
        except pika.exceptions.UnroutableError:
            print(f" [‚úó] Message was returned: {message}")
        except pika.exceptions.NackError:
            print(f" [‚úó] Message was nacked: {message}")
        
        time.sleep(0.5)
    
    connection.close()

if __name__ == '__main__':
    main()
```

**2. Consumer —Å manual ack –∏ retry logic:**

```python
# reliable_consumer.py
import pika
import time
import random

MAX_RETRIES = 3

def callback(ch, method, properties, body):
    message = body.decode()
    print(f" [x] Received {message}")
    
    # –ü–æ–ª—É—á–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –∏–∑ headers
    headers = properties.headers or {}
    retry_count = headers.get('x-retry-count', 0)
    
    # –°–∏–º—É–ª–∏—Ä—É–µ–º —Å–ª—É—á–∞–π–Ω—É—é –æ—à–∏–±–∫—É
    if random.random() < 0.3:  # 30% chance of failure
        print(f" [!] Failed to process {message}")
        
        if retry_count < MAX_RETRIES:
            # Requeue with retry count
            ch.basic_reject(delivery_tag=method.delivery_tag, requeue=True)
            
            # –ò–ª–∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ retry queue —Å –∑–∞–¥–µ—Ä–∂–∫–æ–π
            ch.basic_publish(
                exchange='',
                routing_key='task_queue',
                body=body,
                properties=pika.BasicProperties(
                    headers={'x-retry-count': retry_count + 1},
                    delivery_mode=2
                )
            )
            ch.basic_ack(delivery_tag=method.delivery_tag)
            print(f" [‚Üª] Requeued with retry count: {retry_count + 1}")
        else:
            # Max retries exceeded, send to DLQ
            print(f" [‚úó] Max retries exceeded, sending to DLQ")
            ch.basic_reject(delivery_tag=method.delivery_tag, requeue=False)
    else:
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —É—Å–ø–µ—à–Ω–∞
        time.sleep(message.count('.'))
        print(f" [‚úì] Done processing {message}")
        ch.basic_ack(delivery_tag=method.delivery_tag)

def main():
    connection = pika.BlockingConnection(
        pika.ConnectionParameters('localhost')
    )
    channel = connection.channel()
    
    channel.queue_declare(queue='task_queue', durable=True)
    
    # QoS: –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–æ –æ–¥–Ω–æ–º—É —Å–æ–æ–±—â–µ–Ω–∏—é
    channel.basic_qos(prefetch_count=1)
    
    channel.basic_consume(
        queue='task_queue',
        on_message_callback=callback,
        auto_ack=False
    )
    
    print(' [*] Waiting for messages. To exit press CTRL+C')
    channel.start_consuming()

if __name__ == '__main__':
    try:
        main()
    except KeyboardInterrupt:
        print('Interrupted')
```

**3. –ù–∞—Å—Ç—Ä–æ–π Dead Letter Queue:**

```python
# setup_dlq.py
import pika

connection = pika.BlockingConnection(
    pika.ConnectionParameters('localhost')
)
channel = connection.channel()

# DLX Exchange
channel.exchange_declare(
    exchange='dlx',
    exchange_type='fanout',
    durable=True
)

# Dead Letter Queue
channel.queue_declare(
    queue='dead_letters',
    durable=True
)

channel.queue_bind(
    exchange='dlx',
    queue='dead_letters'
)

# Main queue —Å DLX
channel.queue_declare(
    queue='task_queue_with_dlx',
    durable=True,
    arguments={
        'x-dead-letter-exchange': 'dlx',
        'x-message-ttl': 60000,  # 60 —Å–µ–∫—É–Ω–¥
        'x-max-length': 100,
    }
)

print("DLQ setup completed")
connection.close()
```

**4. –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–π –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å:**

```bash
# –ó–∞–ø—É—Å—Ç–∏ consumer
python reliable_consumer.py

# –í –¥—Ä—É–≥–æ–º —Ç–µ—Ä–º–∏–Ω–∞–ª–µ –æ—Ç–ø—Ä–∞–≤—å –∑–∞–¥–∞—á–∏
python reliable_producer.py

# –û—Å—Ç–∞–Ω–æ–≤–∏ consumer (Ctrl+C) –≤–æ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
# –ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏ - —Å–æ–æ–±—â–µ–Ω–∏—è –Ω–µ –ø–æ—Ç–µ—Ä—è—é—Ç—Å—è

# –ü—Ä–æ–≤–µ—Ä—å –≤ Web UI:
# - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ unacked —Å–æ–æ–±—â–µ–Ω–∏–π
# - Ready —Å–æ–æ–±—â–µ–Ω–∏—è
# - Dead letters queue
```

### üöÄ –ë–æ–Ω—É—Å (–Ω–æ–≤–æ–µ)

**1. Quorum Queues (–¥–ª—è –≤—ã—Å–æ–∫–æ–π –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏):**

```python
channel.queue_declare(
    queue='quorum_queue',
    durable=True,
    arguments={
        'x-queue-type': 'quorum',  # –≤–º–µ—Å—Ç–æ classic
        'x-max-in-memory-length': 0,  # –≤—Å–µ –Ω–∞ –¥–∏—Å–∫
    }
)
```

**2. Stream Queues (–¥–ª—è –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–æ–≤, RabbitMQ 3.9+):**

```python
channel.queue_declare(
    queue='stream_queue',
    durable=True,
    arguments={
        'x-queue-type': 'stream',
        'x-max-age': '7D',  # retention period
    }
)
```

**3. Delayed Message Plugin:**

```bash
# –í–∫–ª—é—á–∏ plugin
docker exec rabbitmq rabbitmq-plugins enable rabbitmq_delayed_message_exchange
```

```python
channel.exchange_declare(
    exchange='delayed',
    exchange_type='x-delayed-message',
    arguments={'x-delayed-type': 'direct'}
)

channel.basic_publish(
    exchange='delayed',
    routing_key='queue',
    body=message,
    properties=pika.BasicProperties(
        headers={'x-delay': 5000}  # 5 —Å–µ–∫—É–Ω–¥ –∑–∞–¥–µ—Ä–∂–∫–∏
    )
)
```

**4. Shovel –¥–ª—è –º–∏–≥—Ä–∞—Ü–∏–∏ —Å–æ–æ–±—â–µ–Ω–∏–π:**

```bash
docker exec rabbitmq rabbitmq-plugins enable rabbitmq_shovel
docker exec rabbitmq rabbitmq-plugins enable rabbitmq_shovel_management
```

---

## –ú–æ–¥—É–ª—å 4: Clustering –∏ High Availability (35 –º–∏–Ω—É—Ç)

### üéØ –ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞

**–¢–∏–ø—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏:**

1. **Classic Mirroring (deprecated)** - —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–µ –∫–æ–ø–∏–∏ queue
2. **Quorum Queues** - Raft consensus, —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è
3. **Federation** - –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è —Ä–µ–ø–ª–∏–∫–∞—Ü–∏—è –º–µ–∂–¥—É –∫–ª–∞—Å—Ç–µ—Ä–∞–º–∏
4. **Shovel** - –ø–µ—Ä–µ–º–µ—â–µ–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏–π –º–µ–∂–¥—É –±—Ä–æ–∫–µ—Ä–∞–º–∏

**–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –∫–ª–∞—Å—Ç–µ—Ä–∞:**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  RabbitMQ 1  ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ  RabbitMQ 2  ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ  RabbitMQ 3  ‚îÇ
‚îÇ   (node1)    ‚îÇ      ‚îÇ   (node2)    ‚îÇ      ‚îÇ   (node3)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ                     ‚îÇ                     ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    Erlang Distribution
```

**Docker Compose –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∞:**

```yaml
# docker-compose-cluster.yml
version: '3.8'

services:
  rabbitmq1:
    image: rabbitmq:3-management
    hostname: rabbit1
    environment:
      - RABBITMQ_ERLANG_COOKIE=secret_cookie
      - RABBITMQ_DEFAULT_USER=admin
      - RABBITMQ_DEFAULT_PASS=admin
    ports:
      - "5672:5672"
      - "15672:15672"
    networks:
      - rabbitmq-cluster

  rabbitmq2:
    image: rabbitmq:3-management
    hostname: rabbit2
    environment:
      - RABBITMQ_ERLANG_COOKIE=secret_cookie
      - RABBITMQ_DEFAULT_USER=admin
      - RABBITMQ_DEFAULT_PASS=admin
    ports:
      - "5673:5672"
      - "15673:15672"
    networks:
      - rabbitmq-cluster
    depends_on:
      - rabbitmq1

  rabbitmq3:
    image: rabbitmq:3-management
    hostname: rabbit3
    environment:
      - RABBITMQ_ERLANG_COOKIE=secret_cookie
      - RABBITMQ_DEFAULT_USER=admin
      - RABBITMQ_DEFAULT_PASS=admin
    ports:
      - "5674:5672"
      - "15674:15672"
    networks:
      - rabbitmq-cluster
    depends_on:
      - rabbitmq1

networks:
  rabbitmq-cluster:
    driver: bridge
```

**–°–æ–∑–¥–∞–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–∞:**

```bash
# –ó–∞–ø—É—Å—Ç–∏ –Ω–æ–¥—ã
docker-compose -f docker-compose-cluster.yml up -d

# –ü–æ–¥–æ–∂–¥–∏ –ø–æ–∫–∞ –≤—Å–µ —Å—Ç–∞—Ä—Ç–∞–Ω—É—Ç
sleep 10

# –ü—Ä–∏—Å–æ–µ–¥–∏–Ω–∏ rabbit2 –∫ rabbit1
docker exec rabbitmq2 rabbitmqctl stop_app
docker exec rabbitmq2 rabbitmqctl reset
docker exec rabbitmq2 rabbitmqctl join_cluster rabbit@rabbit1
docker exec rabbitmq2 rabbitmqctl start_app

# –ü—Ä–∏—Å–æ–µ–¥–∏–Ω–∏ rabbit3 –∫ rabbit1
docker exec rabbitmq3 rabbitmqctl stop_app
docker exec rabbitmq3 rabbitmqctl reset
docker exec rabbitmq3 rabbitmqctl join_cluster rabbit@rabbit1
docker exec rabbitmq3 rabbitmqctl start_app

# –ü—Ä–æ–≤–µ—Ä—å —Å—Ç–∞—Ç—É—Å –∫–ª–∞—Å—Ç–µ—Ä–∞
docker exec rabbitmq1 rabbitmqctl cluster_status
```

**Quorum Queue (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è HA):**

```python
channel.queue_declare(
    queue='ha_queue',
    durable=True,
    arguments={
        'x-queue-type': 'quorum',
        'x-quorum-initial-group-size': 3,  # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–ø–ª–∏–∫
    }
)
```

**Classic Queue Mirroring (deprecated, –Ω–æ –µ—â–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è):**

```bash
# Policy –¥–ª—è HA
docker exec rabbitmq1 rabbitmqctl set_policy ha-all \
  "^ha\." '{"ha-mode":"all"}' \
  --apply-to queues

# –ò–ª–∏ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ä–µ–ø–ª–∏–∫
docker exec rabbitmq1 rabbitmqctl set_policy ha-two \
  "^ha\." '{"ha-mode":"exactly","ha-params":2,"ha-sync-mode":"automatic"}' \
  --apply-to queues
```

**HAProxy –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏:**

```
# haproxy.cfg
global
    maxconn 4096

defaults
    mode tcp
    timeout connect 5s
    timeout client 30s
    timeout server 30s

listen rabbitmq
    bind *:5672
    mode tcp
    balance roundrobin
    server rabbit1 rabbit1:5672 check
    server rabbit2 rabbit2:5672 check
    server rabbit3 rabbit3:5672 check

listen rabbitmq_management
    bind *:15672
    mode http
    balance roundrobin
    server rabbit1 rabbit1:15672 check
    server rabbit2 rabbit2:15672 check
    server rabbit3 rabbit3:15672 check
```

**Federation –¥–ª—è —Å–≤—è–∑–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤:**

```bash
# –í–∫–ª—é—á–∏ plugin
docker exec rabbitmq1 rabbitmq-plugins enable rabbitmq_federation
docker exec rabbitmq1 rabbitmq-plugins enable rabbitmq_federation_management

# –ù–∞—Å—Ç—Ä–æ–π upstream
rabbitmqctl set_parameter federation-upstream my-upstream \
  '{"uri":"amqp://guest:guest@remote-rabbit:5672","trust-user-id":false}'

# –°–æ–∑–¥–∞–π policy –¥–ª—è federation
rabbitmqctl set_policy federate-me \
  "^federated\." '{"federation-upstream-set":"all"}' \
  --apply-to exchanges
```

**Network Partitions:**

```bash
# –ü—Ä–æ–≤–µ—Ä–∫–∞ partition
docker exec rabbitmq1 rabbitmqctl cluster_status

# –°—Ç—Ä–∞—Ç–µ–≥–∏–∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è (–≤ rabbitmq.conf):
# cluster_partition_handling = pause_minority (default)
# cluster_partition_handling = autoheal
# cluster_partition_handling = ignore
```

### üíª –ó–∞–¥–∞–Ω–∏–µ

**–ù–∞—Å—Ç—Ä–æ–π HA –∫–ª–∞—Å—Ç–µ—Ä:**

**1. –°–æ–∑–¥–∞–π –∫–ª–∞—Å—Ç–µ—Ä –∏–∑ 3 –Ω–æ–¥:**

```bash
# –°–æ–∑–¥–∞–π —Ñ–∞–π–ª docker-compose-cluster.yml (–∏–∑ –ø—Ä–∏–º–µ—Ä–∞ –≤—ã—à–µ)

# –ó–∞–ø—É—Å—Ç–∏
docker-compose -f docker-compose-cluster.yml up -d

# –î–æ–∂–¥–∏—Å—å –∑–∞–ø—É—Å–∫–∞
sleep 15

# –°–æ–∑–¥–∞–π –∫–ª–∞—Å—Ç–µ—Ä
docker exec rabbitmq2 rabbitmqctl stop_app
docker exec rabbitmq2 rabbitmqctl reset
docker exec rabbitmq2 rabbitmqctl join_cluster rabbit@rabbit1
docker exec rabbitmq2 rabbitmqctl start_app

docker exec rabbitmq3 rabbitmqctl stop_app
docker exec rabbitmq3 rabbitmqctl reset
docker exec rabbitmq3 rabbitmqctl join_cluster rabbit@rabbit1
docker exec rabbitmq3 rabbitmqctl start_app

# –ü—Ä–æ–≤–µ—Ä—å
docker exec rabbitmq1 rabbitmqctl cluster_status
```

**2. –°–æ–∑–¥–∞–π Quorum Queue:**

```python
# cluster_producer.py
import pika
import sys

# –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –ø–µ—Ä–≤–æ–π –Ω–æ–¥–µ
connection = pika.BlockingConnection(
    pika.ConnectionParameters(
        host='localhost',
        port=5672
    )
)
channel = connection.channel()

# Quorum queue –¥–ª—è HA
channel.queue_declare(
    queue='cluster_queue',
    durable=True,
    arguments={
        'x-queue-type': 'quorum',
    }
)

for i in range(10):
    message = f'Message {i}'
    channel.basic_publish(
        exchange='',
        routing_key='cluster_queue',
        body=message,
        properties=pika.BasicProperties(
            delivery_mode=2,
        )
    )
    print(f" [x] Sent '{message}'")

connection.close()
```

**3. –°–æ–∑–¥–∞–π consumer —Å –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ–º –∫ —Ä–∞–∑–Ω—ã–º –Ω–æ–¥–∞–º:**

```python
# cluster_consumer.py
import pika
import time
import random

NODES = [
    ('localhost', 5672),
    ('localhost', 5673),
    ('localhost', 5674),
]

def connect():
    """–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ —Å–ª—É—á–∞–π–Ω–æ–π –Ω–æ–¥–µ"""
    host, port = random.choice(NODES)
    try:
        connection = pika.BlockingConnection(
            pika.ConnectionParameters(
                host=host,
                port=port,
                heartbeat=600,
                connection_attempts=3,
                retry_delay=2
            )
        )
        print(f" [‚úì] Connected to {host}:{port}")
        return connection
    except Exception as e:
        print(f" [‚úó] Failed to connect to {host}:{port}: {e}")
        return None

def callback(ch, method, properties, body):
    print(f" [x] Received {body.decode()}")
    time.sleep(1)
    ch.basic_ack(delivery_tag=method.delivery_tag)

def main():
    while True:
        connection = connect()
        if not connection:
            time.sleep(5)
            continue
        
        try:
            channel = connection.channel()
            channel.queue_declare(
                queue='cluster_queue',
                durable=True,
                arguments={'x-queue-type': 'quorum'}
            )
            channel.basic_qos(prefetch_count=1)
            channel.basic_consume(
                queue='cluster_queue',
                on_message_callback=callback,
                auto_ack=False
            )
            
            print(' [*] Waiting for messages')
            channel.start_consuming()
        except KeyboardInterrupt:
            break
        except Exception as e:
            print(f" [!] Error: {e}")
            time.sleep(5)
        finally:
            try:
                connection.close()
            except:
                pass

if __name__ == '__main__':
    main()
```

**4. –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–π failover:**

```bash
# –¢–µ—Ä–º–∏–Ω–∞–ª 1: –∑–∞–ø—É—Å—Ç–∏ consumer
python cluster_consumer.py

# –¢–µ—Ä–º–∏–Ω–∞–ª 2: –æ—Ç–ø—Ä–∞–≤—å —Å–æ–æ–±—â–µ–Ω–∏—è
python cluster_producer.py

# –¢–µ—Ä–º–∏–Ω–∞–ª 3: –æ—Å—Ç–∞–Ω–æ–≤–∏ –æ–¥–Ω—É –Ω–æ–¥—É
docker stop rabbitmq2

# –ü—Ä–æ–≤–µ—Ä—å —á—Ç–æ consumer –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å
# –û—Ç–ø—Ä–∞–≤—å –µ—â–µ —Å–æ–æ–±—â–µ–Ω–∏–π
python cluster_producer.py

# –í–µ—Ä–Ω–∏ –Ω–æ–¥—É –æ–±—Ä–∞—Ç–Ω–æ
docker start rabbitmq2

# –û—Å—Ç–∞–Ω–æ–≤–∏ –¥—Ä—É–≥—É—é –Ω–æ–¥—É
docker stop rabbitmq1

# Consumer –¥–æ–ª–∂–µ–Ω –ø–µ—Ä–µ–ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
```

**5. –ü—Ä–æ–≤–µ—Ä—å —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–∞:**

```bash
# –°—Ç–∞—Ç—É—Å –∫–ª–∞—Å—Ç–µ—Ä–∞
docker exec rabbitmq1 rabbitmqctl cluster_status

# Quorum queue —Å—Ç–∞—Ç—É—Å
docker exec rabbitmq1 rabbitmqctl list_queues name type messages_ram messages_persistent state

# –ü—Ä–æ–≤–µ—Ä—å –≤ Web UI:
# http://localhost:15672/#/queues
# –ü–æ—Å–º–æ—Ç—Ä–∏ –Ω–∞ "Node" –∏ "Mirrors" –∫–æ–ª–æ–Ω–∫–∏
```

### üöÄ –ë–æ–Ω—É—Å (–Ω–æ–≤–æ–µ)

**1. –ù–∞—Å—Ç—Ä–æ–π –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π sync –ø–æ–ª–∏—Ç–∏–∫—É:**

```bash
# Policy –¥–ª—è quorum queues
docker exec rabbitmq1 rabbitmqctl set_policy quorum-policy \
  "^quorum\." \
  '{"queue-type":"quorum","delivery-limit":3}' \
  --apply-to queues
```

**2. –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∫–ª–∞—Å—Ç–µ—Ä–∞ —Å Prometheus:**

```yaml
# prometheus.yml
scrape_configs:
  - job_name: 'rabbitmq'
    static_configs:
      - targets:
        - 'rabbit1:15692'
        - 'rabbit2:15692'
        - 'rabbit3:15692'
```

**3. Blue-Green deployment –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∞:**

```bash
# Drain –æ–¥–Ω—É –Ω–æ–¥—É (–ø–µ—Ä–µ–Ω–µ—Å—Ç–∏ –≤—Å–µ queues)
docker exec rabbitmq1 rabbitmqctl suspend_listeners
docker exec rabbitmq1 rabbitmqctl await_startup

# –û–±–Ω–æ–≤–∏ –Ω–æ–¥—É
docker stop rabbitmq1
docker rm rabbitmq1
# ... update and restart ...

# –í–µ—Ä–Ω–∏ –≤ –∫–ª–∞—Å—Ç–µ—Ä
docker exec rabbitmq1 rabbitmqctl resume_listeners
```

---

## –ú–æ–¥—É–ª—å 5: –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ Observability (30 –º–∏–Ω—É—Ç)

### üéØ –ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞

**Management Plugin –º–µ—Ç—Ä–∏–∫–∏:**

- Overview: –û–±—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –±—Ä–æ–∫–µ—Ä–µ
- Connections: –ê–∫—Ç–∏–≤–Ω—ã–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
- Channels: –ö–∞–Ω–∞–ª—ã
- Exchanges: Exchange'—ã –∏ –∏—Ö —Ç—Ä–∞—Ñ–∏–∫
- Queues: Queue'–∏, —Å–æ–æ–±—â–µ–Ω–∏—è, consumers

**HTTP API endpoints:**

```bash
# Overview
curl -u admin:admin http://localhost:15672/api/overview

# Nodes
curl -u admin:admin http://localhost:15672/api/nodes

# Connections
curl -u admin:admin http://localhost:15672/api/connections

# Channels
curl -u admin:admin http://localhost:15672/api/channels

# Queues
curl -u admin:admin http://localhost:15672/api/queues

# Specific queue
curl -u admin:admin http://localhost:15672/api/queues/%2F/my_queue

# Consumers
curl -u admin:admin http://localhost:15672/api/consumers
```

**Prometheus –º–µ—Ç—Ä–∏–∫–∏:**

```bash
# –í–∫–ª—é—á–∏ prometheus plugin
docker exec rabbitmq rabbitmq-plugins enable rabbitmq_prometheus

# –ú–µ—Ç—Ä–∏–∫–∏ –¥–æ—Å—Ç—É–ø–Ω—ã –Ω–∞
curl http://localhost:15692/metrics
```

**–ö–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞:**

```
# Queue metrics
rabbitmq_queue_messages_ready          # –ì–æ—Ç–æ–≤—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
rabbitmq_queue_messages_unacknowledged # Unacked —Å–æ–æ–±—â–µ–Ω–∏—è
rabbitmq_queue_messages                # –í—Å–µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏–π
rabbitmq_queue_consumers               # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ consumers

# Connection metrics
rabbitmq_connections                   # –ê–∫—Ç–∏–≤–Ω—ã–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
rabbitmq_channels                      # –ê–∫—Ç–∏–≤–Ω—ã–µ –∫–∞–Ω–∞–ª—ã

# Node metrics
rabbitmq_node_mem_used                 # –ò—Å–ø–æ–ª—å–∑—É–µ–º–∞—è –ø–∞–º—è—Ç—å
rabbitmq_node_fd_used                  # File descriptors
rabbitmq_node_sockets_used             # –°–æ–∫–µ—Ç—ã
rabbitmq_node_disk_free                # –°–≤–æ–±–æ–¥–Ω–æ–µ –º–µ—Å—Ç–æ

# Message rates
rabbitmq_queue_messages_published_total
rabbitmq_queue_messages_delivered_total
rabbitmq_queue_messages_redelivered_total
```

**–ê–ª–µ—Ä—Ç—ã (–ø—Ä–∏–º–µ—Ä—ã):**

```yaml
# prometheus_alerts.yml
groups:
  - name: rabbitmq
    rules:
      # –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏–π –≤ queue
      - alert: RabbitMQQueueMessagesHigh
        expr: rabbitmq_queue_messages > 1000
        for: 5m
        annotations:
          summary: "Queue {{ $labels.queue }} has too many messages"
      
      # –ù–µ—Ç consumers
      - alert: RabbitMQNoConsumers
        expr: rabbitmq_queue_consumers == 0
        for: 5m
        annotations:
          summary: "Queue {{ $labels.queue }} has no consumers"
      
      # –í—ã—Å–æ–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏
      - alert: RabbitMQMemoryHigh
        expr: rabbitmq_node_mem_used / rabbitmq_node_mem_limit > 0.9
        for: 5m
        annotations:
          summary: "Node {{ $labels.node }} memory usage > 90%"
      
      # –ú–Ω–æ–≥–æ unacked —Å–æ–æ–±—â–µ–Ω–∏–π
      - alert: RabbitMQUnackedMessagesHigh
        expr: rabbitmq_queue_messages_unacknowledged > 100
        for: 10m
        annotations:
          summary: "Queue {{ $labels.queue }} has many unacked messages"
```

**–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ:**

```bash
# –£—Ä–æ–≤–Ω–∏ –ª–æ–≥–æ–≤
docker exec rabbitmq rabbitmqctl set_log_level debug
docker exec rabbitmq rabbitmqctl set_log_level info  # default
docker exec rabbitmq rabbitmqctl set_log_level warning
docker exec rabbitmq rabbitmqctl set_log_level error

# –õ–æ–≥–∏
docker logs rabbitmq
docker logs -f rabbitmq --tail 100
```

**rabbitmq.conf –¥–ª—è –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è:**

```
# rabbitmq.conf
log.file.level = info
log.console.level = info
log.file = /var/log/rabbitmq/rabbit.log
log.file.rotation.count = 5
log.file.rotation.size = 10485760
```

**Tracing (–¥–ª—è –æ—Ç–ª–∞–¥–∫–∏):**

```bash
# –í–∫–ª—é—á–∏ tracing plugin
docker exec rabbitmq rabbitmq-plugins enable rabbitmq_tracing

# –°–æ–∑–¥–∞–π trace —á–µ—Ä–µ–∑ Web UI –∏–ª–∏ API
curl -u admin:admin -X PUT http://localhost:15672/api/traces/%2F/my-trace \
  -H "content-type:application/json" \
  -d '{"format":"text","pattern":"#"}'

# –í—ã–∫–ª—é—á–∏ trace
curl -u admin:admin -X DELETE http://localhost:15672/api/traces/%2F/my-trace
```

### üíª –ó–∞–¥–∞–Ω–∏–µ

**–ù–∞—Å—Ç—Ä–æ–π –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥:**

**1. –ù–∞—Å—Ç—Ä–æ–π Prometheus + Grafana:**

```yaml
# docker-compose-monitoring.yml
version: '3.8'

services:
  rabbitmq:
    image: rabbitmq:3-management
    ports:
      - "5672:5672"
      - "15672:15672"
      - "15692:15692"
    environment:
      - RABBITMQ_DEFAULT_USER=admin
      - RABBITMQ_DEFAULT_PASS=admin
    command: >
      bash -c "
        rabbitmq-plugins enable rabbitmq_prometheus &&
        rabbitmq-server
      "

  prometheus:
    image: prom/prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - ./alerts.yml:/etc/prometheus/alerts.yml
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'

  grafana:
    image: grafana/grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana-storage:/var/lib/grafana

volumes:
  grafana-storage:
```

**prometheus.yml:**

```yaml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - 'alerts.yml'

scrape_configs:
  - job_name: 'rabbitmq'
    static_configs:
      - targets: ['rabbitmq:15692']
```

**alerts.yml:**

```yaml
groups:
  - name: rabbitmq_alerts
    rules:
      - alert: RabbitMQDown
        expr: up{job="rabbitmq"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "RabbitMQ is down"
      
      - alert: RabbitMQQueueMessagesHigh
        expr: rabbitmq_queue_messages > 1000
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Queue {{ $labels.queue }} has {{ $value }} messages"
      
      - alert: RabbitMQNoConsumers
        expr: rabbitmq_queue_consumers == 0 and rabbitmq_queue_messages > 0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Queue {{ $labels.queue }} has no consumers but has messages"
      
      - alert: RabbitMQMemoryHigh
        expr: (rabbitmq_node_mem_used / rabbitmq_node_mem_limit) > 0.9
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Memory usage > 90% on {{ $labels.node }}"
```

**2. –ó–∞–ø—É—Å—Ç–∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥:**

```bash
# –ó–∞–ø—É—Å—Ç–∏ stack
docker-compose -f docker-compose-monitoring.yml up -d

# –ü—Ä–æ–≤–µ—Ä—å
docker-compose -f docker-compose-monitoring.yml ps

# –î–æ—Å—Ç—É–ø:
# RabbitMQ Management: http://localhost:15672
# Prometheus: http://localhost:9090
# Grafana: http://localhost:3000 (admin/admin)
```

**3. –ù–∞—Å—Ç—Ä–æ–π Grafana:**

```bash
# –ó–∞–π–¥–∏ –≤ Grafana: http://localhost:3000
# Login: admin / admin

# –î–æ–±–∞–≤—å Prometheus data source:
# Configuration ‚Üí Data Sources ‚Üí Add data source ‚Üí Prometheus
# URL: http://prometheus:9090

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–π RabbitMQ dashboard:
# Create ‚Üí Import ‚Üí Dashboard ID: 10991
# (RabbitMQ-Overview –æ—Ç rabbitmq-prometheus)
```

**4. –°–æ–∑–¥–∞–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º–µ—Ç—Ä–∏–∫:**

```python
# generate_metrics.py
import pika
import time
import random

connection = pika.BlockingConnection(
    pika.ConnectionParameters('localhost')
)
channel = connection.channel()

# –°–æ–∑–¥–∞—ë–º –Ω–µ—Å–∫–æ–ª—å–∫–æ queues
queues = ['metrics_queue_1', 'metrics_queue_2', 'metrics_queue_3']

for queue in queues:
    channel.queue_declare(queue=queue, durable=True)

print("Generating metrics...")

for i in range(100):
    queue = random.choice(queues)
    message = f'Message {i}'
    
    channel.basic_publish(
        exchange='',
        routing_key=queue,
        body=message,
        properties=pika.BasicProperties(delivery_mode=2)
    )
    
    if i % 10 == 0:
        print(f"Sent {i} messages")
    
    time.sleep(0.1)

connection.close()
print("Done!")
```

**5. –ú–æ–Ω–∏—Ç–æ—Ä—å –º–µ—Ç—Ä–∏–∫–∏:**

```bash
# –ì–µ–Ω–µ—Ä–∏—Ä—É–π –Ω–∞–≥—Ä—É–∑–∫—É
python generate_metrics.py

# –ü—Ä–æ–≤–µ—Ä—å –º–µ—Ç—Ä–∏–∫–∏ –≤ Prometheus:
# http://localhost:9090/graph
# –ó–∞–ø—Ä–æ—Å—ã:
# - rabbitmq_queue_messages
# - rate(rabbitmq_queue_messages_published_total[1m])
# - rabbitmq_node_mem_used

# –°–º–æ—Ç—Ä–∏ –≤ Grafana dashboard
```

### üöÄ –ë–æ–Ω—É—Å (–Ω–æ–≤–æ–µ)

**1. Custom –º–µ—Ç—Ä–∏–∫–∏ —á–µ—Ä–µ–∑ StatsD:**

```bash
# –í–∫–ª—é—á–∏ plugin
docker exec rabbitmq rabbitmq-plugins enable rabbitmq_statsd
```

**2. ELK stack –¥–ª—è –ª–æ–≥–æ–≤:**

```yaml
# –î–æ–±–∞–≤—å –≤ docker-compose
  elasticsearch:
    image: elasticsearch:7.17.0
    environment:
      - discovery.type=single-node
    ports:
      - "9200:9200"

  logstash:
    image: logstash:7.17.0
    volumes:
      - ./logstash.conf:/usr/share/logstash/pipeline/logstash.conf
    depends_on:
      - elasticsearch

  kibana:
    image: kibana:7.17.0
    ports:
      - "5601:5601"
    depends_on:
      - elasticsearch
```

**3. Health Check endpoints:**

```python
import requests

def check_rabbitmq_health():
    try:
        # Aliveness test
        response = requests.get(
            'http://localhost:15672/api/aliveness-test/%2F',
            auth=('admin', 'admin')
        )
        
        if response.status_code == 200:
            data = response.json()
            if data.get('status') == 'ok':
                print("‚úì RabbitMQ is healthy")
                return True
        
        print("‚úó RabbitMQ health check failed")
        return False
    except Exception as e:
        print(f"‚úó Error: {e}")
        return False

if __name__ == '__main__':
    check_rabbitmq_health()
```

---

## –ú–æ–¥—É–ª—å 6: Security –∏ Best Practices (25 –º–∏–Ω—É—Ç)

### üéØ –ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞

**Authentication:**

```bash
# –°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
docker exec rabbitmq rabbitmqctl add_user myuser mypassword

# –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ —Ç–µ–≥–æ–≤ (roles)
docker exec rabbitmq rabbitmqctl set_user_tags myuser administrator
# –î–æ—Å—Ç—É–ø–Ω—ã–µ —Ç–µ–≥–∏: administrator, monitoring, policymaker, management

# –£–¥–∞–ª–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
docker exec rabbitmq rabbitmqctl delete_user myuser

# –°–º–µ–Ω–∞ –ø–∞—Ä–æ–ª—è
docker exec rabbitmq rabbitmqctl change_password myuser newpassword
```

**Permissions (vhost level):**

```bash
# –§–æ—Ä–º–∞—Ç: configure, write, read permissions (regex)
docker exec rabbitmq rabbitmqctl set_permissions -p / myuser ".*" ".*" ".*"

# –¢–æ–ª—å–∫–æ —á—Ç–µ–Ω–∏–µ
docker exec rabbitmq rabbitmqctl set_permissions -p / readonly_user "" "" ".*"

# –¢–æ–ª—å–∫–æ –∑–∞–ø–∏—Å—å
docker exec rabbitmq rabbitmqctl set_permissions -p / writeonly_user "" ".*" ""

# Specific queues
docker exec rabbitmq rabbitmqctl set_permissions -p / app_user "^app-.*" "^app-.*" "^app-.*"

# –ü—Ä–æ—Å–º–æ—Ç—Ä permissions
docker exec rabbitmq rabbitmqctl list_permissions -p /
docker exec rabbitmq rabbitmqctl list_user_permissions myuser
```

**Virtual Hosts (–∏–∑–æ–ª—è—Ü–∏—è):**

```bash
# –°–æ–∑–¥–∞–Ω–∏–µ vhost
docker exec rabbitmq rabbitmqctl add_vhost /dev
docker exec rabbitmq rabbitmqctl add_vhost /prod

# Permissions –¥–ª—è vhost
docker exec rabbitmq rabbitmqctl set_permissions -p /dev dev_user ".*" ".*" ".*"

# –£–¥–∞–ª–µ–Ω–∏–µ vhost
docker exec rabbitmq rabbitmqctl delete_vhost /dev
```

**TLS/SSL:**

**rabbitmq.conf:**

```
listeners.ssl.default = 5671

ssl_options.cacertfile = /path/to/ca_certificate.pem
ssl_options.certfile   = /path/to/server_certificate.pem
ssl_options.keyfile    = /path/to/server_key.pem
ssl_options.verify     = verify_peer
ssl_options.fail_if_no_peer_cert = true
```

**Python client —Å TLS:**

```python
import pika
import ssl

context = ssl.create_default_context(
    cafile="/path/to/ca_certificate.pem"
)
context.load_cert_chain(
    "/path/to/client_certificate.pem",
    "/path/to/client_key.pem"
)

ssl_options = pika.SSLOptions(context, "rabbitmq.example.com")

connection = pika.BlockingConnection(
    pika.ConnectionParameters(
        host='rabbitmq.example.com',
        port=5671,
        ssl_options=ssl_options,
        credentials=pika.PlainCredentials('user', 'pass')
    )
)
```

**Resource Limits:**

**rabbitmq.conf:**

```
# Memory
vm_memory_high_watermark.relative = 0.6
vm_memory_high_watermark_paging_ratio = 0.75

# Disk
disk_free_limit.absolute = 2GB
disk_free_limit.relative = 1.0

# Connection limits
connection_max = 1000

# Channel limits per connection
channel_max = 100
```

**Policy-based limits:**

```bash
# Max length policy
docker exec rabbitmq rabbitmqctl set_policy max-length-policy \
  "^limited-.*" \
  '{"max-length":1000,"overflow":"reject-publish"}' \
  --apply-to queues

# TTL policy
docker exec rabbitmq rabbitmqctl set_policy ttl-policy \
  "^temp-.*" \
  '{"message-ttl":60000,"expires":300000}' \
  --apply-to queues

# Max priority
docker exec rabbitmq rabbitmqctl set_policy priority-policy \
  "^priority-.*" \
  '{"max-priority":10}' \
  --apply-to queues
```

**Network Segmentation:**

```bash
# Bind to specific interface
listeners.tcp.default = 192.168.1.100:5672

# Management only on localhost
management.listener.port = 15672
management.listener.ip   = 127.0.0.1
```

**Audit Logging:**

```
# rabbitmq.conf
management.enable_queue_totals = true
management.enable_connection_lifecycle_events = true
```

### üíª –ó–∞–¥–∞–Ω–∏–µ

**–ù–∞—Å—Ç—Ä–æ–π secure RabbitMQ:**

**1. –°–æ–∑–¥–∞–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π —Å —Ä–∞–∑–Ω—ã–º–∏ —Ä–æ–ª—è–º–∏:**

```bash
# Admin user
docker exec rabbitmq rabbitmqctl add_user admin_user admin_pass
docker exec rabbitmq rabbitmqctl set_user_tags admin_user administrator
docker exec rabbitmq rabbitmqctl set_permissions -p / admin_user ".*" ".*" ".*"

# Application user (producer)
docker exec rabbitmq rabbitmqctl add_user producer_user prod_pass
docker exec rabbitmq rabbitmqctl set_user_tags producer_user management
docker exec rabbitmq rabbitmqctl set_permissions -p / producer_user "" "^app\..*" ""

# Application user (consumer)
docker exec rabbitmq rabbitmqctl add_user consumer_user cons_pass
docker exec rabbitmq rabbitmqctl set_user_tags consumer_user management
docker exec rabbitmq rabbitmqctl set_permissions -p / consumer_user "" "" "^app\..*"

# Monitoring user
docker exec rabbitmq rabbitmqctl add_user monitor_user monitor_pass
docker exec rabbitmq rabbitmqctl set_user_tags monitor_user monitoring
docker exec rabbitmq rabbitmqctl set_permissions -p / monitor_user "" "" ".*"

# –ü—Ä–æ–≤–µ—Ä—å
docker exec rabbitmq rabbitmqctl list_users
docker exec rabbitmq rabbitmqctl list_permissions -p /
```

**2. –°–æ–∑–¥–∞–π –∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ vhosts:**

```bash
# Development vhost
docker exec rabbitmq rabbitmqctl add_vhost /dev
docker exec rabbitmq rabbitmqctl add_user dev_user dev_pass
docker exec rabbitmq rabbitmqctl set_permissions -p /dev dev_user ".*" ".*" ".*"

# Production vhost
docker exec rabbitmq rabbitmqctl add_vhost /prod
docker exec rabbitmq rabbitmqctl add_user prod_user prod_pass
docker exec rabbitmq rabbitmqctl set_permissions -p /prod prod_user ".*" ".*" ".*"

# –ü—Ä–æ–≤–µ—Ä—å
docker exec rabbitmq rabbitmqctl list_vhosts
```

**3. –ù–∞—Å—Ç—Ä–æ–π producer —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ –ø—Ä–∞–≤–∞–º–∏:**

```python
# secure_producer.py
import pika

credentials = pika.PlainCredentials('producer_user', 'prod_pass')

connection = pika.BlockingConnection(
    pika.ConnectionParameters(
        host='localhost',
        credentials=credentials,
        virtual_host='/'
    )
)
channel = connection.channel()

# –ú–æ–∂–µ—Ç –ø—É–±–ª–∏–∫–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ –≤ app.* queues
channel.queue_declare(queue='app.orders', durable=True)

try:
    channel.basic_publish(
        exchange='',
        routing_key='app.orders',
        body='Order #123'
    )
    print(" [‚úì] Message published successfully")
except Exception as e:
    print(f" [‚úó] Failed to publish: {e}")

# –ü–æ–ø—ã—Ç–∫–∞ —Å–æ–∑–¥–∞—Ç—å –¥—Ä—É–≥—É—é queue (–¥–æ–ª–∂–Ω–∞ fail)
try:
    channel.queue_declare(queue='other.queue')
    print(" [‚úó] Should not be able to create other.queue!")
except Exception as e:
    print(f" [‚úì] Correctly denied: {e}")

connection.close()
```

**4. –ù–∞—Å—Ç—Ä–æ–π consumer —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º–∏ –ø—Ä–∞–≤–∞–º–∏:**

```python
# secure_consumer.py
import pika

credentials = pika.PlainCredentials('consumer_user', 'cons_pass')

connection = pika.BlockingConnection(
    pika.ConnectionParameters(
        host='localhost',
        credentials=credentials,
        virtual_host='/'
    )
)
channel = connection.channel()

def callback(ch, method, properties, body):
    print(f" [x] Received {body.decode()}")
    ch.basic_ack(delivery_tag=method.delivery_tag)

try:
    channel.basic_consume(
        queue='app.orders',
        on_message_callback=callback,
        auto_ack=False
    )
    print(" [*] Waiting for messages")
    channel.start_consuming()
except KeyboardInterrupt:
    channel.stop_consuming()
except Exception as e:
    print(f" [‚úó] Error: {e}")

connection.close()
```

**5. –ù–∞—Å—Ç—Ä–æ–π resource limits:**

```bash
# –°–æ–∑–¥–∞–π policy –¥–ª—è limits
docker exec rabbitmq rabbitmqctl set_policy limits-policy \
  "^app\..*" \
  '{"max-length":10000,"max-length-bytes":104857600,"overflow":"reject-publish","message-ttl":3600000}' \
  --apply-to queues

# –ü—Ä–æ–≤–µ—Ä—å policies
docker exec rabbitmq rabbitmqctl list_policies
```

**6. –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–π:**

```bash
# –ó–∞–ø—É—Å—Ç–∏ consumer
python secure_consumer.py

# –í –¥—Ä—É–≥–æ–º —Ç–µ—Ä–º–∏–Ω–∞–ª–µ –∑–∞–ø—É—Å—Ç–∏ producer
python secure_producer.py

# –ü–æ–ø—Ä–æ–±—É–π –∑–∞–π—Ç–∏ –≤ Web UI —Å —Ä–∞–∑–Ω—ã–º–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º–∏
# admin_user - –ø–æ–ª–Ω—ã–π –¥–æ—Å—Ç—É–ø
# producer_user/consumer_user - –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–π –¥–æ—Å—Ç—É–ø
# monitor_user - —Ç–æ–ª—å–∫–æ –ø—Ä–æ—Å–º–æ—Ç—Ä
```

### üöÄ –ë–æ–Ω—É—Å (–Ω–æ–≤–æ–µ)

**1. –ù–∞—Å—Ç—Ä–æ–π LDAP authentication:**

```
# rabbitmq.conf
auth_backends.1 = ldap
auth_backends.2 = internal

auth_ldap.servers.1  = ldap.example.com
auth_ldap.port       = 389
auth_ldap.user_dn_pattern = cn=${username},ou=users,dc=example,dc=com
```

**2. –í–∫–ª—é—á–∏ audit logging:**

```bash
docker exec rabbitmq rabbitmq-plugins enable rabbitmq_event_exchange
```

**3. Rate limiting –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π:**

```bash
# –£—Å—Ç–∞–Ω–æ–≤–∏ max connections per user
docker exec rabbitmq rabbitmqctl set_user_limits producer_user '{"max-connections": 10}'
docker exec rabbitmq rabbitmqctl set_user_limits consumer_user '{"max-channels": 50}'
```

---

## –ú–æ–¥—É–ª—å 7: Production Deployment –∏ Troubleshooting (35 –º–∏–Ω—É—Ç)

### üéØ –ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞

**Production Checklist:**

```
‚úÖ Clustering (3+ nodes)
‚úÖ Quorum queues –¥–ª—è HA
‚úÖ Monitoring (Prometheus + Grafana)
‚úÖ Alerting –Ω–∞—Å—Ç—Ä–æ–µ–Ω
‚úÖ Backups (definitions + messages)
‚úÖ Resource limits —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã
‚úÖ RBAC –Ω–∞—Å—Ç—Ä–æ–µ–Ω
‚úÖ TLS –≤–∫–ª—é—á–µ–Ω
‚úÖ Load balancer –Ω–∞—Å—Ç—Ä–æ–µ–Ω
‚úÖ Disaster recovery –ø–ª–∞–Ω
‚úÖ –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è
```

**Kubernetes Deployment:**

```yaml
# rabbitmq-statefulset.yaml
apiVersion: v1
kind: Service
metadata:
  name: rabbitmq
  labels:
    app: rabbitmq
spec:
  clusterIP: None
  ports:
    - name: amqp
      port: 5672
      targetPort: 5672
    - name: management
      port: 15672
      targetPort: 15672
  selector:
    app: rabbitmq
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: rabbitmq
spec:
  serviceName: rabbitmq
  replicas: 3
  selector:
    matchLabels:
      app: rabbitmq
  template:
    metadata:
      labels:
        app: rabbitmq
    spec:
      containers:
      - name: rabbitmq
        image: rabbitmq:3-management
        env:
        - name: RABBITMQ_ERLANG_COOKIE
          valueFrom:
            secretKeyRef:
              name: rabbitmq-secret
              key: erlang-cookie
        - name: RABBITMQ_DEFAULT_USER
          valueFrom:
            secretKeyRef:
              name: rabbitmq-secret
              key: username
        - name: RABBITMQ_DEFAULT_PASS
          valueFrom:
            secretKeyRef:
              name: rabbitmq-secret
              key: password
        ports:
        - containerPort: 5672
          name: amqp
        - containerPort: 15672
          name: management
        volumeMounts:
        - name: rabbitmq-data
          mountPath: /var/lib/rabbitmq
        resources:
          requests:
            cpu: 500m
            memory: 1Gi
          limits:
            cpu: 2000m
            memory: 2Gi
        livenessProbe:
          exec:
            command: ["rabbitmq-diagnostics", "status"]
          initialDelaySeconds: 60
          periodSeconds: 60
          timeoutSeconds: 15
        readinessProbe:
          exec:
            command: ["rabbitmq-diagnostics", "ping"]
          initialDelaySeconds: 20
          periodSeconds: 60
          timeoutSeconds: 10
  volumeClaimTemplates:
  - metadata:
      name: rabbitmq-data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 10Gi
```

**Backup –∏ Restore:**

```bash
# Export definitions (exchanges, queues, bindings, users, etc)
docker exec rabbitmq rabbitmqctl export_definitions /tmp/definitions.json
docker cp rabbitmq:/tmp/definitions.json ./backup/definitions.json

# Import definitions
docker cp ./backup/definitions.json rabbitmq:/tmp/definitions.json
docker exec rabbitmq rabbitmqctl import_definitions /tmp/definitions.json

# –ß–µ—Ä–µ–∑ HTTP API
curl -u admin:admin http://localhost:15672/api/definitions -o definitions.json
curl -u admin:admin -X POST -H "Content-Type: application/json" \
  -d @definitions.json http://localhost:15672/api/definitions

# Backup messages (—Ç—Ä–µ–±—É–µ—Ç shovel plugin)
docker exec rabbitmq rabbitmq-plugins enable rabbitmq_shovel
docker exec rabbitmq rabbitmq-plugins enable rabbitmq_shovel_management
```

**Common Issues –∏ Solutions:**

**1. Memory Alarm:**

```bash
# –ü—Ä–æ–≤–µ—Ä–∫–∞
docker exec rabbitmq rabbitmqctl status | grep memory

# –†–µ—à–µ–Ω–∏–µ
docker exec rabbitmq rabbitmqctl set_vm_memory_high_watermark 0.7

# –ò–ª–∏ –≤ rabbitmq.conf
vm_memory_high_watermark.relative = 0.7
```

**2. Disk Space Alarm:**

```bash
# –ü—Ä–æ–≤–µ—Ä–∫–∞
docker exec rabbitmq rabbitmqctl status | grep disk

# –†–µ—à–µ–Ω–∏–µ - –æ—Å–≤–æ–±–æ–¥–∏ –º–µ—Å—Ç–æ –∏–ª–∏ —É–≤–µ–ª–∏—á—å –ª–∏–º–∏—Ç
docker exec rabbitmq rabbitmqctl set_disk_free_limit 2GB

# –ò–ª–∏ –≤ rabbitmq.conf
disk_free_limit.absolute = 2GB
```

**3. Connection Leak:**

```bash
# –ù–∞–π–¥–∏ –∑–∞–≤–∏—Å—à–∏–µ connections
docker exec rabbitmq rabbitmqctl list_connections name state timeout | grep blocked

# –ó–∞–∫—Ä–æ–π connection
docker exec rabbitmq rabbitmqctl close_connection "<connection_name>" "Cleanup"

# –ù–∞–π–¥–∏ –≤—Å–µ connections –æ—Ç IP
docker exec rabbitmq rabbitmqctl list_connections peer_host | grep 192.168.1.100
```

**4. High Unacked Messages:**

```bash
# –ü—Ä–æ–≤–µ—Ä–∫–∞
docker exec rabbitmq rabbitmqctl list_queues name messages_unacknowledged

# –ü—Ä–∏—á–∏–Ω—ã:
# - Consumer –º–µ–¥–ª–µ–Ω–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç
# - Consumer —É–º–µ—Ä –±–µ–∑ ack
# - Prefetch —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π

# –†–µ—à–µ–Ω–∏–µ - —É–º–µ–Ω—å—à–∏ prefetch
channel.basic_qos(prefetch_count=1)
```

**5. Queue Blocked:**

```bash
# –ü—Ä–æ–≤–µ—Ä–∫–∞
docker exec rabbitmq rabbitmqctl list_queues name state

# –ü—Ä–∏—á–∏–Ω—ã: memory alarm, disk alarm, flow control

# –ü—Ä–æ–≤–µ—Ä—å –∞–ª–∞—Ä–º—ã
docker exec rabbitmq rabbitmqctl alarm_list
```

**Troubleshooting Commands:**

```bash
# –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞
docker exec rabbitmq rabbitmq-diagnostics status
docker exec rabbitmq rabbitmq-diagnostics check_running
docker exec rabbitmq rabbitmq-diagnostics check_local_alarms
docker exec rabbitmq rabbitmq-diagnostics check_port_connectivity
docker exec rabbitmq rabbitmq-diagnostics memory_breakdown

# Network
docker exec rabbitmq rabbitmq-diagnostics check_port_listener 5672
docker exec rabbitmq rabbitmq-diagnostics check_protocol_listener amqp

# Cluster
docker exec rabbitmq rabbitmq-diagnostics cluster_status
docker exec rabbitmq rabbitmq-diagnostics check_if_node_is_quorum_critical

# Consumers
docker exec rabbitmq rabbitmqctl list_consumers queue_name channel_pid

# Report
docker exec rabbitmq rabbitmq-diagnostics status > report.txt
docker exec rabbitmq rabbitmq-diagnostics environment >> report.txt
```

**Performance Tuning:**

```
# rabbitmq.conf

# Disk I/O
queue_index_embed_msgs_below = 4096

# Network
tcp_listen_options.backlog = 128
tcp_listen_options.nodelay = true
tcp_listen_options.sndbuf = 196608
tcp_listen_options.recbuf = 196608

# Channel
channel_max = 256

# Heartbeat
heartbeat = 60

# Collect statistics (disable in production for performance)
collect_statistics = coarse
collect_statistics_interval = 5000
```

### üíª –ó–∞–¥–∞–Ω–∏–µ

**–°–æ–∑–¥–∞–π production-ready deployment:**

**1. –°–æ–∑–¥–∞–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è backup:**

```bash
#!/bin/bash
# backup.sh

BACKUP_DIR="./backups/$(date +%Y%m%d_%H%M%S)"
mkdir -p "$BACKUP_DIR"

echo "Starting RabbitMQ backup..."

# Backup definitions
docker exec rabbitmq rabbitmqctl export_definitions /tmp/definitions.json
docker cp rabbitmq:/tmp/definitions.json "$BACKUP_DIR/definitions.json"
echo "‚úì Definitions backed up"

# Backup queue stats
docker exec rabbitmq rabbitmqctl list_queues name messages messages_ready messages_unacknowledged > "$BACKUP_DIR/queue_stats.txt"
echo "‚úì Queue stats backed up"

# Backup connections
docker exec rabbitmq rabbitmqctl list_connections > "$BACKUP_DIR/connections.txt"
echo "‚úì Connections backed up"

# Backup users
docker exec rabbitmq rabbitmqctl list_users > "$BACKUP_DIR/users.txt"
echo "‚úì Users backed up"

# Compress
tar -czf "$BACKUP_DIR.tar.gz" -C ./backups "$(basename $BACKUP_DIR)"
rm -rf "$BACKUP_DIR"

echo "‚úì Backup completed: $BACKUP_DIR.tar.gz"
```

**2. –°–æ–∑–¥–∞–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è restore:**

```bash
#!/bin/bash
# restore.sh

if [ -z "$1" ]; then
    echo "Usage: $0 <backup_file.tar.gz>"
    exit 1
fi

BACKUP_FILE="$1"
TEMP_DIR="./temp_restore"

echo "Starting RabbitMQ restore from $BACKUP_FILE..."

# Extract
mkdir -p "$TEMP_DIR"
tar -xzf "$BACKUP_FILE" -C "$TEMP_DIR"

# Find definitions file
DEFINITIONS_FILE=$(find "$TEMP_DIR" -name "definitions.json")

if [ -z "$DEFINITIONS_FILE" ]; then
    echo "‚úó definitions.json not found in backup"
    exit 1
fi

# Restore definitions
docker cp "$DEFINITIONS_FILE" rabbitmq:/tmp/definitions.json
docker exec rabbitmq rabbitmqctl import_definitions /tmp/definitions.json

echo "‚úì Definitions restored"

# Cleanup
rm -rf "$TEMP_DIR"

echo "‚úì Restore completed"
```

**3. –°–æ–∑–¥–∞–π health check —Å–∫—Ä–∏–ø—Ç:**

```python
# health_check.py
import requests
import sys

def check_aliveness():
    try:
        response = requests.get(
            'http://localhost:15672/api/aliveness-test/%2F',
            auth=('admin', 'admin'),
            timeout=5
        )
        return response.status_code == 200 and response.json().get('status') == 'ok'
    except Exception as e:
        print(f"Aliveness check failed: {e}")
        return False

def check_memory():
    try:
        response = requests.get(
            'http://localhost:15672/api/nodes',
            auth=('admin', 'admin'),
            timeout=5
        )
        
        if response.status_code != 200:
            return False
        
        nodes = response.json()
        for node in nodes:
            mem_used = node.get('mem_used', 0)
            mem_limit = node.get('mem_limit', 1)
            mem_percent = (mem_used / mem_limit) * 100
            
            if mem_percent > 90:
                print(f"‚ö† Memory usage high: {mem_percent:.1f}%")
                return False
        
        return True
    except Exception as e:
        print(f"Memory check failed: {e}")
        return False

def check_disk():
    try:
        response = requests.get(
            'http://localhost:15672/api/nodes',
            auth=('admin', 'admin'),
            timeout=5
        )
        
        if response.status_code != 200:
            return False
        
        nodes = response.json()
        for node in nodes:
            disk_free = node.get('disk_free', 0)
            disk_limit = node.get('disk_free_limit', 1)
            
            if disk_free < disk_limit:
                print(f"‚ö† Disk space low: {disk_free} < {disk_limit}")
                return False
        
        return True
    except Exception as e:
        print(f"Disk check failed: {e}")
        return False

def check_queues():
    try:
        response = requests.get(
            'http://localhost:15672/api/queues',
            auth=('admin', 'admin'),
            timeout=5
        )
        
        if response.status_code != 200:
            return False
        
        queues = response.json()
        issues = []
        
        for queue in queues:
            name = queue['name']
            messages = queue.get('messages', 0)
            consumers = queue.get('consumers', 0)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞: –º–Ω–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏–π, –Ω–æ –Ω–µ—Ç consumers
            if messages > 100 and consumers == 0:
                issues.append(f"Queue '{name}' has {messages} messages but no consumers")
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞: –º–Ω–æ–≥–æ unacked —Å–æ–æ–±—â–µ–Ω–∏–π
            unacked = queue.get('messages_unacknowledged', 0)
            if unacked > 1000:
                issues.append(f"Queue '{name}' has {unacked} unacked messages")
        
        if issues:
            for issue in issues:
                print(f"‚ö† {issue}")
            return False
        
        return True
    except Exception as e:
        print(f"Queue check failed: {e}")
        return False

def main():
    checks = {
        'Aliveness': check_aliveness,
        'Memory': check_memory,
        'Disk': check_disk,
        'Queues': check_queues,
    }
    
    print("Running RabbitMQ health checks...")
    print("-" * 40)
    
    all_passed = True
    for name, check_func in checks.items():
        result = check_func()
        status = "‚úì PASS" if result else "‚úó FAIL"
        print(f"{name:20s} {status}")
        
        if not result:
            all_passed = False
    
    print("-" * 40)
    
    if all_passed:
        print("‚úì All checks passed")
        sys.exit(0)
    else:
        print("‚úó Some checks failed")
        sys.exit(1)

if __name__ == '__main__':
    main()
```

**4. –°–æ–∑–¥–∞–π stress test:**

```python
# stress_test.py
import pika
import threading
import time
import argparse

class Producer(threading.Thread):
    def __init__(self, thread_id, num_messages):
        threading.Thread.__init__(self)
        self.thread_id = thread_id
        self.num_messages = num_messages
        self.sent = 0
    
    def run(self):
        connection = pika.BlockingConnection(
            pika.ConnectionParameters('localhost')
        )
        channel = connection.channel()
        channel.queue_declare(queue='stress_test', durable=True)
        
        for i in range(self.num_messages):
            message = f'Thread-{self.thread_id} Message-{i}'
            channel.basic_publish(
                exchange='',
                routing_key='stress_test',
                body=message,
                properties=pika.BasicProperties(delivery_mode=2)
            )
            self.sent += 1
        
        connection.close()
        print(f"Producer {self.thread_id} sent {self.sent} messages")

class Consumer(threading.Thread):
    def __init__(self, thread_id, duration):
        threading.Thread.__init__(self)
        self.thread_id = thread_id
        self.duration = duration
        self.received = 0
        self.running = True
    
    def callback(self, ch, method, properties, body):
        self.received += 1
        ch.basic_ack(delivery_tag=method.delivery_tag)
    
    def run(self):
        connection = pika.BlockingConnection(
            pika.ConnectionParameters('localhost')
        )
        channel = connection.channel()
        channel.queue_declare(queue='stress_test', durable=True)
        channel.basic_qos(prefetch_count=10)
        
        channel.basic_consume(
            queue='stress_test',
            on_message_callback=self.callback,
            auto_ack=False
        )
        
        start_time = time.time()
        while self.running and (time.time() - start_time) < self.duration:
            connection.process_data_events(time_limit=1)
        
        connection.close()
        print(f"Consumer {self.thread_id} received {self.received} messages")

def main():
    parser = argparse.ArgumentParser(description='RabbitMQ Stress Test')
    parser.add_argument('--producers', type=int, default=5, help='Number of producers')
    parser.add_argument('--consumers', type=int, default=5, help='Number of consumers')
    parser.add_argument('--messages', type=int, default=1000, help='Messages per producer')
    parser.add_argument('--duration', type=int, default=60, help='Consumer duration (seconds)')
    
    args = parser.parse_args()
    
    print(f"Starting stress test:")
    print(f"  Producers: {args.producers}")
    print(f"  Consumers: {args.consumers}")
    print(f"  Messages per producer: {args.messages}")
    print(f"  Consumer duration: {args.duration}s")
    print("-" * 40)
    
    # Start producers
    producers = []
    for i in range(args.producers):
        p = Producer(i, args.messages)
        p.start()
        producers.append(p)
    
    # Start consumers
    consumers = []
    for i in range(args.consumers):
        c = Consumer(i, args.duration)
        c.start()
        consumers.append(c)
    
    # Wait for completion
    for p in producers:
        p.join()
    
    print("-" * 40)
    print("All producers completed")
    
    time.sleep(args.duration)
    
    for c in consumers:
        c.running = False
    
    for c in consumers:
        c.join()
    
    print("-" * 40)
    total_sent = sum(p.sent for p in producers)
    total_received = sum(c.received for c in consumers)
    
    print(f"Total sent: {total_sent}")
    print(f"Total received: {total_received}")
    print(f"Throughput: {total_received / args.duration:.2f} msg/s")

if __name__ == '__main__':
    main()
```

**5. –ó–∞–ø—É—Å—Ç–∏ —Ç–µ—Å—Ç—ã:**

```bash
# Health check
python health_check.py

# Backup
chmod +x backup.sh
./backup.sh

# Stress test
python stress_test.py --producers 10 --consumers 10 --messages 5000

# Monitor –≤–æ –≤—Ä–µ–º—è stress test
watch -n 1 'docker exec rabbitmq rabbitmqctl list_queues name messages consumers'
```

### üöÄ –ë–æ–Ω—É—Å (–Ω–æ–≤–æ–µ)

**1. –ù–∞—Å—Ç—Ä–æ–π –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π failover —Å Consul:**

```yaml
# consul-template –¥–ª—è –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–Ω—Ñ–∏–≥–∞
{{range service "rabbitmq"}}
server {{.Address}}:{{.Port}} check
{{end}}
```

**2. Blue-Green Deployment Strategy:**

```bash
# 1. –†–∞–∑–≤–µ—Ä–Ω–∏ –Ω–æ–≤—É—é –≤–µ—Ä—Å–∏—é (green)
docker-compose -f docker-compose-green.yml up -d

# 2. Drain —Å—Ç–∞—Ä—É—é –≤–µ—Ä—Å–∏—é (blue)
# –û—Å—Ç–∞–Ω–æ–≤–∏ publishers –Ω–∞ blue
# –î–æ–∂–¥–∏—Å—å –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤—Å–µ—Ö —Å–æ–æ–±—â–µ–Ω–∏–π

# 3. –ü–µ—Ä–µ–∫–ª—é—á–∏ load balancer –Ω–∞ green

# 4. –£–¥–∞–ª–∏ blue –ø–æ—Å–ª–µ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è
```

**3. Chaos Engineering:**

```bash
# –£—Å—Ç–∞–Ω–æ–≤–∏ pumba –¥–ª—è chaos testing
docker run -d --name pumba \
  -v /var/run/docker.sock:/var/run/docker.sock \
  gaiaadm/pumba \
  --random \
  --interval 1m \
  kill --signal SIGKILL \
  re2:^rabbitmq
```

---

## –ú–æ–¥—É–ª—å 8: Advanced Patterns –∏ Use Cases ((30 –º–∏–Ω—É—Ç))

### Event-Driven Architecture 

```python
# event_bus.py (–ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ)
        self.channel.basic_publish(
            exchange='events',
            routing_key=routing_key,
            body=json.dumps(event),
            properties=pika.BasicProperties(
                delivery_mode=2,
                content_type='application/json',
                message_id=event['id']
            )
        )
        print(f" [x] Published event: {event_type}")
        return event['id']
    
    def subscribe(self, event_pattern, callback):
        """Subscribe to events matching pattern"""
        result = self.channel.queue_declare(queue='', exclusive=True)
        queue_name = result.method.queue
        
        routing_key = event_pattern.replace('.', '_').lower()
        self.channel.queue_bind(
            exchange='events',
            queue=queue_name,
            routing_key=routing_key
        )
        
        def wrapper(ch, method, properties, body):
            event = json.loads(body)
            callback(event)
            ch.basic_ack(delivery_tag=method.delivery_tag)
        
        self.channel.basic_consume(
            queue=queue_name,
            on_message_callback=wrapper,
            auto_ack=False
        )
        print(f" [*] Subscribed to: {event_pattern}")
    
    def start(self):
        print(" [*] Starting event bus...")
        self.channel.start_consuming()
    
    def close(self):
        self.connection.close()


# order_service.py
import time
from event_bus import EventBus

def handle_payment_processed(event):
    print(f" [Order] Payment processed for order: {event['data']['order_id']}")
    # Ship order
    time.sleep(1)
    event_bus.publish('order.shipped', {
        'order_id': event['data']['order_id'],
        'tracking_number': 'TRACK123'
    })

def handle_payment_failed(event):
    print(f" [Order] Payment failed, cancelling order: {event['data']['order_id']}")
    # Cancel order

event_bus = EventBus()
event_bus.subscribe('payment.processed', handle_payment_processed)
event_bus.subscribe('payment.failed', handle_payment_failed)

# Create new order
order_data = {'order_id': 'ORD-001', 'amount': 99.99}
event_bus.publish('order.created', order_data)

event_bus.start()


# payment_service.py
import random
import time
from event_bus import EventBus

def handle_order_created(event):
    order_id = event['data']['order_id']
    amount = event['data']['amount']
    
    print(f" [Payment] Processing payment for order {order_id}: ${amount}")
    time.sleep(2)
    
    # Simulate payment processing
    if random.random() > 0.2:  # 80% success rate
        event_bus.publish('payment.processed', {
            'order_id': order_id,
            'amount': amount,
            'transaction_id': 'TXN-' + str(int(time.time()))
        })
    else:
        event_bus.publish('payment.failed', {
            'order_id': order_id,
            'reason': 'Insufficient funds'
        })

event_bus = EventBus()
event_bus.subscribe('order.created', handle_order_created)
event_bus.start()
```

### 5. –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–π Event-Driven —Å–∏—Å—Ç–µ–º—É:

```bash
# –¢–µ—Ä–º–∏–Ω–∞–ª 1: Order Service
python order_service.py

# –¢–µ—Ä–º–∏–Ω–∞–ª 2: Payment Service
python payment_service.py

# –ù–∞–±–ª—é–¥–∞–π –∑–∞ flow —Å–æ–±—ã—Ç–∏–π:
# order.created ‚Üí payment.processed ‚Üí order.shipped
```

---

## üöÄ –ë–æ–Ω—É—Å (–Ω–æ–≤–æ–µ)

### 1. Message Compression:

```python
# compressed_producer.py
import pika
import zlib
import json

connection = pika.BlockingConnection(
    pika.ConnectionParameters('localhost')
)
channel = connection.channel()

channel.queue_declare(queue='compressed_queue', durable=True)

# Large message
large_data = {'users': [{'id': i, 'name': f'User{i}'} for i in range(1000)]}
message = json.dumps(large_data)

# Compress
compressed = zlib.compress(message.encode())

print(f"Original size: {len(message)} bytes")
print(f"Compressed size: {len(compressed)} bytes")
print(f"Compression ratio: {len(compressed)/len(message)*100:.1f}%")

channel.basic_publish(
    exchange='',
    routing_key='compressed_queue',
    body=compressed,
    properties=pika.BasicProperties(
        delivery_mode=2,
        content_encoding='gzip'
    )
)

connection.close()


# compressed_consumer.py
import pika
import zlib
import json

def callback(ch, method, properties, body):
    # Decompress
    if properties.content_encoding == 'gzip':
        decompressed = zlib.decompress(body).decode()
        data = json.loads(decompressed)
    else:
        data = json.loads(body.decode())
    
    print(f" [x] Received {len(data['users'])} users")
    ch.basic_ack(delivery_tag=method.delivery_tag)

connection = pika.BlockingConnection(
    pika.ConnectionParameters('localhost')
)
channel = connection.channel()

channel.queue_declare(queue='compressed_queue', durable=True)
channel.basic_consume(
    queue='compressed_queue',
    on_message_callback=callback,
    auto_ack=False
)

channel.start_consuming()
```

### 2. Circuit Breaker Pattern:

```python
# circuit_breaker.py
import pika
import time
from enum import Enum

class CircuitState(Enum):
    CLOSED = "closed"
    OPEN = "open"
    HALF_OPEN = "half_open"

class CircuitBreaker:
    def __init__(self, failure_threshold=5, timeout=60):
        self.failure_threshold = failure_threshold
        self.timeout = timeout
        self.failure_count = 0
        self.last_failure_time = None
        self.state = CircuitState.CLOSED
    
    def call(self, func, *args, **kwargs):
        if self.state == CircuitState.OPEN:
            if time.time() - self.last_failure_time > self.timeout:
                print(" [Circuit] Attempting recovery (HALF_OPEN)")
                self.state = CircuitState.HALF_OPEN
            else:
                raise Exception("Circuit breaker is OPEN")
        
        try:
            result = func(*args, **kwargs)
            self.on_success()
            return result
        except Exception as e:
            self.on_failure()
            raise e
    
    def on_success(self):
        self.failure_count = 0
        if self.state == CircuitState.HALF_OPEN:
            print(" [Circuit] Recovery successful (CLOSED)")
            self.state = CircuitState.CLOSED
    
    def on_failure(self):
        self.failure_count += 1
        self.last_failure_time = time.time()
        
        if self.failure_count >= self.failure_threshold:
            print(f" [Circuit] Too many failures (OPEN)")
            self.state = CircuitState.OPEN


# resilient_consumer.py
import pika
import time
import random

circuit_breaker = CircuitBreaker(failure_threshold=3, timeout=10)

def process_message(body):
    # Simulate occasional failures
    if random.random() < 0.3:
        raise Exception("Processing failed")
    print(f" [x] Processed: {body.decode()}")

def callback(ch, method, properties, body):
    try:
        circuit_breaker.call(process_message, body)
        ch.basic_ack(delivery_tag=method.delivery_tag)
    except Exception as e:
        print(f" [!] Error: {e}")
        # Requeue or send to DLQ
        ch.basic_nack(delivery_tag=method.delivery_tag, requeue=False)

connection = pika.BlockingConnection(
    pika.ConnectionParameters('localhost')
)
channel = connection.channel()

channel.queue_declare(queue='resilient_queue', durable=True)
channel.basic_qos(prefetch_count=1)
channel.basic_consume(
    queue='resilient_queue',
    on_message_callback=callback,
    auto_ack=False
)

print(" [*] Waiting for messages with circuit breaker")
channel.start_consuming()
```

### 3. Message Batching:

```python
# batch_producer.py
import pika
import json
import time

class BatchProducer:
    def __init__(self, batch_size=100, flush_interval=5):
        self.connection = pika.BlockingConnection(
            pika.ConnectionParameters('localhost')
        )
        self.channel = self.connection.channel()
        self.channel.queue_declare(queue='batch_queue', durable=True)
        
        self.batch_size = batch_size
        self.flush_interval = flush_interval
        self.batch = []
        self.last_flush = time.time()
    
    def add(self, message):
        self.batch.append(message)
        
        if len(self.batch) >= self.batch_size:
            self.flush()
        elif time.time() - self.last_flush > self.flush_interval:
            self.flush()
    
    def flush(self):
        if not self.batch:
            return
        
        batch_message = json.dumps({
            'count': len(self.batch),
            'messages': self.batch
        })
        
        self.channel.basic_publish(
            exchange='',
            routing_key='batch_queue',
            body=batch_message,
            properties=pika.BasicProperties(delivery_mode=2)
        )
        
        print(f" [x] Flushed batch of {len(self.batch)} messages")
        self.batch = []
        self.last_flush = time.time()
    
    def close(self):
        self.flush()
        self.connection.close()


# Usage
producer = BatchProducer(batch_size=50, flush_interval=3)

for i in range(200):
    producer.add({'id': i, 'data': f'Message {i}'})
    time.sleep(0.01)

producer.close()


# batch_consumer.py
import pika
import json

def callback(ch, method, properties, body):
    batch = json.loads(body)
    print(f" [x] Received batch of {batch['count']} messages")
    
    # Process each message
    for message in batch['messages']:
        print(f"    Processing: {message['id']}")
    
    ch.basic_ack(delivery_tag=method.delivery_tag)

connection = pika.BlockingConnection(
    pika.ConnectionParameters('localhost')
)
channel = connection.channel()

channel.queue_declare(queue='batch_queue', durable=True)
channel.basic_consume(
    queue='batch_queue',
    on_message_callback=callback,
    auto_ack=False
)

print(" [*] Waiting for batches")
channel.start_consuming()
```

---

## –ú–æ–¥—É–ª—å 9: –§–∏–Ω–∞–ª—å–Ω—ã–π –ü—Ä–æ–µ–∫—Ç (40 –º–∏–Ω—É—Ç)

### üéØ –¶–µ–ª—å
–°–æ–∑–¥–∞—Ç—å –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—É—é microservices —Å–∏—Å—Ç–µ–º—É —Å RabbitMQ

### –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Å–∏—Å—Ç–µ–º—ã

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   API        ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  RabbitMQ    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   Workers    ‚îÇ
‚îÇ  Gateway     ‚îÇ     ‚îÇ   Broker     ‚îÇ     ‚îÇ  (3 types)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚ñº               ‚ñº
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ   Dead   ‚îÇ    ‚îÇ  Audit   ‚îÇ
              ‚îÇ  Letter  ‚îÇ    ‚îÇ   Log    ‚îÇ
              ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### üíª –§–∏–Ω–∞–ª—å–Ω–æ–µ –ó–∞–¥–∞–Ω–∏–µ

#### 1. API Gateway:

```python
# api_gateway.py
from flask import Flask, request, jsonify
import pika
import json
import uuid

app = Flask(__name__)

def get_channel():
    connection = pika.BlockingConnection(
        pika.ConnectionParameters('localhost')
    )
    channel = connection.channel()
    
    # Declare exchanges
    channel.exchange_declare(
        exchange='tasks',
        exchange_type='topic',
        durable=True
    )
    
    return connection, channel

@app.route('/api/task', methods=['POST'])
def create_task():
    data = request.json
    task_type = data.get('type', 'general')
    
    connection, channel = get_channel()
    
    task_id = str(uuid.uuid4())
    message = {
        'task_id': task_id,
        'type': task_type,
        'data': data.get('data', {}),
        'priority': data.get('priority', 5)
    }
    
    routing_key = f'task.{task_type}'
    
    channel.basic_publish(
        exchange='tasks',
        routing_key=routing_key,
        body=json.dumps(message),
        properties=pika.BasicProperties(
            delivery_mode=2,
            priority=message['priority'],
            message_id=task_id,
            content_type='application/json'
        )
    )
    
    connection.close()
    
    return jsonify({
        'status': 'success',
        'task_id': task_id,
        'message': f'Task {task_type} queued'
    }), 202

@app.route('/health', methods=['GET'])
def health():
    try:
        connection, channel = get_channel()
        # Test connection
        channel.queue_declare(queue='health_check', passive=True)
        connection.close()
        return jsonify({'status': 'healthy'}), 200
    except Exception as e:
        return jsonify({'status': 'unhealthy', 'error': str(e)}), 503

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000, debug=True)
```

#### 2. Worker Base Class:

```python
# worker_base.py
import pika
import json
import time
import logging
from abc import ABC, abstractmethod

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

class WorkerBase(ABC):
    def __init__(self, worker_type, queue_name, routing_keys):
        self.worker_type = worker_type
        self.queue_name = queue_name
        self.routing_keys = routing_keys
        self.logger = logging.getLogger(worker_type)
        
        self.connection = pika.BlockingConnection(
            pika.ConnectionParameters(
                host='localhost',
                heartbeat=600,
                blocked_connection_timeout=300
            )
        )
        self.channel = self.connection.channel()
        
        self.setup_queues()
    
    def setup_queues(self):
        # Main exchange
        self.channel.exchange_declare(
            exchange='tasks',
            exchange_type='topic',
            durable=True
        )
        
        # DLX
        self.channel.exchange_declare(
            exchange='dlx',
            exchange_type='fanout',
            durable=True
        )
        
        # Dead letter queue
        self.channel.queue_declare(
            queue='dead_letters',
            durable=True
        )
        self.channel.queue_bind(
            exchange='dlx',
            queue='dead_letters'
        )
        
        # Worker queue
        self.channel.queue_declare(
            queue=self.queue_name,
            durable=True,
            arguments={
                'x-dead-letter-exchange': 'dlx',
                'x-max-priority': 10,
                'x-message-ttl': 3600000  # 1 hour
            }
        )
        
        # Bind routing keys
        for routing_key in self.routing_keys:
            self.channel.queue_bind(
                exchange='tasks',
                queue=self.queue_name,
                routing_key=routing_key
            )
        
        self.channel.basic_qos(prefetch_count=1)
    
    @abstractmethod
    def process_task(self, task):
        """Override this method to implement task processing"""
        pass
    
    def callback(self, ch, method, properties, body):
        try:
            task = json.loads(body)
            self.logger.info(f"Processing task: {task['task_id']}")
            
            start_time = time.time()
            result = self.process_task(task)
            duration = time.time() - start_time
            
            self.logger.info(
                f"Task {task['task_id']} completed in {duration:.2f}s"
            )
            
            # Acknowledge
            ch.basic_ack(delivery_tag=method.delivery_tag)
            
            # Publish result to audit log
            self.publish_audit_log(task, result, duration)
            
        except Exception as e:
            self.logger.error(f"Error processing task: {e}")
            
            # Reject and don't requeue (goes to DLX)
            ch.basic_nack(
                delivery_tag=method.delivery_tag,
                requeue=False
            )
    
    def publish_audit_log(self, task, result, duration):
        self.channel.exchange_declare(
            exchange='audit',
            exchange_type='fanout',
            durable=True
        )
        
        audit_message = {
            'task_id': task['task_id'],
            'worker_type': self.worker_type,
            'duration': duration,
            'result': result,
            'timestamp': time.time()
        }
        
        self.channel.basic_publish(
            exchange='audit',
            routing_key='',
            body=json.dumps(audit_message),
            properties=pika.BasicProperties(delivery_mode=2)
        )
    
    def start(self):
        self.logger.info(f"Starting {self.worker_type} worker...")
        self.channel.basic_consume(
            queue=self.queue_name,
            on_message_callback=self.callback,
            auto_ack=False
        )
        
        try:
            self.channel.start_consuming()
        except KeyboardInterrupt:
            self.logger.info("Stopping worker...")
            self.channel.stop_consuming()
            self.connection.close()
```

#### 3. Specific Workers:

```python
# email_worker.py
from worker_base import WorkerBase
import time

class EmailWorker(WorkerBase):
    def __init__(self):
        super().__init__(
            worker_type='email',
            queue_name='email_queue',
            routing_keys=['task.email', 'task.email.*']
        )
    
    def process_task(self, task):
        # Simulate email sending
        recipient = task['data'].get('recipient')
        subject = task['data'].get('subject')
        
        self.logger.info(f"Sending email to {recipient}: {subject}")
        time.sleep(2)  # Simulate delay
        
        return {
            'status': 'sent',
            'recipient': recipient
        }

if __name__ == '__main__':
    worker = EmailWorker()
    worker.start()


# image_worker.py
from worker_base import WorkerBase
import time
import random

class ImageWorker(WorkerBase):
    def __init__(self):
        super().__init__(
            worker_type='image',
            queue_name='image_queue',
            routing_keys=['task.image', 'task.image.*']
        )
    
    def process_task(self, task):
        # Simulate image processing
        image_url = task['data'].get('url')
        operation = task['data'].get('operation', 'resize')
        
        self.logger.info(f"Processing image {image_url}: {operation}")
        time.sleep(random.randint(3, 7))  # Simulate variable processing time
        
        return {
            'status': 'processed',
            'url': image_url,
            'operation': operation
        }

if __name__ == '__main__':
    worker = ImageWorker()
    worker.start()


# analytics_worker.py
from worker_base import WorkerBase
import time

class AnalyticsWorker(WorkerBase):
    def __init__(self):
        super().__init__(
            worker_type='analytics',
            queue_name='analytics_queue',
            routing_keys=['task.analytics', 'task.analytics.*']
        )
    
    def process_task(self, task):
        # Simulate analytics computation
        metric_type = task['data'].get('metric')
        
        self.logger.info(f"Computing analytics: {metric_type}")
        time.sleep(4)
        
        return {
            'status': 'computed',
            'metric': metric_type,
            'value': 42.5  # Mock result
        }

if __name__ == '__main__':
    worker = AnalyticsWorker()
    worker.start()
```

#### 4. Audit Logger:

```python
# audit_logger.py
import pika
import json
import logging
from datetime import datetime

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('audit')

connection = pika.BlockingConnection(
    pika.ConnectionParameters('localhost')
)
channel = connection.channel()

channel.exchange_declare(
    exchange='audit',
    exchange_type='fanout',
    durable=True
)

result = channel.queue_declare(queue='audit_log', durable=True)
channel.queue_bind(exchange='audit', queue='audit_log')

def callback(ch, method, properties, body):
    audit = json.loads(body)
    
    timestamp = datetime.fromtimestamp(audit['timestamp']).isoformat()
    
    logger.info(
        f"[{timestamp}] Task {audit['task_id']} "
        f"processed by {audit['worker_type']} "
        f"in {audit['duration']:.2f}s"
    )
    
    # Could write to database or file here
    
    ch.basic_ack(delivery_tag=method.delivery_tag)

channel.basic_consume(
    queue='audit_log',
    on_message_callback=callback,
    auto_ack=False
)

logger.info("Audit logger started...")
channel.start_consuming()
```

#### 5. Dead Letter Monitor:

```python
# dlq_monitor.py
import pika
import json
import logging

logging.basicConfig(
    level=logging.WARNING,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('dlq')

connection = pika.BlockingConnection(
    pika.ConnectionParameters('localhost')
)
channel = connection.channel()

channel.queue_declare(queue='dead_letters', durable=True)

def callback(ch, method, properties, body):
    try:
        task = json.loads(body)
        logger.warning(
            f"Dead letter detected: Task {task['task_id']} "
            f"of type {task['type']}"
        )
        
        # Could implement retry logic or alerting here
        
    except Exception as e:
        logger.error(f"Error processing dead letter: {e}")
    
    ch.basic_ack(delivery_tag=method.delivery_tag)

channel.basic_consume(
    queue='dead_letters',
    on_message_callback=callback,
    auto_ack=False
)

logger.info("Dead letter monitor started...")
channel.start_consuming()
```

#### 6. Docker Compose –¥–ª—è –≤—Å–µ–π —Å–∏—Å—Ç–µ–º—ã:

```yaml
# docker-compose-final.yml
version: '3.8'

services:
  rabbitmq:
    image: rabbitmq:3-management
    hostname: rabbitmq
    ports:
      - "5672:5672"
      - "15672:15672"
      - "15692:15692"
    environment:
      - RABBITMQ_DEFAULT_USER=admin
      - RABBITMQ_DEFAULT_PASS=admin
    command: >
      bash -c "
        rabbitmq-plugins enable rabbitmq_prometheus &&
        rabbitmq-server
      "
    healthcheck:
      test: ["CMD", "rabbitmq-diagnostics", "ping"]
      interval: 30s
      timeout: 10s
      retries: 5

  api:
    build: .
    command: python api_gateway.py
    ports:
      - "5000:5000"
    depends_on:
      - rabbitmq
    environment:
      - RABBITMQ_HOST=rabbitmq

  email_worker:
    build: .
    command: python email_worker.py
    depends_on:
      - rabbitmq
    deploy:
      replicas: 2

  image_worker:
    build: .
    command: python image_worker.py
    depends_on:
      - rabbitmq
    deploy:
      replicas: 3

  analytics_worker:
    build: .
    command: python analytics_worker.py
    depends_on:
      - rabbitmq

  audit_logger:
    build: .
    command: python audit_logger.py
    depends_on:
      - rabbitmq

  dlq_monitor:
    build: .
    command: python dlq_monitor.py
    depends_on:
      - rabbitmq

  prometheus:
    image: prom/prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml

  grafana:
    image: grafana/grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
```

#### 7. Load Test:

```python
# load_test.py
import requests
import time
import random
from concurrent.futures import ThreadPoolExecutor

API_URL = "http://localhost:5000/api/task"

task_types = ['email', 'image', 'analytics']

def send_task():
    task_type = random.choice(task_types)
    
    payload = {
        'type': task_type,
        'data': {
            'recipient': 'user@example.com' if task_type == 'email' else None,
            'url': 'http://example.com/image.jpg' if task_type == 'image' else None,
            'metric': 'conversion_rate' if task_type == 'analytics' else None
        },
        'priority': random.randint(1, 10)
    }
    
    try:
        response = requests.post(API_URL, json=payload, timeout=5)
        if response.status_code == 202:
            print(f"‚úì Task {task_type} queued: {response.json()['task_id']}")
        else:
            print(f"‚úó Failed: {response.status_code}")
    except Exception as e:
        print(f"‚úó Error: {e}")

def run_load_test(num_requests=100, num_threads=10):
    print(f"Starting load test: {num_requests} requests, {num_threads} threads")
    print("-" * 60)
    
    start_time = time.time()
    
    with ThreadPoolExecutor(max_workers=num_threads) as executor:
        futures = [executor.submit(send_task) for _ in range(num_requests)]
        
        for future in futures:
            future.result()
    
    duration = time.time() - start_time
    
    print("-" * 60)
    print(f"Completed in {duration:.2f}s")
    print(f"Throughput: {num_requests/duration:.2f} requests/sec")

if __name__ == '__main__':
    run_load_test(num_requests=200, num_threads=20)
```

#### 8. –ó–∞–ø—É—Å–∫ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞:

```bash
# 1. –ó–∞–ø—É—Å—Ç–∏ –≤—Å–µ —Å–µ—Ä–≤–∏—Å—ã
docker-compose -f docker-compose-final.yml up -d

# 2. –ü—Ä–æ–≤–µ—Ä—å –∑–¥–æ—Ä–æ–≤—å–µ —Å–∏—Å—Ç–µ–º—ã
curl http://localhost:5000/health

# 3. –í —Ä–∞–∑–Ω—ã—Ö —Ç–µ—Ä–º–∏–Ω–∞–ª–∞—Ö –∑–∞–ø—É—Å—Ç–∏ workers
python email_worker.py &
python image_worker.py &
python analytics_worker.py &
python audit_logger.py &
python dlq_monitor.py &

# 4. –û—Ç–ø—Ä–∞–≤—å —Ç–µ—Å—Ç–æ–≤—ã–µ –∑–∞–¥–∞—á–∏
curl -X POST http://localhost:5000/api/task \
  -H "Content-Type: application/json" \
  -d '{"type":"email","data":{"recipient":"test@example.com","subject":"Hello"}}'

# 5. –ó–∞–ø—É—Å—Ç–∏ –Ω–∞–≥—Ä—É–∑–æ—á–Ω—ã–π —Ç–µ—Å—Ç
python load_test.py

# 6. –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –≤ Web UI
# RabbitMQ: http://localhost:15672
# Prometheus: http://localhost:9090
# Grafana: http://localhost:3000

# 7. –ü—Ä–æ–≤–µ—Ä—å –º–µ—Ç—Ä–∏–∫–∏
watch -n 1 'docker exec rabbitmq rabbitmqctl list_queues name messages consumers'
```

---

## üéì –ó–∞–∫–ª—é—á–µ–Ω–∏–µ

### –ß—Ç–æ —Ç—ã –∏–∑—É—á–∏–ª:

‚úÖ **–û—Å–Ω–æ–≤—ã RabbitMQ**: Exchanges, Queues, Bindings, Routing  
‚úÖ **–ù–∞–¥–µ–∂–Ω–æ—Å—Ç—å**: Durability, Acknowledgments, Publisher Confirms  
‚úÖ **High Availability**: Clustering, Quorum Queues  
‚úÖ **–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥**: Prometheus, Grafana, Health Checks  
‚úÖ **Security**: RBAC, Virtual Hosts, Permissions  
‚úÖ **Advanced Patterns**: RPC, Event-Driven, SAGA, Circuit Breaker  
‚úÖ **Production**: Deployment, Troubleshooting, Performance Tuning  


### –ß—Ç–æ –¥–µ–ª–∞—Ç—å —Å —ç—Ç–∏–º –∫—É—Ä—Å–æ–º?

- –ü—Ä–æ–π–¥–∏ –º–æ–¥—É–ª–∏ 1-4, 7 ‚Äî —ç—Ç–æ –æ—Å–Ω–æ–≤–∞ –¥–ª—è DevOps
- –ú–æ–¥—É–ª–∏ 5-6 ‚Äî –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ security (–∫—Ä–∏—Ç–∏—á–Ω–æ!)
- –ú–æ–¥—É–ª–∏ 8-9 ‚Äî –ø—Ä–æ–ø—É—Å—Ç–∏ –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ –ø—Ä–æ—á–∏—Ç–∞–π –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è

### –§–æ–∫—É—Å –Ω–∞:
- CLI –∫–æ–º–∞–Ω–¥—ã
- Troubleshooting
- Monitoring setup
- Production deployment

–ö–æ–¥ –Ω–∞ Python –≤–æ—Å–ø—Ä–∏–Ω–∏–º–∞–π –∫–∞–∫ –±–æ–Ω—É—Å –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏, –∞ –Ω–µ –æ—Å–Ω–æ–≤–Ω–æ–π –Ω–∞–≤—ã–∫!

### –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:

1. **–ü—Ä–∞–∫—Ç–∏–∫–∞**: –†–µ–∞–ª–∏–∑—É–π —Ä–µ–∞–ª—å–Ω—ã–π –ø—Ä–æ–µ–∫—Ç —Å RabbitMQ
2. **–ò–∑—É—á–∏**: Kafka –¥–ª—è stream processing
3. **–û—Å–≤–æ–π**: Service Mesh (Istio, Linkerd)
4. **–í–Ω–µ–¥—Ä–∏**: –í production —Å –ø–æ–ª–Ω—ã–º –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–æ–º
