# –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –¥–ª—è DevOps: –ï–∂–µ–≥–æ–¥–Ω—ã–π/–ü–æ–ª—É–≥–æ–¥–æ–≤–æ–π –∫—É—Ä—Å-–æ—Å–≤–µ–∂–∏—Ç–µ–ª—å

**–¶–µ–ª—å:** –û—Å–≤–µ–∂–∏—Ç—å –≤ –ø–∞–º—è—Ç–∏ –∫–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∑–∞ 2-3 —á–∞—Å–∞ –ø—Ä–∞–∫—Ç–∏–∫–∏ –∏ —É–∑–Ω–∞—Ç—å 1-2 –Ω–æ–≤—ã–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏.

**–§–æ—Ä–º–∞—Ç:** –ö–∞–∂–¥—ã–π —Ä–∞–∑–¥–µ–ª —Å–æ—Å—Ç–æ–∏—Ç –∏–∑:
1. **–ö—Ä–∞—Ç–∫–æ–π —Ç–µ–æ—Ä–∏–∏ (–ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞)**: –°–∞–º–æ–µ –≥–ª–∞–≤–Ω–æ–µ, —á—Ç–æ –≤—ã –º–æ–≥–ª–∏ –∑–∞–±—ã—Ç—å
2. **–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–≥–æ –∑–∞–¥–∞–Ω–∏—è**: –†–µ–∞–ª—å–Ω–∞—è –∑–∞–¥–∞—á–∞, –∫–æ—Ç–æ—Ä—É—é –Ω—É–∂–Ω–æ —Ä–µ—à–∏—Ç—å
3. **–ë–æ–Ω—É—Å–Ω–æ–≥–æ –∑–∞–¥–∞–Ω–∏—è (–¥–ª—è —Ä–æ—Å—Ç–∞)**: –ó–∞–¥–∞—á–∞ –ø–æ—Å–ª–æ–∂–Ω–µ–µ –∏–ª–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –Ω–æ–≤–æ–π —Ñ–∏—á–∏

**–ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è:**
- –ë–∞–∑–æ–≤–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ Linux/Unix
- –î–æ—Å—Ç—É–ø –∫ —Å–µ—Ä–≤–µ—Ä—É –∏–ª–∏ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–π –º–∞—à–∏–Ω–µ
- Docker —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω (–¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ –∑–∞–¥–∞–Ω–∏–π)
- –ë–∞–∑–æ–≤—ã–µ –∑–Ω–∞–Ω–∏—è –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏

---

## –ú–æ–¥—É–ª—å 1: –û—Å–Ω–æ–≤—ã –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∏ –º–µ—Ç—Ä–∏–∫–∏ (20 –º–∏–Ω—É—Ç)

### üéØ –ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞

**–ß–µ—Ç—ã—Ä–µ –∑–æ–ª–æ—Ç—ã—Ö —Å–∏–≥–Ω–∞–ª–∞ (Four Golden Signals):**
```
1. Latency (–ó–∞–¥–µ—Ä–∂–∫–∞)      - –í—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –∑–∞–ø—Ä–æ—Å—ã
2. Traffic (–¢—Ä–∞—Ñ–∏–∫)        - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø—Ä–æ—Å–æ–≤
3. Errors (–û—à–∏–±–∫–∏)         - –ü—Ä–æ—Ü–µ–Ω—Ç –Ω–µ—É–¥–∞—á–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
4. Saturation (–ù–∞—Å—ã—â–µ–Ω–∏–µ)  - –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤ (CPU, –ø–∞–º—è—Ç—å, –¥–∏—Å–∫)
```

**–¢–∏–ø—ã –º–µ—Ç—Ä–∏–∫:**
```
Counter   - –ú–æ–Ω–æ—Ç–æ–Ω–Ω–æ –≤–æ–∑—Ä–∞—Å—Ç–∞—é—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ (–∑–∞–ø—Ä–æ—Å—ã, –æ—à–∏–±–∫–∏)
Gauge     - –¢–µ–∫—É—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ (CPU, –ø–∞–º—è—Ç—å, —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞)
Histogram - –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π (latency buckets)
Summary   - –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∑–∞ –ø–µ—Ä–∏–æ–¥ (percentiles)
```

**USE Method (–¥–ª—è —Ä–µ—Å—É—Ä—Å–æ–≤):**
```
Utilization - –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –∑–∞–Ω—è—Ç–æ—Å—Ç–∏ —Ä–µ—Å—É—Ä—Å–∞
Saturation  - –°—Ç–µ–ø–µ–Ω—å –ø–µ—Ä–µ–≥—Ä—É–∑–∫–∏
Errors      - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—à–∏–±–æ–∫
```

**RED Method (–¥–ª—è —Å–µ—Ä–≤–∏—Å–æ–≤):**
```
Rate     - –ó–∞–ø—Ä–æ—Å–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É
Errors   - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—à–∏–±–æ–∫
Duration - –í—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞
```

**–£—Ä–æ–≤–Ω–∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Application (APM)             ‚îÇ  - –ö–æ–¥, —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ   Service/Container             ‚îÇ  - Docker, K8s
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ   Operating System              ‚îÇ  - CPU, RAM, Disk
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ   Infrastructure                ‚îÇ  - Network, Hardware
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**–ö–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ Linux:**
```bash
# CPU
top, htop
mpstat -P ALL 1

# Memory
free -m
vmstat 1

# Disk I/O
iostat -x 1
iotop

# Network
iftop
nethogs
ss -s

# Process
ps aux --sort=-%mem | head
ps aux --sort=-%cpu | head
```

**–ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π:**
```
- Request rate (req/s)
- Error rate (%)
- Response time (ms) - p50, p95, p99
- Active connections
- Queue depth
- Database query time
- Cache hit ratio
```

### üíª –ó–∞–¥–∞–Ω–∏–µ

–ù–∞—Å—Ç—Ä–æ–π –±–∞–∑–æ–≤—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Å–∏—Å—Ç–µ–º—ã:

1. **–£—Å—Ç–∞–Ω–æ–≤–∏ –∏ –∑–∞–ø—É—Å—Ç–∏ Node Exporter** (–¥–ª—è —Å–±–æ—Ä–∞ –º–µ—Ç—Ä–∏–∫ —Ö–æ—Å—Ç–∞):
```bash
# –ß–µ—Ä–µ–∑ Docker
docker run -d \
  --name node-exporter \
  --net="host" \
  --pid="host" \
  -v "/:/host:ro,rslave" \
  prom/node-exporter:latest \
  --path.rootfs=/host

# –ü—Ä–æ–≤–µ—Ä–∫–∞
curl http://localhost:9100/metrics | head -20
```

2. **–ò–∑—É—á–∏ –æ—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏**:
```bash
# CPU
curl -s http://localhost:9100/metrics | grep node_cpu_seconds_total

# Memory
curl -s http://localhost:9100/metrics | grep node_memory

# Disk
curl -s http://localhost:9100/metrics | grep node_disk

# Network
curl -s http://localhost:9100/metrics | grep node_network
```

3. **–°–æ–∑–¥–∞–π –ø—Ä–æ—Å—Ç–æ–π bash —Å–∫—Ä–∏–ø—Ç** –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ (`monitor.sh`):
```bash
#!/bin/bash

echo "=== System Monitoring Report ==="
echo "Date: $(date)"
echo ""

# CPU Usage
echo "CPU Usage:"
top -bn1 | grep "Cpu(s)" | awk '{print "  User: " $2 ", System: " $4 ", Idle: " $8}'

# Memory Usage
echo ""
echo "Memory Usage:"
free -h | awk 'NR==2{printf "  Total: %s, Used: %s (%.2f%%)\n", $2, $3, $3*100/$2}'

# Disk Usage
echo ""
echo "Disk Usage:"
df -h / | awk 'NR==2{printf "  Total: %s, Used: %s (%s)\n", $2, $3, $5}'

# Load Average
echo ""
echo "Load Average:"
uptime | awk -F'load average:' '{print "  " $2}'

# Top 5 processes by CPU
echo ""
echo "Top 5 processes by CPU:"
ps aux --sort=-%cpu | head -6 | tail -5 | awk '{printf "  %s: %.1f%%\n", $11, $3}'

# Top 5 processes by Memory
echo ""
echo "Top 5 processes by Memory:"
ps aux --sort=-%mem | head -6 | tail -5 | awk '{printf "  %s: %.1f%%\n", $11, $4}'
```

4. –ó–∞–ø—É—Å—Ç–∏ —Å–∫—Ä–∏–ø—Ç:
```bash
chmod +x monitor.sh
./monitor.sh
```

### üöÄ –ë–æ–Ω—É—Å (–Ω–æ–≤–æ–µ)

**–ù–∞—Å—Ç—Ä–æ–π cAdvisor** –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ Docker –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤:
```bash
docker run -d \
  --name=cadvisor \
  --volume=/:/rootfs:ro \
  --volume=/var/run:/var/run:ro \
  --volume=/sys:/sys:ro \
  --volume=/var/lib/docker/:/var/lib/docker:ro \
  --publish=8080:8080 \
  --detach=true \
  gcr.io/cadvisor/cadvisor:latest

# –û—Ç–∫—Ä–æ–π –≤ –±—Ä–∞—É–∑–µ—Ä–µ
http://localhost:8080
```

**–°–æ–∑–¥–∞–π —Å–≤–æ–π custom exporter** –Ω–∞ Python:
```python
# custom_exporter.py
from prometheus_client import start_http_server, Gauge, Counter
import time
import random

# –°–æ–∑–¥–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
request_gauge = Gauge('app_requests_in_progress', 'Number of requests in progress')
request_counter = Counter('app_requests_total', 'Total number of requests')
error_counter = Counter('app_errors_total', 'Total number of errors')

def process_request():
    """–°–∏–º—É–ª–∏—Ä—É–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –∑–∞–ø—Ä–æ—Å–∞"""
    request_gauge.inc()
    request_counter.inc()
    
    # –°–ª—É—á–∞–π–Ω–∞—è –æ—à–∏–±–∫–∞
    if random.random() < 0.1:
        error_counter.inc()
    
    time.sleep(random.uniform(0.1, 0.5))
    request_gauge.dec()

if __name__ == '__main__':
    start_http_server(8000)
    print("Exporter started on port 8000")
    
    while True:
        process_request()
        time.sleep(random.uniform(0.5, 2))
```

---

## –ú–æ–¥—É–ª—å 2: Prometheus - —Å–±–æ—Ä –∏ —Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ (30 –º–∏–Ω—É—Ç)

### üéØ –ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞

**–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ Prometheus:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Targets   ‚îÇ ‚Üê HTTP Pull (scrape)
‚îÇ  (Metrics)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
   ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ Prom-  ‚îÇ
   ‚îÇ etheus ‚îÇ ‚Üê Time Series DB (TSDB)
   ‚îÇ Server ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
   ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ Alert- ‚îÇ
   ‚îÇ manager‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Prometheus config structure:**
```yaml
global:
  scrape_interval: 15s      # –ö–∞–∫ —á–∞—Å—Ç–æ —Å–æ–±–∏—Ä–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏
  evaluation_interval: 15s  # –ö–∞–∫ —á–∞—Å—Ç–æ –ø—Ä–æ–≤–µ—Ä—è—Ç—å –ø—Ä–∞–≤–∏–ª–∞

scrape_configs:
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']
```

**PromQL –æ—Å–Ω–æ–≤—ã:**
```promql
# Instant vector - —Ç–µ–∫—É—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
node_cpu_seconds_total

# Range vector - –∑–Ω–∞—á–µ–Ω–∏—è –∑–∞ –ø–µ—Ä–∏–æ–¥
node_cpu_seconds_total[5m]

# –§–∏–ª—å—Ç—Ä—ã
node_cpu_seconds_total{mode="idle"}
node_cpu_seconds_total{mode!="idle"}
node_cpu_seconds_total{mode=~"user|system"}

# –ê–≥—Ä–µ–≥–∞—Ü–∏—è
sum(node_cpu_seconds_total)
avg(node_cpu_seconds_total)
max(node_cpu_seconds_total)
min(node_cpu_seconds_total)
count(node_cpu_seconds_total)

# –ü–æ label
sum(node_cpu_seconds_total) by (mode)
sum(node_cpu_seconds_total) by (cpu)

# –§—É–Ω–∫—Ü–∏–∏
rate(node_cpu_seconds_total[5m])           # –°–∫–æ—Ä–æ—Å—Ç—å –∏–∑–º–µ–Ω–µ–Ω–∏—è
irate(node_cpu_seconds_total[5m])          # –ú–≥–Ω–æ–≤–µ–Ω–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å
increase(node_cpu_seconds_total[5m])       # –£–≤–µ–ª–∏—á–µ–Ω–∏–µ –∑–∞ –ø–µ—Ä–∏–æ–¥
delta(node_cpu_seconds_total[5m])          # –ò–∑–º–µ–Ω–µ–Ω–∏–µ
```

**–†–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã:**
```promql
# CPU utilization
100 - (avg by (instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)

# Memory usage %
(1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100

# Disk usage %
100 - ((node_filesystem_avail_bytes / node_filesystem_size_bytes) * 100)

# Network traffic
rate(node_network_receive_bytes_total[5m])
rate(node_network_transmit_bytes_total[5m])

# HTTP request rate
rate(http_requests_total[5m])

# Error rate
rate(http_requests_total{status=~"5.."}[5m]) / rate(http_requests_total[5m])

# Latency percentiles (–¥–ª—è histogram)
histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))
histogram_quantile(0.99, rate(http_request_duration_seconds_bucket[5m]))
```

**Metric types –≤ –¥–µ—Ç–∞–ª—è—Ö:**
```promql
# Counter - —Ç–æ–ª—å–∫–æ —Ä–∞—Å—Ç–µ—Ç
http_requests_total
# –ò—Å–ø–æ–ª—å–∑—É–π rate() –∏–ª–∏ increase()
rate(http_requests_total[5m])

# Gauge - –º–æ–∂–µ—Ç —Ä–∞—Å—Ç–∏ –∏ –ø–∞–¥–∞—Ç—å
node_memory_MemAvailable_bytes
# –ò—Å–ø–æ–ª—å–∑—É–π –Ω–∞–ø—Ä—è–º—É—é –∏–ª–∏ —Å —Ñ—É–Ω–∫—Ü–∏—è–º–∏ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏
avg(node_memory_MemAvailable_bytes)

# Histogram - —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π
http_request_duration_seconds_bucket
http_request_duration_seconds_sum
http_request_duration_seconds_count
# –ò—Å–ø–æ–ª—å–∑—É–π histogram_quantile()
histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))

# Summary - –ø—Ä–µ–¥—Ä–∞—Å—á–∏—Ç–∞–Ω–Ω—ã–µ –∫–≤–∞–Ω—Ç–∏–ª–∏
http_request_duration_seconds{quantile="0.95"}
```

**Recording rules** (–¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏):
```yaml
groups:
  - name: example
    interval: 30s
    rules:
    - record: job:node_cpu_utilization:avg
      expr: 100 - (avg by (job) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)
```

**Alerting rules:**
```yaml
groups:
  - name: alerts
    rules:
    - alert: HighCPUUsage
      expr: job:node_cpu_utilization:avg > 80
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "High CPU usage on {{ $labels.instance }}"
        description: "CPU usage is {{ $value }}%"
```

### üíª –ó–∞–¥–∞–Ω–∏–µ

–ù–∞—Å—Ç—Ä–æ–π –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π Prometheus:

1. **–°–æ–∑–¥–∞–π docker-compose.yml**:
```yaml
version: '3.8'

services:
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - ./alerts.yml:/etc/prometheus/alerts.yml
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--web.enable-lifecycle'
    restart: unless-stopped

  node-exporter:
    image: prom/node-exporter:latest
    container_name: node-exporter
    ports:
      - "9100:9100"
    command:
      - '--path.rootfs=/host'
    pid: host
    restart: unless-stopped
    volumes:
      - '/:/host:ro,rslave'

  cadvisor:
    image: gcr.io/cadvisor/cadvisor:latest
    container_name: cadvisor
    ports:
      - "8080:8080"
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
      - /dev/disk/:/dev/disk:ro
    privileged: true
    restart: unless-stopped

volumes:
  prometheus-data:
```

2. **–°–æ–∑–¥–∞–π prometheus.yml**:
```yaml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–∞–≤–∏–ª –∞–ª–µ—Ä—Ç–æ–≤
rule_files:
  - "alerts.yml"

scrape_configs:
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  - job_name: 'node-exporter'
    static_configs:
      - targets: ['node-exporter:9100']

  - job_name: 'cadvisor'
    static_configs:
      - targets: ['cadvisor:8080']
```

3. **–°–æ–∑–¥–∞–π alerts.yml**:
```yaml
groups:
  - name: system_alerts
    rules:
    - alert: HighCPUUsage
      expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "High CPU usage detected"
        description: "CPU usage is above 80% (current value: {{ $value }}%)"

    - alert: HighMemoryUsage
      expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 90
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "High memory usage detected"
        description: "Memory usage is above 90% (current value: {{ $value }}%)"

    - alert: DiskSpaceLow
      expr: (1 - (node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"})) * 100 > 85
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Low disk space"
        description: "Disk usage is above 85% (current value: {{ $value }}%)"

    - alert: InstanceDown
      expr: up == 0
      for: 1m
      labels:
        severity: critical
      annotations:
        summary: "Instance {{ $labels.instance }} down"
        description: "{{ $labels.instance }} has been down for more than 1 minute"
```

4. **–ó–∞–ø—É—Å—Ç–∏ stack**:
```bash
docker-compose up -d

# –ü—Ä–æ–≤–µ—Ä–∫–∞
docker-compose ps
curl http://localhost:9090/api/v1/targets
```

5. **–û—Ç–∫—Ä–æ–π Prometheus UI** –∏ –ø–æ–ø—Ä–æ–±—É–π –∑–∞–ø—Ä–æ—Å—ã:
```
–ü–µ—Ä–µ–π–¥–∏: http://localhost:9090

–ü–æ–ø—Ä–æ–±—É–π –∑–∞–ø—Ä–æ—Å—ã:
- node_cpu_seconds_total
- rate(node_cpu_seconds_total[5m])
- 100 - (avg(irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)
- node_memory_MemAvailable_bytes / 1024 / 1024 / 1024
```

### üöÄ –ë–æ–Ω—É—Å (–Ω–æ–≤–æ–µ)

**–ù–∞—Å—Ç—Ä–æ–π Service Discovery** –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è —Ü–µ–ª–µ–π:

**File-based SD** (`file_sd.json`):
```json
[
  {
    "targets": ["node-exporter:9100"],
    "labels": {
      "job": "node",
      "env": "production"
    }
  },
  {
    "targets": ["cadvisor:8080"],
    "labels": {
      "job": "containers",
      "env": "production"
    }
  }
]
```

–î–æ–±–∞–≤—å –≤ `prometheus.yml`:
```yaml
scrape_configs:
  - job_name: 'dynamic-targets'
    file_sd_configs:
      - files:
        - '/etc/prometheus/file_sd.json'
        refresh_interval: 30s
```

**–ù–∞—Å—Ç—Ä–æ–π Pushgateway** –¥–ª—è –º–µ—Ç—Ä–∏–∫ batch jobs:
```bash
docker run -d \
  --name pushgateway \
  -p 9091:9091 \
  prom/pushgateway

# Push –º–µ—Ç—Ä–∏–∫—É
echo "backup_duration_seconds 125.5" | curl --data-binary @- http://localhost:9091/metrics/job/backup/instance/db1

# –î–æ–±–∞–≤—å –≤ prometheus.yml
scrape_configs:
  - job_name: 'pushgateway'
    static_configs:
      - targets: ['pushgateway:9091']
    honor_labels: true
```

**Recording rules –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏**:
```yaml
# recording_rules.yml
groups:
  - name: performance_rules
    interval: 30s
    rules:
    # CPU utilization per instance
    - record: instance:node_cpu_utilization:rate5m
      expr: 100 - (avg by (instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)
    
    # Memory utilization per instance
    - record: instance:node_memory_utilization:ratio
      expr: 1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)
    
    # Request rate per job
    - record: job:http_requests:rate5m
      expr: sum(rate(http_requests_total[5m])) by (job)
```

---

## –ú–æ–¥—É–ª—å 3: Grafana - –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö (30 –º–∏–Ω—É—Ç)

### üéØ –ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞

**–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ Grafana:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Data     ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ Grafana  ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ Users    ‚îÇ
‚îÇ Sources  ‚îÇ      ‚îÇ Server   ‚îÇ      ‚îÇ          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
   ‚îÇ                    ‚îÇ
   ‚îÇ                    ‚îÇ
   ‚ñº                    ‚ñº
Prometheus       Dashboards
InfluxDB         Alerts
Elasticsearch    Users
Loki             Teams
```

**–¢–∏–ø—ã –ø–∞–Ω–µ–ª–µ–π:**
```
Graph        - –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä—è–¥—ã
Stat         - –û–¥–Ω–æ –∑–Ω–∞—á–µ–Ω–∏–µ
Gauge        - –®–∫–∞–ª–∞
Bar Gauge    - –ì–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω—ã–µ –ø–æ–ª–æ—Å–∫–∏
Table        - –¢–∞–±–ª–∏—Ü–∞
Heatmap      - –¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞
Logs         - –õ–æ–≥–∏
```

**–ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–∞—à–±–æ—Ä–¥–∞:**
```
Query      - –ò–∑ –¥–∞–Ω–Ω—ã—Ö (label_values(metric, label))
Custom     - –°–ø–∏—Å–æ–∫ –∑–Ω–∞—á–µ–Ω–∏–π
Constant   - –ö–æ–Ω—Å—Ç–∞–Ω—Ç–∞
Interval   - –í—Ä–µ–º–µ–Ω–Ω–æ–π –∏–Ω—Ç–µ—Ä–≤–∞–ª
Data source - –í—ã–±–æ—Ä –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö
```

**–ü–æ–ª–µ–∑–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ Grafana:**
```
$__interval        - –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –∏–Ω—Ç–µ—Ä–≤–∞–ª
$__rate_interval   - –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª –¥–ª—è rate()
$timeFilter        - –í—Ä–µ–º–µ–Ω–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä
$__from / $__to    - –ù–∞—á–∞–ª–æ/–∫–æ–Ω–µ—Ü –ø–µ—Ä–∏–æ–¥–∞

# –ü—Ä–∏–º–µ—Ä —Å –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
rate(http_requests_total{job="$job"}[$__rate_interval])
```

**Templating examples:**
```promql
# –ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è instance
label_values(node_cpu_seconds_total, instance)

# –ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è job
label_values(up, job)

# –ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è mountpoint
label_values(node_filesystem_size_bytes, mountpoint)

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ –∑–∞–ø—Ä–æ—Å–µ
node_filesystem_avail_bytes{instance="$instance", mountpoint="$mountpoint"}
```

**Alert channels:**
```
Email
Slack
PagerDuty
Webhook
Telegram
Discord
Teams
OpsGenie
```

**Dashboard best practices:**
```
1. –ò—Å–ø–æ–ª—å–∑—É–π Row –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ –ø–∞–Ω–µ–ª–µ–π
2. –î–æ–±–∞–≤–ª—è–π –æ–ø–∏—Å–∞–Ω–∏—è –∫ –ø–∞–Ω–µ–ª—è–º
3. –ò—Å–ø–æ–ª—å–∑—É–π –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –≥–∏–±–∫–æ—Å—Ç–∏
4. –£–∫–∞–∑—ã–≤–∞–π –µ–¥–∏–Ω–∏—Ü—ã –∏–∑–º–µ—Ä–µ–Ω–∏—è
5. –ò—Å–ø–æ–ª—å–∑—É–π —Ü–≤–µ—Ç–æ–≤—ã–µ –ø–æ—Ä–æ–≥–∏
6. –î–æ–±–∞–≤–ª—è–π —Å—Å—ã–ª–∫–∏ –Ω–∞ runbook'–∏
7. –ì—Ä—É–ø–ø–∏—Ä—É–π —Å–≤—è–∑–∞–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
8. –ò—Å–ø–æ–ª—å–∑—É–π consistent naming
```

### üíª –ó–∞–¥–∞–Ω–∏–µ

–ù–∞—Å—Ç—Ä–æ–π Grafana –∏ —Å–æ–∑–¥–∞–π dashboard:

1. **–î–æ–±–∞–≤—å Grafana –≤ docker-compose.yml**:
```yaml
  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - grafana-data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning
    restart: unless-stopped
    depends_on:
      - prometheus

volumes:
  grafana-data:
```

2. **–°–æ–∑–¥–∞–π provisioning –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏** (`grafana/provisioning/datasources/prometheus.yml`):
```yaml
apiVersion: 1

datasources:
  - name: Prometheus
    type: prometheus
    access: proxy
    url: http://prometheus:9090
    isDefault: true
    editable: true
```

3. **–°–æ–∑–¥–∞–π provisioning –¥–ª—è dashboard** (`grafana/provisioning/dashboards/dashboard.yml`):
```yaml
apiVersion: 1

providers:
  - name: 'Default'
    orgId: 1
    folder: ''
    type: file
    disableDeletion: false
    updateIntervalSeconds: 10
    allowUiUpdates: true
    options:
      path: /etc/grafana/provisioning/dashboards
```

4. **–ó–∞–ø—É—Å—Ç–∏ Grafana**:
```bash
# –°–æ–∑–¥–∞–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
mkdir -p grafana/provisioning/datasources
mkdir -p grafana/provisioning/dashboards

docker-compose up -d grafana

# –û—Ç–∫—Ä–æ–π –≤ –±—Ä–∞—É–∑–µ—Ä–µ
http://localhost:3000
# Login: admin
# Password: admin
```

5. **–°–æ–∑–¥–∞–π System Monitoring Dashboard** –≤—Ä—É—á–Ω—É—é:

**Panel 1: CPU Usage**
- Visualization: Time series
- Query: `100 - (avg(irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)`
- Legend: CPU Usage %
- Unit: Percent (0-100)
- Threshold: Yellow at 70, Red at 90

**Panel 2: Memory Usage**
- Visualization: Time series
- Query: `(1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100`
- Legend: Memory Usage %
- Unit: Percent (0-100)

**Panel 3: Disk Usage**
- Visualization: Gauge
- Query: `100 - ((node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"}) * 100)`
- Unit: Percent (0-100)
- Threshold: Green 0-70, Yellow 70-85, Red 85-100

**Panel 4: Network Traffic**
- Visualization: Time series
- Query A: `rate(node_network_receive_bytes_total[5m]) / 1024 / 1024`
- Query B: `rate(node_network_transmit_bytes_total[5m]) / 1024 / 1024`
- Unit: MB/s

**Panel 5: Top Processes by CPU**
- Visualization: Table
- Query: `topk(5, irate(process_cpu_seconds_total[5m]))`

6. **–°–æ–∑–¥–∞–π –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è dashboard**:
- Variable: instance
  - Type: Query
  - Query: `label_values(node_cpu_seconds_total, instance)`
  
–ò–∑–º–µ–Ω–∏ –∑–∞–ø—Ä–æ—Å—ã –Ω–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π:
```promql
100 - (avg(irate(node_cpu_seconds_total{instance="$instance", mode="idle"}[5m])) * 100)
```

### üöÄ –ë–æ–Ω—É—Å (–Ω–æ–≤–æ–µ)

**–°–æ–∑–¥–∞–π JSON dashboard —á–µ—Ä–µ–∑ provisioning** (`grafana/provisioning/dashboards/system-overview.json`):
```json
{
  "dashboard": {
    "title": "System Overview",
    "tags": ["system", "monitoring"],
    "timezone": "browser",
    "panels": [
      {
        "id": 1,
        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 0},
        "type": "timeseries",
        "title": "CPU Usage",
        "targets": [
          {
            "expr": "100 - (avg(irate(node_cpu_seconds_total{instance=\"$instance\",mode=\"idle\"}[5m])) * 100)",
            "legendFormat": "CPU Usage %"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "percent",
            "thresholds": {
              "steps": [
                {"value": 0, "color": "green"},
                {"value": 70, "color": "yellow"},
                {"value": 90, "color": "red"}
              ]
            }
          }
        }
      }
    ],
    "templating": {
      "list": [
        {
          "name": "instance",
          "type": "query",
          "datasource": "Prometheus",
          "query": "label_values(node_cpu_seconds_total, instance)",
          "refresh": 1
        }
      ]
    }
  }
}
```

**–ù–∞—Å—Ç—Ä–æ–π Alerting –≤ Grafana**:
1. Configuration ‚Üí Alerting ‚Üí Contact points
2. –°–æ–∑–¥–∞–π Email contact point
3. –°–æ–∑–¥–∞–π Alert rule:
   - Name: High CPU Alert
   - Query: `avg(irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100 < 20`
   - Condition: WHEN last() OF query(A) IS BELOW 20
   - For: 5m

**–£—Å—Ç–∞–Ω–æ–≤–∏ Grafana plugins**:
```bash
# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ —á–µ—Ä–µ–∑ UI
Configuration ‚Üí Plugins ‚Üí Search

# –ü–æ–ª–µ–∑–Ω—ã–µ –ø–ª–∞–≥–∏–Ω—ã:
- Pie Chart
- Worldmap Panel
- Clock Panel
- Status Panel

# –ß–µ—Ä–µ–∑ CLI (–≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–µ)
docker exec grafana grafana-cli plugins install grafana-piechart-panel
docker restart grafana
```

---
## –ú–æ–¥—É–ª—å 4: –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–∞—Ü–∏—è –ª–æ–≥–æ–≤ (30 –º–∏–Ω—É—Ç)

### üéØ –ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞

**–£—Ä–æ–≤–Ω–∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è:**

```
TRACE   - –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
DEBUG   - –û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
INFO    - –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
WARN    - –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
ERROR   - –û—à–∏–±–∫–∏, –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω—ã–µ –¥–ª—è —Ä–∞–±–æ—Ç—ã
FATAL   - –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏, –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –ø–∞–¥–∞–µ—Ç
```

**Structured logging (JSON):**

json

````json
{
  "timestamp": "2025-01-15T10:30:00Z",
  "level": "ERROR",
  "service": "api",
  "message": "Database connection failed",
  "error": "connection timeout",
  "user_id": "12345",
  "request_id": "abc-123",
  "duration_ms": 5000
}
```

**ELK Stack:**
```
Elasticsearch  - –•—Ä–∞–Ω–µ–Ω–∏–µ –∏ –ø–æ–∏—Å–∫
Logstash       - –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –ø–∞—Ä—Å–∏–Ω–≥
Kibana         - –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
```

**Alternative: Loki Stack:**
```
Loki           - –•—Ä–∞–Ω–µ–Ω–∏–µ –ª–æ–≥–æ–≤ (–∫–∞–∫ Prometheus –¥–ª—è –ª–æ–≥–æ–≤)
Promtail       - –ê–≥–µ–Ω—Ç —Å–±–æ—Ä–∞ (–∫–∞–∫ node-exporter)
Grafana        - –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
```

**Log aggregation patterns:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   App    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
                ‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îú‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ Log     ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ Centralized  ‚îÇ
‚îÇ   App    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚î§    ‚îÇ Shipper ‚îÇ    ‚îÇ Log Storage  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ   App    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
````

**–ü–æ–ª–µ–∑–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã –¥–ª—è –ª–æ–≥–æ–≤:**

bash

```bash
# journalctl (systemd)
journalctl -u nginx                  # –õ–æ–≥–∏ —Å–µ—Ä–≤–∏—Å–∞
journalctl -f                        # Follow –ª–æ–≥–∏
journalctl --since "1 hour ago"
journalctl -p err                    # –¢–æ–ª—å–∫–æ –æ—à–∏–±–∫–∏
journalctl --disk-usage              # –†–∞–∑–º–µ—Ä –ª–æ–≥–æ–≤

# Docker logs
docker logs <container>
docker logs -f <container>
docker logs --tail 100 <container>
docker logs --since 1h <container>

# –¢—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–µ –ª–æ–≥–∏ Linux
tail -f /var/log/syslog
tail -f /var/log/nginx/access.log
grep "ERROR" /var/log/application.log
zgrep "pattern" /var/log/old.log.gz  # –ü–æ–∏—Å–∫ –≤ —Å–∂–∞—Ç—ã—Ö –ª–æ–≥–∞—Ö

# –õ–æ–≥–∏ —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏
tail -f /var/log/app.log | ts '%Y-%m-%d %H:%M:%S'

# –ú–Ω–æ–≥–æ—Ñ–∞–π–ª–æ–≤—ã–π tail
multitail /var/log/nginx/access.log /var/log/nginx/error.log

# –ê–Ω–∞–ª–∏–∑ –ª–æ–≥–æ–≤
awk '{print $1}' access.log | sort | uniq -c | sort -rn | head -10  # Top 10 IP
grep "500" access.log | wc -l  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ 500 –æ—à–∏–±–æ–∫
```

**Log rotation:**

bash

```bash
# logrotate –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (/etc/logrotate.d/app)
/var/log/app/*.log {
    daily                # –†–æ—Ç–∞—Ü–∏—è –∫–∞–∂–¥—ã–π –¥–µ–Ω—å
    rotate 7             # –•—Ä–∞–Ω–∏—Ç—å 7 –∞—Ä—Ö–∏–≤–æ–≤
    compress             # –°–∂–∏–º–∞—Ç—å —Å—Ç–∞—Ä—ã–µ
    delaycompress        # –ù–µ —Å–∂–∏–º–∞—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–π
    missingok            # –ù–µ –æ—à–∏–±–∞—Ç—å—Å—è –µ—Å–ª–∏ —Ñ–∞–π–ª–∞ –Ω–µ—Ç
    notifempty           # –ù–µ —Ä–æ—Ç–∏—Ä–æ–≤–∞—Ç—å –ø—É—Å—Ç—ã–µ
    create 0640 app app  # –°–æ–∑–¥–∞—Ç—å —Å –ø—Ä–∞–≤–∞–º–∏
    sharedscripts
    postrotate
        systemctl reload app > /dev/null
    endscript
}

# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
logrotate -d /etc/logrotate.d/app    # Dry run
logrotate -f /etc/logrotate.d/app    # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —Ä–æ—Ç–∞—Ü–∏—è
```

**–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è—Ö:**

**Python (structured logging):**

python

```python
import logging
import json
from datetime import datetime

class JSONFormatter(logging.Formatter):
    def format(self, record):
        log_data = {
            "timestamp": datetime.utcnow().isoformat(),
            "level": record.levelname,
            "service": "my-api",
            "message": record.getMessage(),
            "module": record.module,
            "function": record.funcName,
            "line": record.lineno
        }
        if record.exc_info:
            log_data["exception"] = self.formatException(record.exc_info)
        return json.dumps(log_data)

logging.basicConfig(level=logging.INFO)
handler = logging.StreamHandler()
handler.setFormatter(JSONFormatter())
logger = logging.getLogger()
logger.handlers = [handler]

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
logger.info("User logged in", extra={"user_id": "123", "ip": "192.168.1.1"})
logger.error("Database error", extra={"query": "SELECT *", "duration_ms": 5000})
```

**Node.js (Winston):**

javascript

```javascript
const winston = require('winston');

const logger = winston.createLogger({
  level: 'info',
  format: winston.format.combine(
    winston.format.timestamp(),
    winston.format.json()
  ),
  defaultMeta: { service: 'api-service' },
  transports: [
    new winston.transports.File({ filename: 'error.log', level: 'error' }),
    new winston.transports.File({ filename: 'combined.log' }),
    new winston.transports.Console({
      format: winston.format.simple()
    })
  ]
});

// –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
logger.info('User action', { user_id: '123', action: 'login' });
logger.error('Database error', { error: err.message, query: sql });
```

**Loki query patterns (LogQL):**

logql

```logql
# –ë–∞–∑–æ–≤—ã–π –ø–æ–∏—Å–∫
{job="varlogs"}

# –§–∏–ª—å—Ç—Ä—ã
{job="varlogs"} |= "error"                    # –°–æ–¥–µ—Ä–∂–∏—Ç "error"
{job="varlogs"} != "debug"                    # –ù–µ —Å–æ–¥–µ—Ä–∂–∏—Ç "debug"
{job="varlogs"} |~ "error|ERROR"              # Regex
{job="varlogs"} !~ "info|INFO"                # –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–π regex

# JSON parsing
{job="varlogs"} | json | level="error"
{job="varlogs"} | json | response_time > 1000

# –ê–≥—Ä–µ–≥–∞—Ü–∏—è
rate({job="varlogs"}[5m])                     # –õ–æ–≥-–∑–∞–ø–∏—Å–µ–π –≤ —Å–µ–∫—É–Ω–¥—É
sum(rate({job="varlogs"}[5m])) by (level)     # –ü–æ —É—Ä–æ–≤–Ω—é
count_over_time({job="varlogs"}[1h])          # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞ —á–∞—Å

# Pattern extraction
{job="varlogs"} | pattern `<_> level=<level> <_>`
{job="varlogs"} | regexp `status=(?P<status>\d+)`

# –ú–µ—Ç—Ä–∏–∫–∏ –∏–∑ –ª–æ–≥–æ–≤
sum(rate({job="api"} | json | status="500" [5m]))
```

**Elasticsearch query patterns:**

json

```json
// –ë–∞–∑–æ–≤—ã–π –ø–æ–∏—Å–∫
GET /logs-*/_search
{
  "query": {
    "match": {
      "message": "error"
    }
  }
}

// –í—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏–∞–ø–∞–∑–æ–Ω
GET /logs-*/_search
{
  "query": {
    "range": {
      "@timestamp": {
        "gte": "now-1h",
        "lte": "now"
      }
    }
  }
}

// –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å
GET /logs-*/_search
{
  "query": {
    "bool": {
      "must": [
        { "match": { "level": "ERROR" }},
        { "match": { "service": "api" }}
      ],
      "filter": [
        { "range": { "@timestamp": { "gte": "now-1h" }}}
      ]
    }
  },
  "aggs": {
    "errors_by_service": {
      "terms": { "field": "service.keyword" }
    }
  }
}
```

**Fluentd/Fluent Bit basics:**

conf

````conf
# Fluentd –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (fluent.conf)
<source>
  @type tail
  path /var/log/nginx/access.log
  pos_file /var/log/td-agent/nginx-access.log.pos
  tag nginx.access
  <parse>
    @type nginx
  </parse>
</source>

<filter nginx.access>
  @type record_transformer
  <record>
    hostname "#{Socket.gethostname}"
    service "nginx"
  </record>
</filter>

<match nginx.access>
  @type elasticsearch
  host elasticsearch
  port 9200
  index_name nginx-access
  type_name _doc
</match>

# Fluent Bit –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (–±–æ–ª–µ–µ –ª–µ–≥–∫–æ–≤–µ—Å–Ω–∞—è –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞)
[INPUT]
    Name              tail
    Path              /var/log/containers/*.log
    Parser            docker
    Tag               kube.*

[FILTER]
    Name                kubernetes
    Match               kube.*
    Kube_URL            https://kubernetes.default.svc:443
    Kube_CA_File        /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
    Kube_Token_File     /var/run/secrets/kubernetes.io/serviceaccount/token

[OUTPUT]
    Name              loki
    Match             *
    Host              loki
    Port              3100
```

**Log best practices:**
```
1. –í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–π structured logging (JSON)
2. –í–∫–ª—é—á–∞–π –∫–æ–Ω—Ç–µ–∫—Å—Ç: request_id, user_id, trace_id
3. –õ–æ–≥–∏—Ä—É–π –Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —É—Ä–æ–≤–Ω–µ:
   - DEBUG: –¥–µ—Ç–∞–ª–∏ –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏
   - INFO: –Ω–æ—Ä–º–∞–ª—å–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏
   - WARN: –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã
   - ERROR: –æ—à–∏–±–∫–∏ —Ç—Ä–µ–±—É—é—â–∏–µ –≤–Ω–∏–º–∞–Ω–∏—è
4. –ù–µ –ª–æ–≥–∏—Ä—É–π sensitive data (–ø–∞—Ä–æ–ª–∏, —Ç–æ–∫–µ–Ω—ã, PII)
5. –ò—Å–ø–æ–ª—å–∑—É–π correlation IDs –¥–ª—è —Ç—Ä–µ–π—Å–∏–Ω–≥–∞
6. –†–æ—Ç–∏—Ä—É–π –ª–æ–≥–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
7. –¶–µ–Ω—Ç—Ä–∞–ª–∏–∑—É–π –ª–æ–≥–∏ —Å–æ –≤—Å–µ—Ö —Å–∏—Å—Ç–µ–º
8. –ù–∞—Å—Ç—Ä–æ–π –∞–ª–µ—Ä—Ç—ã –Ω–∞ –∫—Ä–∏—Ç–∏—á–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
````

### üíª –ó–∞–¥–∞–Ω–∏–µ

–ù–∞—Å—Ç—Ä–æ–π —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å Loki:

1. **–°–æ–∑–¥–∞–π docker-compose.yml –¥–ª—è Loki stack**:

yaml

```yaml
version: '3.8'

services:
  loki:
    image: grafana/loki:2.9.3
    container_name: loki
    ports:
      - "3100:3100"
    volumes:
      - ./loki-config.yml:/etc/loki/local-config.yaml
      - loki-data:/loki
    command: -config.file=/etc/loki/local-config.yaml
    restart: unless-stopped

  promtail:
    image: grafana/promtail:2.9.3
    container_name: promtail
    volumes:
      - ./promtail-config.yml:/etc/promtail/config.yml
      - /var/log:/var/log:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
    command: -config.file=/etc/promtail/config.yml
    restart: unless-stopped
    depends_on:
      - loki

  grafana:
    image: grafana/grafana:10.2.3
    container_name: grafana-logs
    ports:
      - "3001:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - grafana-logs-data:/var/lib/grafana
      - ./grafana-datasources.yml:/etc/grafana/provisioning/datasources/datasources.yml
    restart: unless-stopped
    depends_on:
      - loki

  # –¢–µ—Å—Ç–æ–≤–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∏—Ä—É—é—â–µ–µ –ª–æ–≥–∏
  log-generator:
    image: mingrammer/flog
    container_name: log-generator
    command: -f json -l -d 1 -s 1
    restart: unless-stopped

volumes:
  loki-data:
  grafana-logs-data:
```

2. **–°–æ–∑–¥–∞–π loki-config.yml**:

yaml

```yaml
auth_enabled: false

server:
  http_listen_port: 3100
  grpc_listen_port: 9096

common:
  path_prefix: /loki
  storage:
    filesystem:
      chunks_directory: /loki/chunks
      rules_directory: /loki/rules
  replication_factor: 1
  ring:
    instance_addr: 127.0.0.1
    kvstore:
      store: inmemory

query_range:
  results_cache:
    cache:
      embedded_cache:
        enabled: true
        max_size_mb: 100

schema_config:
  configs:
    - from: 2020-10-24
      store: boltdb-shipper
      object_store: filesystem
      schema: v11
      index:
        prefix: index_
        period: 24h

ruler:
  alertmanager_url: http://localhost:9093

# Retention (—É–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö –ª–æ–≥–æ–≤)
limits_config:
  retention_period: 168h  # 7 –¥–Ω–µ–π
```

3. **–°–æ–∑–¥–∞–π promtail-config.yml**:

yaml

```yaml
server:
  http_listen_port: 9080
  grpc_listen_port: 0

positions:
  filename: /tmp/positions.yaml

clients:
  - url: http://loki:3100/loki/api/v1/push

scrape_configs:
  # Docker –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã
  - job_name: docker
    docker_sd_configs:
      - host: unix:///var/run/docker.sock
        refresh_interval: 5s
    relabel_configs:
      - source_labels: ['__meta_docker_container_name']
        regex: '/(.*)'
        target_label: 'container'
      - source_labels: ['__meta_docker_container_log_stream']
        target_label: 'stream'
    pipeline_stages:
      - json:
          expressions:
            level: level
            message: message
            timestamp: timestamp
      - labels:
          level:
          stream:

  # –°–∏—Å—Ç–µ–º–Ω—ã–µ –ª–æ–≥–∏
  - job_name: system
    static_configs:
      - targets:
          - localhost
        labels:
          job: varlogs
          __path__: /var/log/*.log

  # Application logs (—Å –ø–∞—Ä—Å–∏–Ω–≥–æ–º JSON)
  - job_name: app-logs
    static_configs:
      - targets:
          - localhost
        labels:
          job: app
          __path__: /var/log/app/*.log
    pipeline_stages:
      - json:
          expressions:
            timestamp: timestamp
            level: level
            service: service
            message: message
            user_id: user_id
      - timestamp:
          source: timestamp
          format: RFC3339
      - labels:
          level:
          service:
```

4. **–°–æ–∑–¥–∞–π grafana-datasources.yml**:

yaml

```yaml
apiVersion: 1

datasources:
  - name: Loki
    type: loki
    access: proxy
    url: http://loki:3100
    isDefault: true
    editable: true
    jsonData:
      maxLines: 1000
```

5. **–ó–∞–ø—É—Å—Ç–∏ stack**:

bash

```bash
# –°–æ–∑–¥–∞–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
mkdir -p logs/app

# –ó–∞–ø—É—Å—Ç–∏
docker-compose up -d

# –ü—Ä–æ–≤–µ—Ä—å —Å—Ç–∞—Ç—É—Å
docker-compose ps
curl http://localhost:3100/ready

# –ü—Ä–æ–≤–µ—Ä—å –ª–æ–≥–∏
curl http://localhost:3100/loki/api/v1/label
```

6. **–°–æ–∑–¥–∞–π Python —Å–∫—Ä–∏–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ—Å—Ç–æ–≤—ã—Ö –ª–æ–≥–æ–≤** (`generate_logs.py`):

python

````python
#!/usr/bin/env python3
import json
import random
import time
from datetime import datetime

levels = ['DEBUG', 'INFO', 'WARN', 'ERROR']
services = ['api', 'frontend', 'database', 'cache']
messages = {
    'DEBUG': ['Query executed', 'Cache hit', 'Function called'],
    'INFO': ['User logged in', 'Request processed', 'Task completed'],
    'WARN': ['Slow query detected', 'High memory usage', 'Rate limit approaching'],
    'ERROR': ['Database connection failed', 'Timeout occurred', '500 Internal Server Error']
}

def generate_log():
    level = random.choices(levels, weights=[10, 60, 20, 10])[0]
    service = random.choice(services)
    message = random.choice(messages[level])
    
    log_entry = {
        "timestamp": datetime.utcnow().isoformat() + "Z",
        "level": level,
        "service": service,
        "message": message,
        "request_id": f"req-{random.randint(1000, 9999)}",
        "user_id": f"user-{random.randint(1, 100)}",
        "duration_ms": random.randint(10, 5000) if level in ['WARN', 'ERROR'] else random.randint(10, 500)
    }
    
    return json.dumps(log_entry)

if __name__ == "__main__":
    print("Starting log generation...")
    while True:
        log = generate_log()
        print(log)
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Ñ–∞–π–ª
        with open('/var/log/app/application.log', 'a') as f:
            f.write(log + '\n')
        time.sleep(random.uniform(0.1, 2))
```

7. **–û—Ç–∫—Ä–æ–π Grafana –∏ —Å–æ–∑–¥–∞–π dashboard**:
```
URL: http://localhost:3001
Login: admin
Password: admin

–ü—Ä–∏–º–µ—Ä—ã –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –ø–∞–Ω–µ–ª–µ–π:

# –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–æ–≥–æ–≤ –ø–æ —É—Ä–æ–≤–Ω—é
sum(rate({job="docker"}[1m])) by (level)

# –õ–æ–≥–∏ —Å –æ—à–∏–±–∫–∞–º–∏
{job="docker"} |= "ERROR"

# Top services –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –ª–æ–≥–æ–≤
topk(5, sum(rate({job="docker"}[5m])) by (container))

# –õ–æ–≥–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Å–µ—Ä–≤–∏—Å–∞
{job="docker", container="log-generator"}

# –ú–µ–¥–ª–µ–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã (–µ—Å–ª–∏ duration > 1000ms)
{job="docker"} | json | duration_ms > 1000
````

8. **–ü—Ä–æ–≤–µ—Ä—å —Ä–∞–±–æ—Ç—É**:

bash

```bash
# –õ–æ–≥–∏ –≤ Loki
curl -G -s "http://localhost:3100/loki/api/v1/query" \
  --data-urlencode 'query={job="docker"}' | jq

# –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–æ–≥–æ–≤
curl -G -s "http://localhost:3100/loki/api/v1/query" \
  --data-urlencode 'query=count_over_time({job="docker"}[1h])' | jq

# –ú–µ—Ç—Ä–∏–∫–∏ Promtail
curl http://localhost:9080/metrics
```

### üöÄ –ë–æ–Ω—É—Å (–Ω–æ–≤–æ–µ)

**1. –ù–∞—Å—Ç—Ä–æ–π ELK Stack –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è**:

`docker-compose-elk.yml`:

yaml

```yaml
version: '3.8'

services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.3
    container_name: elasticsearch
    environment:
      - discovery.type=single-node
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
      - xpack.security.enabled=false
    ports:
      - "9200:9200"
    volumes:
      - elasticsearch-data:/usr/share/elasticsearch/data
    restart: unless-stopped

  logstash:
    image: docker.elastic.co/logstash/logstash:8.11.3
    container_name: logstash
    volumes:
      - ./logstash.conf:/usr/share/logstash/pipeline/logstash.conf
    ports:
      - "5000:5000"
      - "9600:9600"
    environment:
      - "LS_JAVA_OPTS=-Xmx256m -Xms256m"
    depends_on:
      - elasticsearch
    restart: unless-stopped

  kibana:
    image: docker.elastic.co/kibana/kibana:8.11.3
    container_name: kibana
    ports:
      - "5601:5601"
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    depends_on:
      - elasticsearch
    restart: unless-stopped

  filebeat:
    image: docker.elastic.co/beats/filebeat:8.11.3
    container_name: filebeat
    user: root
    volumes:
      - ./filebeat.yml:/usr/share/filebeat/filebeat.yml:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
    command: filebeat -e -strict.perms=false
    depends_on:
      - elasticsearch
    restart: unless-stopped

volumes:
  elasticsearch-data:
```

`logstash.conf`:

conf

```conf
input {
  beats {
    port => 5000
  }
}

filter {
  if [message] =~ /^\{.*\}$/ {
    json {
      source => "message"
    }
  }
  
  date {
    match => ["timestamp", "ISO8601"]
    target => "@timestamp"
  }
  
  mutate {
    remove_field => ["message"]
  }
}

output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "logs-%{+YYYY.MM.dd}"
  }
  
  stdout {
    codec => rubydebug
  }
}
```

`filebeat.yml`:

yaml

```yaml
filebeat.inputs:
  - type: container
    paths:
      - '/var/lib/docker/containers/*/*.log'
    processors:
      - add_docker_metadata:
          host: "unix:///var/run/docker.sock"

output.logstash:
  hosts: ["logstash:5000"]

logging.level: info
```

**2. –°–æ–∑–¥–∞–π log alerting rules**:

–î–ª—è Loki (—á–µ—Ä–µ–∑ Grafana Alerting):

yaml

```yaml
# Alert: High Error Rate
groups:
  - name: log_alerts
    interval: 1m
    rules:
      - alert: HighErrorRate
        expr: |
          sum(rate({job="docker"} |= "ERROR" [5m])) > 10
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value }} errors/sec"

      - alert: ServiceDown
        expr: |
          absent(rate({job="docker", container="api"}[5m]))
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Service {{ $labels.container }} is down"
```

**3. –ù–∞—Å—Ç—Ä–æ–π log parsing –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤**:

Nginx access log parsing –≤ Promtail:

yaml

````yaml
- job_name: nginx
  static_configs:
    - targets:
        - localhost
      labels:
        job: nginx
        __path__: /var/log/nginx/access.log
  pipeline_stages:
    - regex:
        expression: '^(?P<remote_addr>[\w\.]+) - (?P<remote_user>[^ ]*) \[(?P<time_local>.*)\] "(?P<method>[^ ]*) (?P<request>[^ ]*) (?P<protocol>[^ ]*)" (?P<status>[\d]+) (?P<body_bytes_sent>[\d]+) "(?P<http_referer>[^"]*)" "(?P<http_user_agent>[^"]*)"'
    - labels:
        method:
        status:
    - timestamp:
        source: time_local
        format: 02/Jan/2006:15:04:05 -0700
```

**4. –°–æ–∑–¥–∞–π log analysis dashboard**:

Grafana panels –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ª–æ–≥–æ–≤:
```
Panel 1: Log volume over time
Query: sum(rate({job="docker"}[1m])) by (level)
Visualization: Time series

Panel 2: Top error messages
Query: topk(10, sum(rate({job="docker"} |= "ERROR" [5m])) by (message))
Visualization: Bar chart

Panel 3: Logs table
Query: {job="docker"}
Visualization: Logs

Panel 4: Response time distribution
Query: quantile_over_time(0.95, {job="docker"} | json | unwrap duration_ms [5m])
Visualization: Gauge

Panel 5: Service health
Query: count(rate({job="docker"}[1m])) by (container)
Visualization: Stat
````

**5. –ù–∞—Å—Ç—Ä–æ–π log sampling –¥–ª—è –≤—ã—Å–æ–∫–æ–Ω–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —Å–∏—Å—Ç–µ–º**:

yaml

```yaml
# Promtail sampling configuration
scrape_configs:
  - job_name: high-volume-app
    static_configs:
      - targets:
          - localhost
        labels:
          job: app
          __path__: /var/log/app/*.log
    pipeline_stages:
      # –°–æ—Ö—Ä–∞–Ω—è–π —Ç–æ–ª—å–∫–æ ERROR –∏ WARN + sample INFO/DEBUG
      - match:
          selector: '{job="app"}'
          stages:
            - json:
                expressions:
                  level: level
            - drop:
                expression: "level == 'DEBUG' and __sample__ > 0.1"  # 10% DEBUG
            - drop:
                expression: "level == 'INFO' and __sample__ > 0.5"   # 50% INFO
```

**6. Log retention –∏ archiving**:

yaml

```yaml
# Loki retention config
limits_config:
  retention_period: 168h  # 7 –¥–Ω–µ–π

# Compactor –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ —Å—Ç–∞—Ä—ã—Ö –ª–æ–≥–æ–≤
compactor:
  working_directory: /loki/compactor
  shared_store: filesystem
  compaction_interval: 10m
  retention_enabled: true
  retention_delete_delay: 2h
  retention_delete_worker_count: 150
```

**7. –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Alertmanager**:

yaml

```yaml
# Loki ruler config –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –∞–ª–µ—Ä—Ç–æ–≤
ruler:
  storage:
    type: local
    local:
      directory: /loki/rules
  rule_path: /tmp/rules
  alertmanager_url: http://alertmanager:9093
  ring:
    kvstore:
      store: inmemory
  enable_api: true
```

Rules file (`/loki/rules/alerts.yml`):

yaml

````yaml
groups:
  - name: logs
    interval: 1m
    rules:
      - alert: HighErrorRate
        expr: |
          sum(rate({job="docker"} |= "ERROR" [5m])) > 1
        for: 5m
        labels:
          severity: warning
          team: backend
        annotations:
          summary: "High error rate in {{ $labels.container }}"
          description: "Error rate: {{ $value }} errors/sec"
          dashboard: "http://grafana:3000/d/logs"
```

**8. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ Loki vs ELK**:
```
Loki –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:
‚úÖ –õ–µ–≥–∫–æ–≤–µ—Å–Ω—ã–π (–º–µ–Ω—å—à–µ —Ä–µ—Å—É—Ä—Å–æ–≤)
‚úÖ –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Prometheus/Grafana
‚úÖ –ü—Ä–æ—Å—Ç–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
‚úÖ –•–æ—Ä–æ—à–æ –¥–ª—è Kubernetes
‚úÖ –î–µ—à–µ–≤–ª–µ –≤ —ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏–∏

ELK –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:
‚úÖ –ú–æ—â–Ω—ã–π –ø–æ–ª–Ω–æ—Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫
‚úÖ –ë–æ–≥–∞—Ç—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
‚úÖ Advanced analytics
‚úÖ –ë–æ–ª—å—à–µ –ø–ª–∞–≥–∏–Ω–æ–≤ –∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–π
‚úÖ Mature ecosystem

–í—ã–±–æ—Ä:
- Loki: –¥–ª—è –º–µ—Ç—Ä–∏–∫-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞, K8s
- ELK: –¥–ª—è —Å–ª–æ–∂–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –ª–æ–≥–æ–≤, compliance
````

---

## –ò—Ç–æ–≥–∏ –º–æ–¥—É–ª—è 4

–ü–æ—Å–ª–µ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏—è —ç—Ç–æ–≥–æ –º–æ–¥—É–ª—è —Ç—ã –¥–æ–ª–∂–µ–Ω —É–º–µ—Ç—å:

‚úÖ –ü–æ–Ω–∏–º–∞—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã –∫ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—é ‚úÖ –ù–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å Loki + Promtail + Grafana ‚úÖ –ü–∏—Å–∞—Ç—å LogQL –∑–∞–ø—Ä–æ—Å—ã ‚úÖ –ü–∞—Ä—Å–∏—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –ª–æ–≥–æ–≤ ‚úÖ –°–æ–∑–¥–∞–≤–∞—Ç—å –¥–∞—à–±–æ—Ä–¥—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ª–æ–≥–æ–≤ ‚úÖ –ù–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å –∞–ª–µ—Ä—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –ª–æ–≥–æ–≤ ‚úÖ –£–ø—Ä–∞–≤–ª—è—Ç—å retention –∏ rotation ‚úÖ –°—Ä–∞–≤–Ω–∏–≤–∞—Ç—å Loki –∏ ELK —Å—Ç–µ–∫–∏


## –ú–æ–¥—É–ª—å 5: Alerting –∏ Notification - —É–º–Ω—ã–µ –∞–ª–µ—Ä—Ç—ã –±–µ–∑ alert fatigue (35 –º–∏–Ω—É—Ç)

### üéØ –ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞

**–§–∏–ª–æ—Å–æ—Ñ–∏—è –∞–ª–µ—Ä—Ç–∏–Ω–≥–∞:**

```
–•–æ—Ä–æ—à–∏–π –∞–ª–µ—Ä—Ç = Actionable + Urgent + Real Problem

‚ùå –ü–ª–æ—Ö–æ–π –∞–ª–µ—Ä—Ç: "CPU usage > 80%"
‚úÖ –•–æ—Ä–æ—à–∏–π –∞–ª–µ—Ä—Ç: "API response time > 1s for 5min, affecting users"

–ü—Ä–∞–≤–∏–ª–æ: –ï—Å–ª–∏ –∞–ª–µ—Ä—Ç –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –¥–µ–π—Å—Ç–≤–∏—è –ø—Ä—è–º–æ —Å–µ–π—á–∞—Å - —ç—Ç–æ –Ω–µ –∞–ª–µ—Ä—Ç, —ç—Ç–æ –º–µ—Ç—Ä–∏–∫–∞
```

**–£—Ä–æ–≤–Ω–∏ severity:**

```
CRITICAL (P1)  - –ü–æ–ª–Ω—ã–π outage, —Ç—Ä–µ–±—É–µ—Ç –Ω–µ–º–µ–¥–ª–µ–Ω–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π
                 –ü—Ä–∏–º–µ—Ä: —Å–µ—Ä–≤–∏—Å –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –ø–æ—Ç–µ—Ä—è –¥–∞–Ω–Ω—ã—Ö

WARNING (P2)   - –î–µ–≥—Ä–∞–¥–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–∞, —Ç—Ä–µ–±—É–µ—Ç –¥–µ–π—Å—Ç–≤–∏–π –≤ –±–ª–∏–∂–∞–π—à–µ–µ –≤—Ä–µ–º—è
                 –ü—Ä–∏–º–µ—Ä: –≤—ã—Å–æ–∫–∞—è latency, —Å–∫–æ—Ä–æ –∑–∞–∫–æ–Ω—á–∏—Ç—Å—è –º–µ—Å—Ç–æ

INFO (P3)      - –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–µ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ, –Ω–µ —Ç—Ä–µ–±—É–µ—Ç —Å—Ä–æ—á–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π
                 –ü—Ä–∏–º–µ—Ä: deployment –∑–∞–≤–µ—Ä—à–µ–Ω, –ø–ª–∞–Ω–æ–≤–æ–µ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ
```

**Alertmanager –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Prometheus  ‚îÇ‚îÄ‚îê
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
                ‚îú‚îÄ‚îÄ‚ñ∫ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ    ‚îÇ Alertmanager ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ Receivers   ‚îÇ
‚îÇ    Loki     ‚îÇ‚îÄ‚î§    ‚îÇ              ‚îÇ     ‚îÇ (Slack/etc) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ    ‚îÇ - Grouping   ‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚îÇ    ‚îÇ - Inhibition ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ    ‚îÇ - Silencing  ‚îÇ
‚îÇ   Custom    ‚îÇ‚îÄ‚îò    ‚îÇ - Routing    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Alert states:**

```
Inactive  ‚îÄ‚îÄ‚ñ∫ Pending  ‚îÄ‚îÄ‚ñ∫ Firing  ‚îÄ‚îÄ‚ñ∫ Resolved
               (for)         ‚îÇ
                            ‚Üì
                         Silenced
```

**–ö–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏:**

**1. Grouping** - –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø–æ—Ö–æ–∂–∏—Ö –∞–ª–µ—Ä—Ç–æ–≤:

yaml

```yaml
# –í–º–µ—Å—Ç–æ 100 –∞–ª–µ—Ä—Ç–æ–≤ –æ down –Ω–æ–¥–∞—Ö
# –û–¥–∏–Ω grouped –∞–ª–µ—Ä—Ç: "50 nodes are down in cluster-prod"
route:
  group_by: ['alertname', 'cluster']
  group_wait: 30s
  group_interval: 5m
```

**2. Inhibition** - –ø–æ–¥–∞–≤–ª–µ–Ω–∏–µ –∑–∞–≤–∏—Å–∏–º—ã—Ö –∞–ª–µ—Ä—Ç–æ–≤:

yaml

```yaml
# –ï—Å–ª–∏ –∫–ª–∞—Å—Ç–µ—Ä down, –Ω–µ —Å–ª–∞—Ç—å –∞–ª–µ—Ä—Ç—ã –æ –∫–∞–∂–¥–æ–º —Å–µ—Ä–≤–∏—Å–µ –≤ –Ω–µ–º
inhibit_rules:
  - source_match:
      alertname: ClusterDown
    target_match:
      cluster: production
    equal: ['cluster']
```

**3. Silencing** - –≤—Ä–µ–º–µ–Ω–Ω–æ–µ –æ—Ç–∫–ª—é—á–µ–Ω–∏–µ –∞–ª–µ—Ä—Ç–æ–≤:

bash

```bash
# –í–æ –≤—Ä–µ–º—è maintenance window
amtool silence add alertname=HighCPU --duration=2h --comment="Planned maintenance"
```

**4. Routing** - –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è –ø–æ –∫–æ–º–∞–Ω–¥–∞–º/–∫–∞–Ω–∞–ª–∞–º:

yaml

```yaml
route:
  routes:
    - match:
        team: backend
      receiver: backend-team
    - match:
        severity: critical
      receiver: pagerduty
```

**Prometheus alerting rules —Å—Ç—Ä—É–∫—Ç—É—Ä–∞:**

yaml

```yaml
groups:
  - name: example
    interval: 30s
    rules:
    - alert: HighErrorRate
      expr: |
        rate(http_requests_total{status=~"5.."}[5m]) 
        / 
        rate(http_requests_total[5m]) 
        > 0.05
      for: 5m
      labels:
        severity: warning
        team: backend
        service: api
      annotations:
        summary: "High error rate on {{ $labels.instance }}"
        description: "Error rate is {{ $value | humanizePercentage }}"
        dashboard: "https://grafana.com/d/api-dashboard"
        runbook: "https://wiki.com/runbooks/high-error-rate"
```

**Alert best practices:**

**1. –ù–∞–∑–≤–∞–Ω–∏–µ –∞–ª–µ—Ä—Ç–∞ (–≥–æ–≤–æ—Ä—è—â–µ–µ):**

yaml

```yaml
‚ùå alert: HighCPU
‚úÖ alert: InstanceHighCPUUsage

‚ùå alert: Error
‚úÖ alert: APIHighErrorRate5xx
```

**2. For clause (–∏–∑–±–µ–≥–∞–µ–º flapping):**

yaml

```yaml
# –ù–µ –∞–ª–µ—Ä—Ç–∏—Ç—å –Ω–∞ –∫—Ä–∞—Ç–∫–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Å–ø–∞–π–∫–∏
for: 5m  # –ê–ª–µ—Ä—Ç —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —É—Å–ª–æ–≤–∏–µ true 5 –º–∏–Ω—É—Ç –ø–æ–¥—Ä—è–¥
```

**3. –ê–Ω–Ω–æ—Ç–∞—Ü–∏–∏ (–ø–æ–ª–µ–∑–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç):**

yaml

```yaml
annotations:
  summary: "–ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º—ã"
  description: "{{ $labels.instance }} has {{ $value }}% CPU usage"
  dashboard: "–°—Å—ã–ª–∫–∞ –Ω–∞ dashboard"
  runbook: "–°—Å—ã–ª–∫–∞ –Ω–∞ runbook —Å —Ä–µ—à–µ–Ω–∏–µ–º"
  impact: "Users experiencing slow response times"
```

**4. Labels –¥–ª—è routing:**

yaml

```yaml
labels:
  severity: critical|warning|info
  team: backend|frontend|data
  service: api|web|worker
  environment: prod|staging|dev
```

**–¢–∏–ø–∏—á–Ω—ã–µ –∞–ª–µ—Ä—Ç—ã –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã:**

yaml

```yaml
# Instance down
- alert: InstanceDown
  expr: up == 0
  for: 5m
  labels:
    severity: critical
  annotations:
    summary: "Instance {{ $labels.instance }} down"

# High CPU
- alert: HighCPUUsage
  expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
  for: 10m
  labels:
    severity: warning

# High Memory
- alert: HighMemoryUsage
  expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 90
  for: 5m
  labels:
    severity: warning

# Disk space low
- alert: DiskSpaceLow
  expr: (1 - (node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"})) * 100 > 85
  for: 5m
  labels:
    severity: warning

# High disk I/O
- alert: HighDiskIO
  expr: rate(node_disk_io_time_seconds_total[5m]) > 0.9
  for: 10m
  labels:
    severity: warning
```

**–¢–∏–ø–∏—á–Ω—ã–µ –∞–ª–µ—Ä—Ç—ã –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π:**

yaml

````yaml
# High error rate
- alert: HighErrorRate
  expr: |
    sum(rate(http_requests_total{status=~"5.."}[5m])) by (service)
    /
    sum(rate(http_requests_total[5m])) by (service)
    > 0.05
  for: 5m
  labels:
    severity: critical

# Slow response time
- alert: SlowResponseTime
  expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 1
  for: 10m
  labels:
    severity: warning

# High request rate (DDoS?)
- alert: UnusuallyHighTraffic
  expr: sum(rate(http_requests_total[5m])) > 1000
  for: 5m
  labels:
    severity: warning

# Database connection pool exhausted
- alert: DatabaseConnectionPoolNearLimit
  expr: database_connections_active / database_connections_max > 0.9
  for: 5m
  labels:
    severity: warning

# Queue backed up
- alert: QueueBacklog
  expr: queue_depth > 1000
  for: 10m
  labels:
    severity: warning

# Certificate expiring soon
- alert: CertificateExpiringSoon
  expr: (ssl_certificate_expiry_timestamp - time()) / 86400 < 30
  for: 1h
  labels:
    severity: warning
```

**Alert fatigue - –∫–∞–∫ –∏–∑–±–µ–∂–∞—Ç—å:**
```
–ü—Ä–æ–±–ª–µ–º–∞: –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –∞–ª–µ—Ä—Ç–æ–≤ ‚Üí –∏–≥–Ω–æ—Ä–∏—Ä—É—é—Ç—Å—è ‚Üí –ø—Ä–æ–ø—É—â–µ–Ω—ã —Ä–µ–∞–ª—å–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã

–†–µ—à–µ–Ω–∏—è:
1. ‚úÖ –ê–ª–µ—Ä—Ç–∏—Ç—å —Ç–æ–ª—å–∫–æ –Ω–∞ —Å–∏–º–ø—Ç–æ–º—ã, –∞ –Ω–µ –ø—Ä–∏—á–∏–Ω—ã
   ‚ùå CPU high, Memory high, Disk full (–ø—Ä–∏—á–∏–Ω—ã)
   ‚úÖ Users can't login, API is slow (—Å–∏–º–ø—Ç–æ–º—ã)

2. ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–π –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ threshold
   ‚ùå CPU > 50% (—Å–ª–∏—à–∫–æ–º —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ)
   ‚úÖ CPU > 80% for 10 minutes (—Ä–∞–∑—É–º–Ω–æ)

3. ‚úÖ –ì—Ä—É–ø–ø–∏—Ä—É–π –ø–æ—Ö–æ–∂–∏–µ –∞–ª–µ—Ä—Ç—ã
   ‚ùå 50 –∞–ª–µ—Ä—Ç–æ–≤ "pod X down"
   ‚úÖ 1 –∞–ª–µ—Ä—Ç "50 pods down in namespace Y"

4. ‚úÖ Inhibition rules –¥–ª—è –∑–∞–≤–∏—Å–∏–º—ã—Ö –∞–ª–µ—Ä—Ç–æ–≤
   –ï—Å–ª–∏ –∫–ª–∞—Å—Ç–µ—Ä down ‚Üí –Ω–µ —Å–ª–∞—Ç—å –∞–ª–µ—Ä—Ç—ã –æ —Å–µ—Ä–≤–∏—Å–∞—Ö

5. ‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω–æ–µ –≤—Ä–µ–º—è —Å—É—Ç–æ–∫
   Non-critical –∞–ª–µ—Ä—Ç—ã —Ç–æ–ª—å–∫–æ –≤ —Ä–∞–±–æ—á–µ–µ –≤—Ä–µ–º—è

6. ‚úÖ SLO-based alerting
   –ê–ª–µ—Ä—Ç–∏—Ç—å –∫–æ–≥–¥–∞ error budget –∏—Å—á–µ—Ä–ø—ã–≤–∞–µ—Ç—Å—è

7. ‚úÖ –†–µ–≥—É–ª—è—Ä–Ω—ã–π review –∏ cleanup
   –£–¥–∞–ª—è–π –Ω–µ–∞–∫—Ç—É–∞–ª—å–Ω—ã–µ –∞–ª–µ—Ä—Ç—ã
```

**Notification channels:**
```
–ö—Ä–∏—Ç–∏—á–Ω–æ—Å—Ç—å    –ö–∞–Ω–∞–ª           –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
Critical       PagerDuty       Production outage, —Ç—Ä–µ–±—É–µ—Ç –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è
               OpsGenie        
               
Warning        Slack           –¢—Ä–µ–±—É–µ—Ç –≤–Ω–∏–º–∞–Ω–∏—è, –Ω–æ –Ω–µ —Å—Ä–æ—á–Ω–æ
               Teams           
               
Info           Email           FYI, —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞, –æ—Ç—á–µ—Ç—ã
               Webhook         –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –¥—Ä—É–≥–∏–º–∏ —Å–∏—Å—Ç–µ–º–∞–º–∏
               
–í—Å–µ —É—Ä–æ–≤–Ω–∏     Grafana         –î–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ –∞–Ω–∞–ª–∏–∑–∞
````

**Alertmanager –∫–æ–º–∞–Ω–¥—ã:**

bash

```bash
# –°—Ç–∞—Ç—É—Å
amtool config show
amtool config routes
amtool alert query

# Silences
amtool silence add alertname=HighCPU --duration=2h --comment="Maintenance"
amtool silence query
amtool silence expire <silence-id>

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–Ω—Ñ–∏–≥–∞
amtool check-config alertmanager.yml

# –û—Ç–ø—Ä–∞–≤–∫–∞ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –∞–ª–µ—Ä—Ç–∞
amtool alert add alertname=Test severity=warning

# API –∑–∞–ø—Ä–æ—Å—ã
curl -X GET http://localhost:9093/api/v2/alerts
curl -X GET http://localhost:9093/api/v2/silences
curl -X GET http://localhost:9093/api/v2/status
```

### üíª –ó–∞–¥–∞–Ω–∏–µ

–ù–∞—Å—Ç—Ä–æ–π –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É –∞–ª–µ—Ä—Ç–∏–Ω–≥–∞:

1. **–î–æ–±–∞–≤—å Alertmanager –≤ docker-compose.yml**:

yaml

```yaml
version: '3.8'

services:
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - ./alerts.yml:/etc/prometheus/alerts.yml
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.enable-lifecycle'
    restart: unless-stopped

  alertmanager:
    image: prom/alertmanager:latest
    container_name: alertmanager
    ports:
      - "9093:9093"
    volumes:
      - ./alertmanager.yml:/etc/alertmanager/alertmanager.yml
      - alertmanager-data:/alertmanager
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
    restart: unless-stopped

  node-exporter:
    image: prom/node-exporter:latest
    container_name: node-exporter
    ports:
      - "9100:9100"
    command:
      - '--path.rootfs=/host'
    pid: host
    restart: unless-stopped
    volumes:
      - '/:/host:ro,rslave'

  # Webhook receiver –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
  webhook-receiver:
    image: ghcr.io/tarampampam/webhook-tester:latest
    container_name: webhook-receiver
    ports:
      - "8080:8080"
    restart: unless-stopped

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_UNIFIED_ALERTING_ENABLED=true
    volumes:
      - grafana-data:/var/lib/grafana
      - ./grafana-datasources.yml:/etc/grafana/provisioning/datasources/datasources.yml
    restart: unless-stopped
    depends_on:
      - prometheus

volumes:
  prometheus-data:
  alertmanager-data:
  grafana-data:
```

2. **–°–æ–∑–¥–∞–π prometheus.yml —Å –∞–ª–µ—Ä—Ç–∏–Ω–≥–æ–º**:

yaml

```yaml
global:
  scrape_interval: 15s
  evaluation_interval: 15s
  external_labels:
    cluster: 'local'
    environment: 'dev'

# Alertmanager configuration
alerting:
  alertmanagers:
    - static_configs:
        - targets:
            - alertmanager:9093

# Load rules
rule_files:
  - "alerts.yml"

scrape_configs:
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  - job_name: 'node-exporter'
    static_configs:
      - targets: ['node-exporter:9100']

  - job_name: 'alertmanager'
    static_configs:
      - targets: ['alertmanager:9093']
```

3. **–°–æ–∑–¥–∞–π alerts.yml —Å –ø—Ä–∞–≤–∏–ª–∞–º–∏**:

yaml

```yaml
groups:
  - name: infrastructure
    interval: 30s
    rules:
    # Instance down
    - alert: InstanceDown
      expr: up == 0
      for: 2m
      labels:
        severity: critical
        team: infrastructure
      annotations:
        summary: "Instance {{ $labels.instance }} is down"
        description: "{{ $labels.job }} on {{ $labels.instance }} has been down for more than 2 minutes."
        dashboard: "http://localhost:3000/d/node-exporter"
        runbook: "https://runbooks.example.com/InstanceDown"

    # High CPU
    - alert: HighCPUUsage
      expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
      for: 5m
      labels:
        severity: warning
        team: infrastructure
      annotations:
        summary: "High CPU usage on {{ $labels.instance }}"
        description: "CPU usage is {{ $value | humanize }}% on {{ $labels.instance }}"
        dashboard: "http://localhost:3000/d/node-exporter"

    # High Memory
    - alert: HighMemoryUsage
      expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 85
      for: 5m
      labels:
        severity: warning
        team: infrastructure
      annotations:
        summary: "High memory usage on {{ $labels.instance }}"
        description: "Memory usage is {{ $value | humanize }}% on {{ $labels.instance }}"

    # Disk space critical
    - alert: DiskSpaceCritical
      expr: (1 - (node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"})) * 100 > 90
      for: 5m
      labels:
        severity: critical
        team: infrastructure
      annotations:
        summary: "Critical disk space on {{ $labels.instance }}"
        description: "Disk usage is {{ $value | humanize }}% on {{ $labels.instance }}"
        impact: "System may become unresponsive if disk fills up"

    # Disk space warning
    - alert: DiskSpaceWarning
      expr: (1 - (node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"})) * 100 > 80
      for: 10m
      labels:
        severity: warning
        team: infrastructure
      annotations:
        summary: "Low disk space on {{ $labels.instance }}"
        description: "Disk usage is {{ $value | humanize }}% on {{ $labels.instance }}"

  - name: alertmanager
    interval: 30s
    rules:
    # Alertmanager down
    - alert: AlertmanagerDown
      expr: up{job="alertmanager"} == 0
      for: 2m
      labels:
        severity: critical
        team: monitoring
      annotations:
        summary: "Alertmanager is down"
        description: "Alertmanager has been down for more than 2 minutes. Alerts may not be delivered!"

    # Too many alerts firing
    - alert: TooManyAlerts
      expr: count(ALERTS{alertstate="firing"}) > 10
      for: 5m
      labels:
        severity: warning
        team: monitoring
      annotations:
        summary: "Too many alerts firing"
        description: "There are {{ $value }} alerts currently firing. This may indicate a systemic issue."

  - name: prometheus
    interval: 30s
    rules:
    # Prometheus target missing
    - alert: PrometheusTargetMissing
      expr: up == 0
      for: 2m
      labels:
        severity: critical
        team: monitoring
      annotations:
        summary: "Prometheus target missing"
        description: "A Prometheus target has disappeared. Instance: {{ $labels.instance }}"

    # Prometheus config reload failed
    - alert: PrometheusConfigReloadFailed
      expr: prometheus_config_last_reload_successful == 0
      for: 5m
      labels:
        severity: critical
        team: monitoring
      annotations:
        summary: "Prometheus config reload failed"
        description: "Prometheus config reload has failed on {{ $labels.instance }}"

  - name: deadman
    interval: 30s
    rules:
    # Deadman switch - –∞–ª–µ—Ä—Ç –∫–æ—Ç–æ—Ä—ã–π –≤—Å–µ–≥–¥–∞ –¥–æ–ª–∂–µ–Ω firing
    - alert: DeadMansSwitch
      expr: vector(1)
      labels:
        severity: info
        team: monitoring
      annotations:
        summary: "Monitoring system is alive"
        description: "This is a deadman switch. It should always be firing. If you don't receive this, monitoring is broken."
```

4. **–°–æ–∑–¥–∞–π alertmanager.yml —Å routing –∏ receivers**:

yaml

```yaml
global:
  resolve_timeout: 5m
  # Slack (—Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π –∏ –Ω–∞—Å—Ç—Ä–æ–π –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏)
  # slack_api_url: 'https://hooks.slack.com/services/YOUR/WEBHOOK/URL'

# Templates –¥–ª—è –∫—Ä–∞—Å–∏–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
templates:
  - '/etc/alertmanager/templates/*.tmpl'

# Route tree
route:
  receiver: 'default'
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 10s
  group_interval: 5m
  repeat_interval: 4h
  
  routes:
    # Critical –∞–ª–µ—Ä—Ç—ã ‚Üí webhook + log
    - match:
        severity: critical
      receiver: critical-alerts
      group_wait: 10s
      repeat_interval: 1h
      continue: true

    # Infrastructure team
    - match:
        team: infrastructure
      receiver: infrastructure-team
      group_wait: 30s
      repeat_interval: 4h

    # Monitoring team
    - match:
        team: monitoring
      receiver: monitoring-team

    # Deadman switch (–¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —á—Ç–æ alerting —Ä–∞–±–æ—Ç–∞–µ—Ç)
    - match:
        alertname: DeadMansSwitch
      receiver: deadman
      repeat_interval: 5m

# Inhibition rules (–ø–æ–¥–∞–≤–ª–µ–Ω–∏–µ –∑–∞–≤–∏—Å–∏–º—ã—Ö –∞–ª–µ—Ä—Ç–æ–≤)
inhibit_rules:
  # –ï—Å–ª–∏ instance down, –Ω–µ —Å–ª–∞—Ç—å –¥—Ä—É–≥–∏–µ –∞–ª–µ—Ä—Ç—ã —Å —Ç–æ–≥–æ –∂–µ instance
  - source_match:
      severity: critical
      alertname: InstanceDown
    target_match:
      severity: warning
    equal: ['instance']

  # –ï—Å–ª–∏ –¥–∏—Å–∫ –∫—Ä–∏—Ç–∏—á–µ–Ω, –Ω–µ —Å–ª–∞—Ç—å warning –æ –¥–∏—Å–∫–µ
  - source_match:
      alertname: DiskSpaceCritical
    target_match:
      alertname: DiskSpaceWarning
    equal: ['instance', 'mountpoint']

# Receivers (–∫–∞–Ω–∞–ª—ã —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π)
receivers:
  - name: 'default'
    webhook_configs:
      - url: 'http://webhook-receiver:8080/webhook/default'
        send_resolved: true

  - name: 'critical-alerts'
    webhook_configs:
      - url: 'http://webhook-receiver:8080/webhook/critical'
        send_resolved: true
    # Uncomment for Slack
    # slack_configs:
    #   - channel: '#alerts-critical'
    #     title: 'üö® CRITICAL ALERT'
    #     text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
    #     send_resolved: true

  - name: 'infrastructure-team'
    webhook_configs:
      - url: 'http://webhook-receiver:8080/webhook/infrastructure'
        send_resolved: true
    # Uncomment for Slack
    # slack_configs:
    #   - channel: '#team-infrastructure'
    #     title: '‚ö†Ô∏è Infrastructure Alert'
    #     text: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'

  - name: 'monitoring-team'
    webhook_configs:
      - url: 'http://webhook-receiver:8080/webhook/monitoring'
        send_resolved: true

  - name: 'deadman'
    webhook_configs:
      - url: 'http://webhook-receiver:8080/webhook/deadman'
        send_resolved: false
```

5. **–°–æ–∑–¥–∞–π grafana-datasources.yml**:

yaml

```yaml
apiVersion: 1

datasources:
  - name: Prometheus
    type: prometheus
    access: proxy
    url: http://prometheus:9090
    isDefault: true
    editable: true
    jsonData:
      httpMethod: POST
      
  - name: Alertmanager
    type: alertmanager
    access: proxy
    url: http://alertmanager:9093
    editable: true
    jsonData:
      implementation: prometheus
```

6. **–ó–∞–ø—É—Å—Ç–∏ –∏ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–π**:

bash

```bash
# –ó–∞–ø—É—Å–∫
docker-compose up -d

# –ü—Ä–æ–≤–µ—Ä–∫–∞ Prometheus
curl http://localhost:9090/api/v1/rules

# –ü—Ä–æ–≤–µ—Ä–∫–∞ Alertmanager
curl http://localhost:9093/api/v2/status

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –∞–ª–µ—Ä—Ç–æ–≤ –≤ Prometheus
curl http://localhost:9090/api/v1/alerts | jq

# –°–ø–∏—Å–æ–∫ firing –∞–ª–µ—Ä—Ç–æ–≤
curl http://localhost:9093/api/v2/alerts | jq '.[] | select(.status.state == "active")'
```

7. **–°–æ–∑–¥–∞–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ—Å—Ç–æ–≤–æ–π –Ω–∞–≥—Ä—É–∑–∫–∏** (`stress_test.sh`):

bash

```bash
#!/bin/bash

echo "Starting stress test to trigger alerts..."

# CPU stress (—Ç—Ä–∏–≥–≥–µ—Ä–Ω–µ—Ç HighCPUUsage)
echo "Generating CPU load..."
docker run --rm --name cpu-stress \
  polinux/stress \
  stress --cpu 4 --timeout 300s &

# –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –¥–∏—Å–∫–∞ (–¥–ª—è DiskSpaceWarning)
# –í–ù–ò–ú–ê–ù–ò–ï: –ë—É–¥—å –æ—Å—Ç–æ—Ä–æ–∂–µ–Ω —Å —ç—Ç–∏–º –Ω–∞ –ø—Ä–æ–¥–µ!
# echo "Filling disk space..."
# dd if=/dev/zero of=/tmp/largefile bs=1M count=10000

echo "Stress test running. Check alerts in:"
echo "- Prometheus: http://localhost:9090/alerts"
echo "- Alertmanager: http://localhost:9093"
echo "- Webhook receiver: http://localhost:8080"
echo ""
echo "Wait 5-10 minutes for alerts to fire..."
```

8. **–ü—Ä–æ–≤–µ—Ä—å UI –∏ –∞–ª–µ—Ä—Ç—ã**:

bash

```bash
# Prometheus Alerts UI
open http://localhost:9090/alerts

# Alertmanager UI
open http://localhost:9093

# Grafana Alerting
open http://localhost:3000/alerting/list

# Webhook receiver (–ø—Ä–æ–≤–µ—Ä—å –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –∞–ª–µ—Ä—Ç—ã)
open http://localhost:8080
```

9. **–ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–π silencing**:

bash

```bash
# –£—Å—Ç–∞–Ω–æ–≤–∏ amtool
go install github.com/prometheus/alertmanager/cmd/amtool@latest
# –∏–ª–∏
brew install amtool

# –ù–∞—Å—Ç—Ä–æ–π amtool
cat > ~/.config/amtool/config.yml <<EOF
alertmanager.url: http://localhost:9093
EOF

# –°–æ–∑–¥–∞–π silence –Ω–∞ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∞
amtool silence add \
  alertname=HighCPUUsage \
  --duration=1h \
  --comment="Testing alert system" \
  --author="devops@example.com"

# –ü—Ä–æ–≤–µ—Ä—å silences
amtool silence query

# –£–¥–∞–ª–∏ silence
amtool silence expire <silence-id>
```

### üöÄ –ë–æ–Ω—É—Å (–Ω–æ–≤–æ–µ)

**1. –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å–æ Slack**:

–û–±–Ω–æ–≤–∏ `alertmanager.yml`:

yaml

```yaml
global:
  slack_api_url: 'https://hooks.slack.com/services/YOUR/WEBHOOK/URL'

receivers:
  - name: 'slack-critical'
    slack_configs:
      - channel: '#alerts-critical'
        username: 'Alertmanager'
        icon_emoji: ':fire:'
        title: 'üö® {{ .GroupLabels.alertname }}'
        text: |
          {{ range .Alerts }}
          *Alert:* {{ .Labels.alertname }}
          *Severity:* {{ .Labels.severity }}
          *Instance:* {{ .Labels.instance }}
          *Description:* {{ .Annotations.description }}
          *Dashboard:* {{ .Annotations.dashboard }}
          {{ end }}
        send_resolved: true
        color: '{{ if eq .Status "firing" }}danger{{ else }}good{{ end }}'
```

**2. Custom notification template**:

–°–æ–∑–¥–∞–π `templates/slack.tmpl`:

gotmpl

```gotmpl
{{ define "slack.title" }}
[{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ .GroupLabels.alertname }}
{{ end }}

{{ define "slack.text" }}
{{ range .Alerts }}
*Alert:* {{ .Labels.alertname }} - `{{ .Labels.severity }}`
*Instance:* {{ .Labels.instance }}
*Summary:* {{ .Annotations.summary }}
*Description:* {{ .Annotations.description }}
{{ if .Annotations.runbook }}*Runbook:* {{ .Annotations.runbook }}{{ end }}
{{ if .Annotations.dashboard }}*Dashboard:* {{ .Annotations.dashboard }}{{ end }}
*Started:* {{ .StartsAt.Format "2006-01-02 15:04:05 MST" }}
{{ if .EndsAt }}*Ended:* {{ .EndsAt.Format "2006-01-02 15:04:05 MST" }}{{ end }}
{{ end }}
{{ end }}

{{ define "slack.color" }}
{{ if eq .Status "firing" }}
  {{ if eq .CommonLabels.severity "critical" }}danger{{ else }}warning{{ end }}
{{ else }}
good
{{ end }}
{{ end }}
```

**3. PagerDuty –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è** (–¥–ª—è critical alerts):

yaml

```yaml
receivers:
  - name: 'pagerduty-critical'
    pagerduty_configs:
      - service_key: 'YOUR_PAGERDUTY_SERVICE_KEY'
        description: '{{ .GroupLabels.alertname }}: {{ .Annotations.summary }}'
        severity: '{{ .CommonLabels.severity }}'
        details:
          firing: '{{ .Alerts.Firing | len }}'
          resolved: '{{ .Alerts.Resolved | len }}'
          instance: '{{ .CommonLabels.instance }}'
        client: 'Alertmanager'
        client_url: 'http://alertmanager:9093'
        send_resolved: true
```

**4. Email notifications —Å HTML template**:

yaml

```yaml
receivers:
  - name: 'email-team'
    email_configs:
      - to: 'team@example.com'
        from: 'alertmanager@example.com'
        smarthost: 'smtp.gmail.com:587'
        auth_username: 'alertmanager@example.com'
        auth_password: 'your-app-password'
        headers:
          Subject: '{{ if eq .Status "firing" }}üö®{{ else }}‚úÖ{{ end }} [{{ .Status | toUpper }}] {{ .GroupLabels.alertname }}'
        html: |
          <!DOCTYPE html>
          <html>
          <body>
            <h2 style="color: {{ if eq .Status "firing" }}#d9534f{{ else }}#5cb85c{{ end }}">
              {{ if eq .Status "firing" }}üö® Firing Alerts{{ else }}‚úÖ Resolved{{ end }}
            </h2>
            {{ range .Alerts }}
            <div style="border-left: 4px solid {{ if eq .Status "firing" }}#d9534f{{ else }}#5cb85c{{ end }}; padding: 10px; margin: 10px 0;">
              <h3>{{ .Labels.alertname }}</h3>
              <p><strong>Severity:</strong> {{ .Labels.severity }}</p>
              <p><strong>Instance:</strong> {{ .Labels.instance }}</p>
              <p><strong>Description:</strong> {{ .Annotations.description }}</p>
              {{ if .Annotations.runbook }}
              <p><a href="{{ .Annotations.runbook }}">üìñ Runbook</a></p>
              {{ end }}
              {{ if .Annotations.dashboard }}
              <p><a href="{{ .Annotations.dashboard }}">üìä Dashboard</a></p>
              {{ end }}
            </div>
            {{ end }}
          </body>
          </html>
        send_resolved: true
```

**5. Webhook –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å Jira/ServiceNow**:

–°–æ–∑–¥–∞–π `webhook_handler.py`:

python

```python
#!/usr/bin/env python3
from flask import Flask, request, jsonify
import json
import requests

app = Flask(__name__)

@app.route('/webhook/jira', methods=['POST'])
def jira_webhook():
    """–°–æ–∑–¥–∞–µ—Ç Jira ticket –¥–ª—è –∫—Ä–∏—Ç–∏—á–Ω—ã—Ö –∞–ª–µ—Ä—Ç–æ–≤"""
    data = request.json
    
    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ firing –∏ critical
    if data['status'] == 'firing':
        for alert in data['alerts']:
            if alert['labels'].get('severity') == 'critical':
                create_jira_ticket(alert)
    
    return jsonify({'status': 'ok'}), 200

def create_jira_ticket(alert):
    """–°–æ–∑–¥–∞–µ—Ç Jira ticket —á–µ—Ä–µ–∑ API"""
    jira_url = "https://your-jira.atlassian.net/rest/api/2/issue"
    
    ticket = {
        "fields": {
            "project": {"key": "OPS"},
            "summary": f"[ALERT] {alert['labels']['alertname']}",
            "description": alert['annotations']['description'],
            "issuetype": {"name": "Incident"}, "priority": {"name": "Critical"}, "labels": ["alert", "monitoring"] } }
```
# –û—Ç–ø—Ä–∞–≤–∫–∞ –≤ Jira
response = requests.post(
    jira_url,
    json=ticket,
    auth=('user@example.com', 'jira-api-token'),
    headers={'Content-Type': 'application/json'}
)

print(f"Jira ticket created: {response.json().get('key')}")
```

if **name** == '**main**': app.run(host='0.0.0.0', port=5000)

````

**6. SLO-based alerting** (–ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø–æ–¥—Ö–æ–¥):
```yaml
groups:
  - name: slo_alerts
    interval: 30s
    rules:
    # Error budget burn rate
    - alert: ErrorBudgetBurnRateTooHigh
      expr: |
        (
          sum(rate(http_requests_total{status=~"5.."}[1h]))
          /
          sum(rate(http_requests_total[1h]))
        ) > (1 - 0.999) * 10  # 10x SLO burn rate
      for: 5m
      labels:
        severity: critical
        team: sre
      annotations:
        summary: "Error budget burning too fast"
        description: "Current error rate is {{ $value | humanizePercentage }}. At this rate, monthly error budget will be exhausted in {{ with printf \"(1-0.999)*730/%f\" $value }}{{ . }}{{ end }} hours."
        dashboard: "http://localhost:3000/d/slo-dashboard"

    # SLO violation
    - alert: SLOViolation
      expr: |
        (
          1 - (
            sum(rate(http_requests_total{status!~"5.."}[30d]))
            /
            sum(rate(http_requests_total[30d]))
          )
        ) > 0.001  # –ù–∞—Ä—É—à–µ–Ω–∏–µ 99.9% SLO
      for: 1h
      labels:
        severity: warning
        team: sre
      annotations:
        summary: "SLO violation detected"
        description: "30-day error rate is {{ $value | humanizePercentage }}, violating 99.9% SLO"
```

**7. Multi-window multi-burn-rate alerts** (Google SRE –ø–æ–¥—Ö–æ–¥):
```yaml
groups:
  - name: multiwindow_multiburn_alerts
    interval: 30s
    rules:
    # Fast burn (–Ω—É–∂–Ω–æ –¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ)
    - alert: ErrorBudgetFastBurn
      expr: |
        (
          sum(rate(http_requests_total{status=~"5.."}[5m]))
          /
          sum(rate(http_requests_total[5m]))
        ) > 14.4 * (1 - 0.999)  # 14.4x burn rate
        and
        (
          sum(rate(http_requests_total{status=~"5.."}[1h]))
          /
          sum(rate(http_requests_total[1h]))
        ) > 14.4 * (1 - 0.999)
      for: 2m
      labels:
        severity: critical
        burn_rate: fast
      annotations:
        summary: "Fast error budget burn"
        description: "Error budget will be exhausted in 2 hours at current rate"

    # Slow burn (—Ç—Ä–µ–±—É–µ—Ç –≤–Ω–∏–º–∞–Ω–∏—è –≤ –±–ª–∏–∂–∞–π—à–µ–µ –≤—Ä–µ–º—è)
    - alert: ErrorBudgetSlowBurn
      expr: |
        (
          sum(rate(http_requests_total{status=~"5.."}[30m]))
          /
          sum(rate(http_requests_total[30m]))
        ) > 6 * (1 - 0.999)  # 6x burn rate
        and
        (
          sum(rate(http_requests_total{status=~"5.."}[6h]))
          /
          sum(rate(http_requests_total[6h]))
        ) > 6 * (1 - 0.999)
      for: 15m
      labels:
        severity: warning
        burn_rate: slow
      annotations:
        summary: "Slow error budget burn"
        description: "Error budget will be exhausted in 5 days at current rate"
```

**8. Alert aggregation dashboard**:

–°–æ–∑–¥–∞–π Python —Å–∫—Ä–∏–ø—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∞–ª–µ—Ä—Ç–æ–≤ (`alert_analysis.py`):
```python
#!/usr/bin/env python3
import requests
from collections import Counter
from datetime import datetime, timedelta

ALERTMANAGER_URL = "http://localhost:9093"

def get_alerts():
    """–ü–æ–ª—É—á–∏—Ç—å –≤—Å–µ –∞–ª–µ—Ä—Ç—ã –∏–∑ Alertmanager"""
    response = requests.get(f"{ALERTMANAGER_URL}/api/v2/alerts")
    return response.json()

def analyze_alerts():
    """–ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –∞–ª–µ—Ä—Ç–æ–≤"""
    alerts = get_alerts()
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    total_alerts = len(alerts)
    firing_alerts = [a for a in alerts if a['status']['state'] == 'active']
    
    # –ü–æ severity
    severity_counter = Counter(
        alert['labels'].get('severity', 'unknown') 
        for alert in firing_alerts
    )
    
    # –ü–æ team
    team_counter = Counter(
        alert['labels'].get('team', 'unknown') 
        for alert in firing_alerts
    )
    
    # –°–∞–º—ã–µ —á–∞—Å—Ç—ã–µ –∞–ª–µ—Ä—Ç—ã
    alert_counter = Counter(
        alert['labels']['alertname'] 
        for alert in firing_alerts
    )
    
    # –í—ã–≤–æ–¥ –æ—Ç—á–µ—Ç–∞
    print("=" * 60)
    print("ALERT ANALYSIS REPORT")
    print("=" * 60)
    print(f"Total alerts: {total_alerts}")
    print(f"Firing alerts: {len(firing_alerts)}")
    print()
    
    print("By Severity:")
    for severity, count in severity_counter.most_common():
        print(f"  {severity}: {count}")
    print()
    
    print("By Team:")
    for team, count in team_counter.most_common():
        print(f"  {team}: {count}")
    print()
    
    print("Top 5 Most Frequent Alerts:")
    for alertname, count in alert_counter.most_common(5):
        print(f"  {alertname}: {count}")
    print("=" * 60)

if __name__ == "__main__":
    analyze_alerts()
```

**9. Alert testing framework**:

–°–æ–∑–¥–∞–π `alert_test.py`:
```python
#!/usr/bin/env python3
"""
–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–ª–µ—Ä—Ç–æ–≤ - –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º
—á—Ç–æ –∞–ª–µ—Ä—Ç—ã —Å—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç
"""
import requests
import time
from prometheus_client import CollectorRegistry, Gauge, push_to_gateway

def test_high_cpu_alert():
    """–¢–µ—Å—Ç –∞–ª–µ—Ä—Ç–∞ HighCPUUsage"""
    print("Testing HighCPUUsage alert...")
    
    registry = CollectorRegistry()
    cpu_gauge = Gauge('node_cpu_seconds_total', 
                      'CPU time', 
                      ['mode', 'instance'], 
                      registry=registry)
    
    # –°–∏–º—É–ª–∏—Ä—É–µ–º –≤—ã—Å–æ–∫—É—é CPU –Ω–∞–≥—Ä—É–∑–∫—É
    cpu_gauge.labels(mode='idle', instance='test-instance').set(0.1)
    cpu_gauge.labels(mode='user', instance='test-instance').set(0.8)
    
    # Push –≤ Pushgateway
    push_to_gateway('localhost:9091', job='test', registry=registry)
    
    print("Metrics pushed. Wait 5 minutes and check alerts...")
    print("http://localhost:9090/alerts")

def test_disk_space_alert():
    """–¢–µ—Å—Ç –∞–ª–µ—Ä—Ç–∞ DiskSpaceCritical"""
    print("Testing DiskSpaceCritical alert...")
    
    registry = CollectorRegistry()
    disk_total = Gauge('node_filesystem_size_bytes',
                       'Filesystem size',
                       ['mountpoint', 'instance'],
                       registry=registry)
    disk_avail = Gauge('node_filesystem_avail_bytes',
                       'Available space',
                       ['mountpoint', 'instance'],
                       registry=registry)
    
    # –°–∏–º—É–ª–∏—Ä—É–µ–º 95% –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –¥–∏—Å–∫–∞
    disk_total.labels(mountpoint='/', instance='test-instance').set(100e9)  # 100GB
    disk_avail.labels(mountpoint='/', instance='test-instance').set(5e9)    # 5GB
    
    push_to_gateway('localhost:9091', job='test', registry=registry)
    
    print("Metrics pushed. Check alerts...")

if __name__ == "__main__":
    print("Starting alert tests...")
    test_high_cpu_alert()
    time.sleep(2)
    test_disk_space_alert()
    print("\nTests completed. Monitor alerts for next 10 minutes.")
```

**10. Alert maintenance calendar integration**:
```python
#!/usr/bin/env python3
"""
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ silences –≤–æ –≤—Ä–µ–º—è maintenance windows
"""
import requests
from datetime import datetime, timedelta

ALERTMANAGER_URL = "http://localhost:9093"

def create_maintenance_silence(service, duration_hours, comment):
    """–°–æ–∑–¥–∞—Ç—å silence –Ω–∞ –≤—Ä–µ–º—è maintenance"""
    
    now = datetime.utcnow()
    starts_at = now.isoformat() + "Z"
    ends_at = (now + timedelta(hours=duration_hours)).isoformat() + "Z"
    
    silence = {
        "matchers": [
            {
                "name": "service",
                "value": service,
                "isRegex": False
            }
        ],
        "startsAt": starts_at,
        "endsAt": ends_at,
        "createdBy": "maintenance-script",
        "comment": comment
    }
    
    response = requests.post(
        f"{ALERTMANAGER_URL}/api/v2/silences",
        json=silence
    )
    
    if response.status_code == 200:
        silence_id = response.json()['silenceID']
        print(f"‚úÖ Silence created: {silence_id}")
        print(f"   Service: {service}")
        print(f"   Duration: {duration_hours} hours")
        print(f"   Ends at: {ends_at}")
        return silence_id
    else:
        print(f"‚ùå Failed to create silence: {response.text}")
        return None

if __name__ == "__main__":
    # –ü—Ä–∏–º–µ—Ä: Maintenance –Ω–∞ API —Å–µ—Ä–≤–∏—Å–µ –Ω–∞ 2 —á–∞—Å–∞
    create_maintenance_silence(
        service="api",
        duration_hours=2,
        comment="Planned database migration"
    )
```

---

## –ò—Ç–æ–≥–∏ –º–æ–¥—É–ª—è 5

–ü–æ—Å–ª–µ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏—è —ç—Ç–æ–≥–æ –º–æ–¥—É–ª—è —Ç—ã –¥–æ–ª–∂–µ–Ω —É–º–µ—Ç—å:

‚úÖ –ù–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å Alertmanager —Å routing –∏ inhibition
‚úÖ –ü–∏—Å–∞—Ç—å –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ alert rules –≤ Prometheus
‚úÖ –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –∫–∞–Ω–∞–ª–∞–º–∏ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π (Slack, PagerDuty, Email)
‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å grouping, inhibition –∏ silencing
‚úÖ –°–æ–∑–¥–∞–≤–∞—Ç—å SLO-based alerts
‚úÖ –ò–∑–±–µ–≥–∞—Ç—å alert fatigue —á–µ—Ä–µ–∑ –ø—Ä–∞–≤–∏–ª—å–Ω—É—é –Ω–∞—Å—Ç—Ä–æ–π–∫—É
‚úÖ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –∏ –æ—Ç–ª–∞–∂–∏–≤–∞—Ç—å alerts
‚úÖ –°–æ–∑–¥–∞–≤–∞—Ç—å custom notification templates
‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å maintenance windows

**–ö–ª—é—á–µ–≤—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã –∞–ª–µ—Ä—Ç–∏–Ω–≥–∞:**
1. Alert –Ω–∞ —Å–∏–º–ø—Ç–æ–º—ã, –∞ –Ω–µ –Ω–∞ –ø—Ä–∏—á–∏–Ω—ã
2. –ö–∞–∂–¥—ã–π –∞–ª–µ—Ä—Ç –¥–æ–ª–∂–µ–Ω —Ç—Ä–µ–±–æ–≤–∞—Ç—å –¥–µ–π—Å—Ç–≤–∏—è
3. –ò—Å–ø–æ–ª—å–∑—É–π –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ severity —É—Ä–æ–≤–Ω–∏
4. –ì—Ä—É–ø–ø–∏—Ä—É–π –∏ –ø–æ–¥–∞–≤–ª—è–π –∑–∞–≤–∏—Å–∏–º—ã–µ –∞–ª–µ—Ä—Ç—ã
5. –†–µ–≥—É–ª—è—Ä–Ω–æ review –∏ cleanup –∞–ª–µ—Ä—Ç–æ–≤
6. –î–æ–∫—É–º–µ–Ω—Ç–∏—Ä—É–π runbooks –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∞–ª–µ—Ä—Ç–∞
7. –¢–µ—Å—Ç–∏—Ä—É–π –∞–ª–µ—Ä—Ç—ã —Ä–µ–≥—É–ª—è—Ä–Ω–æ


## –ú–æ–¥—É–ª—å 6: Distributed Tracing –∏ Application Performance Monitoring (40 –º–∏–Ω—É—Ç)

### üéØ –ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞

**–¢—Ä–∏ —Å—Ç–æ–ª–ø–∞ Observability:**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   METRICS   ‚îÇ  - –ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç? (CPU, memory, requests/sec)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    LOGS     ‚îÇ  - –ß—Ç–æ –ø—Ä–æ–∏–∑–æ—à–ª–æ? (—Å–æ–±—ã—Ç–∏—è, –æ—à–∏–±–∫–∏)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   TRACES    ‚îÇ  - –ü–æ—á–µ–º—É —ç—Ç–æ –ø—Ä–æ–∏–∑–æ—à–ª–æ? (–ø—É—Ç—å –∑–∞–ø—Ä–æ—Å–∞ —á–µ—Ä–µ–∑ —Å–∏—Å—Ç–µ–º—É)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Distributed Tracing - –∑–∞—á–µ–º –Ω—É–∂–µ–Ω:**

```
–ü—Ä–æ–±–ª–µ–º–∞ –≤ –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å–∞—Ö:
User Request ‚Üí API Gateway ‚Üí Auth Service ‚Üí Order Service ‚Üí Payment Service ‚Üí Database
                                                                   ‚Üì
                                              ‚ùå SLOW RESPONSE (5 seconds)

–í–æ–ø—Ä–æ—Å: –ì–¥–µ bottleneck?
- API Gateway: 50ms
- Auth Service: 100ms
- Order Service: 200ms
- Payment Service: 4500ms ‚Üê –ù–ê–ô–î–ï–ù–û!
- Database: 150ms
```

**–û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏:**

**Trace** - –ø–æ–ª–Ω—ã–π –ø—É—Ç—å –æ–¥–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ —á–µ—Ä–µ–∑ —Å–∏—Å—Ç–µ–º—É:

```
Trace ID: abc123
‚îú‚îÄ Span 1: API Gateway (50ms)
‚îú‚îÄ Span 2: Auth Service (100ms)
‚îú‚îÄ Span 3: Order Service (200ms)
‚îÇ  ‚îú‚îÄ Span 4: DB Query (50ms)
‚îÇ  ‚îî‚îÄ Span 5: Cache Check (10ms)
‚îî‚îÄ Span 6: Payment Service (4500ms)
   ‚îî‚îÄ Span 7: External API Call (4400ms) ‚Üê –ü—Ä–æ–±–ª–µ–º–∞!
```

**Span** - –µ–¥–∏–Ω–∏—Ü–∞ —Ä–∞–±–æ—Ç—ã –≤ —Å–∏—Å—Ç–µ–º–µ:

yaml

````yaml
Span:
  trace_id: "abc123"
  span_id: "span456"
  parent_span_id: "span789"
  operation_name: "POST /api/orders"
  start_time: "2025-01-15T10:00:00Z"
  duration: 200ms
  tags:
    http.method: "POST"
    http.status_code: 200
    service.name: "order-service"
    db.statement: "SELECT * FROM orders"
  logs:
    - timestamp: "2025-01-15T10:00:00.050Z"
      message: "Order validated"
```

**–ü–æ–ø—É–ª—è—Ä–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã —Ç—Ä–µ–π—Å–∏–Ω–≥–∞:**
```
Jaeger       - CNCF –ø—Ä–æ–µ–∫—Ç, –æ—Ç Uber, Go
Zipkin       - –û—Ç Twitter, Java
Tempo        - –û—Ç Grafana Labs, –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Loki
OpenTelemetry - –°—Ç–∞–Ω–¥–∞—Ä—Ç (–æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ OpenTracing + OpenCensus)
AWS X-Ray    - Managed —Å–µ—Ä–≤–∏—Å –æ—Ç AWS
Datadog APM  - Commercial
New Relic    - Commercial
```

**OpenTelemetry (OTel) - —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ     Your Application             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  OpenTelemetry SDK         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Auto-instrumentation    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Manual instrumentation  ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ OTel Collector ‚îÇ - –û–±—Ä–∞–±–æ—Ç–∫–∞, —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ                       ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îê
‚îÇ Jaeger ‚îÇ            ‚îÇ Tempo  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Sampling (–≤—ã–±–æ—Ä–∫–∞ —Ç—Ä–µ–π—Å–æ–≤):**
```
–ü—Ä–æ–±–ª–µ–º–∞: –ù–µ–ª—å–∑—è —Ö—Ä–∞–Ω–∏—Ç—å 100% —Ç—Ä–µ–π—Å–æ–≤ (—Å–ª–∏—à–∫–æ–º –¥–æ—Ä–æ–≥–æ)

–í–∏–¥—ã sampling:
1. Head sampling (—Ä–µ—à–µ–Ω–∏–µ –≤ –Ω–∞—á–∞–ª–µ)
   - Probabilistic: 10% –≤—Å–µ—Ö —Ç—Ä–µ–π—Å–æ–≤
   - Rate limiting: 100 —Ç—Ä–µ–π—Å–æ–≤/—Å–µ–∫
   
2. Tail sampling (—Ä–µ—à–µ–Ω–∏–µ –≤ –∫–æ–Ω—Ü–µ)
   - –í—Å–µ –º–µ–¥–ª–µ–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã (> 1s)
   - –í—Å–µ –∑–∞–ø—Ä–æ—Å—ã —Å –æ—à–∏–±–∫–∞–º–∏
   - 1% –Ω–æ—Ä–º–∞–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤

–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: Tail sampling + –≤—Å–µ–≥–¥–∞ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –æ—à–∏–±–∫–∏
```

**APM (Application Performance Monitoring) - —á—Ç–æ –≤–∫–ª—é—á–∞–µ—Ç:**
```
1. –¢—Ä–µ–π—Å–∏–Ω–≥ (Distributed Tracing)
2. –ü—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏–µ (CPU, Memory profiling)
3. Error tracking
4. Real User Monitoring (RUM)
5. Database query analysis
6. External services monitoring
```

**–ö–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ APM:**

**RED –º–µ—Ç—Ä–∏–∫–∏ (–¥–ª—è —Å–µ—Ä–≤–∏—Å–æ–≤):**
```
Rate     - Requests per second
Error    - Error rate (%)
Duration - Request latency (p50, p95, p99)
```

**USE –º–µ—Ç—Ä–∏–∫–∏ (–¥–ª—è —Ä–µ—Å—É—Ä—Å–æ–≤):**
```
Utilization - % –≤—Ä–µ–º–µ–Ω–∏ –∑–∞–Ω—è—Ç–æ—Å—Ç–∏
Saturation  - –î–ª–∏–Ω–∞ –æ—á–µ—Ä–µ–¥–∏
Errors      - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—à–∏–±–æ–∫
```

**Service metrics:**
```
Apdex Score = (Satisfied + Tolerating/2) / Total Requests
- Satisfied: < 1s
- Tolerating: 1-4s
- Frustrated: > 4s

Throughput = Requests per second
Error Rate = Errors / Total Requests
Availability = Uptime / Total Time
````

**Context Propagation (–∫–∞–∫ –ø–µ—Ä–µ–¥–∞–µ—Ç—Å—è trace_id):**

**HTTP Headers:**

http

````http
# W3C Trace Context (—Å—Ç–∞–Ω–¥–∞—Ä—Ç)
traceparent: 00-abc123def456-span789-01
tracestate: vendor1=value1,vendor2=value2

# Jaeger
uber-trace-id: abc123:span456:0:1

# Zipkin
X-B3-TraceId: abc123
X-B3-SpanId: span456
X-B3-ParentSpanId: parent789
X-B3-Sampled: 1
```

**gRPC Metadata:**
```
grpc-trace-bin: <binary trace context>
````

**Instrumentation –ø–æ–¥—Ö–æ–¥—ã:**

**Auto-instrumentation** (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π):

python

```python
# Python —Å OpenTelemetry
from opentelemetry.instrumentation.flask import FlaskInstrumentor
from opentelemetry.instrumentation.requests import RequestsInstrumentor

FlaskInstrumentor().instrument()      # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ Flask
RequestsInstrumentor().instrument()   # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ requests
```

**Manual instrumentation** (—Ä—É—á–Ω–æ–π):

python

```python
from opentelemetry import trace

tracer = trace.get_tracer(__name__)

@app.route('/api/order')
def create_order():
    with tracer.start_as_current_span("create_order") as span:
        span.set_attribute("order.id", order_id)
        span.set_attribute("user.id", user_id)
        
        # –í–∞—à –∫–æ–¥
        result = process_order(order_id)
        
        span.add_event("Order processed")
        return result
```

**–Ø–∑—ã–∫-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏:**

**Python:**

python

```python
# OpenTelemetry
opentelemetry-api
opentelemetry-sdk
opentelemetry-instrumentation-flask
opentelemetry-instrumentation-django
opentelemetry-instrumentation-sqlalchemy
opentelemetry-exporter-jaeger
```

**Node.js:**

javascript

```javascript
// OpenTelemetry
@opentelemetry/api
@opentelemetry/sdk-node
@opentelemetry/auto-instrumentations-node
@opentelemetry/exporter-jaeger
```

**Go:**

go

```go
// OpenTelemetry
go.opentelemetry.io/otel
go.opentelemetry.io/otel/trace
go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp
```

**Java:**

java

````java
// OpenTelemetry Java Agent (auto-instrumentation)
java -javaagent:opentelemetry-javaagent.jar \
     -Dotel.service.name=my-service \
     -jar myapp.jar
```

**Jaeger UI - –æ—Å–Ω–æ–≤–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:**
```
1. Search traces:
   - –ü–æ service name
   - –ü–æ operation name
   - –ü–æ tags
   - –ü–æ duration
   - –ü–æ –≤—Ä–µ–º–µ–Ω–∏

2. Trace timeline:
   - –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è spans
   - Waterfall view
   - Gantt chart

3. Dependencies graph:
   - –ö–∞—Ä—Ç–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π —Å–µ—Ä–≤–∏—Å–æ–≤
   - –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≤—ã–∑–æ–≤–æ–≤

4. Comparison:
   - –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ç—Ä–µ–π—Å–æ–≤
   - A/B testing —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
```

**Service Map (–∫–∞—Ä—Ç–∞ —Å–µ—Ä–≤–∏—Å–æ–≤):**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   User     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      ‚îÇ HTTP
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ API Gateway‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      ‚îÇ
   ‚îå‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ      ‚îÇ        ‚îÇ
‚îå‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îê ‚îå‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇAuth ‚îÇ ‚îÇOrder‚îÇ ‚îÇUser  ‚îÇ
‚îÇSvc  ‚îÇ ‚îÇSvc  ‚îÇ ‚îÇSvc   ‚îÇ
‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
   ‚îÇ       ‚îÇ
   ‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ   ‚îÇPayment  ‚îÇ
   ‚îÇ   ‚îÇSvc      ‚îÇ
   ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
   ‚îÇ       ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ         ‚îÇ
   ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îê
   ‚îÇ DB   ‚îÇ  ‚îÇCache ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Error tracking –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è:**
```
–°–≤—è–∑—å —Ç—Ä–µ–π—Å–æ–≤ —Å –æ—à–∏–±–∫–∞–º–∏:

Exception –≤ –∫–æ–¥–µ ‚Üí Trace ID ‚Üí –ü–æ–ª–Ω—ã–π –ø—É—Ç—å –∑–∞–ø—Ä–æ—Å–∞
                               + stack trace
                               + request params
                               + user context
```

**Database query analysis:**
```
–ß–∞—Å—Ç—ã–µ –ø—Ä–æ–±–ª–µ–º—ã:
1. N+1 queries
   - 1 –∑–∞–ø—Ä–æ—Å —Å–ø–∏—Å–∫–∞ + N –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–µ—Ç–∞–ª–µ–π
   
2. Missing indexes
   - Full table scan
   
3. Slow queries
   - –°–ª–æ–∂–Ω—ã–µ JOIN
   - –ë–æ–ª—å—à–∏–µ SELECT *
   
4. Connection pool exhaustion
   - –ù–µ –∑–∞–∫—Ä—ã—Ç—ã–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
```

**–ü—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏–µ (CPU/Memory):**
```
Continuous Profiling:
- Flamegraph –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
- –ö–∞–∫–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –∑–∞–Ω–∏–º–∞—é—Ç –±–æ–ª—å—à–µ –≤—Ä–µ–º–µ–Ω–∏
- Memory allocations
- Goroutines/Threads

–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã:
- pprof (Go)
- py-spy (Python)
- async-profiler (Java)
- Pyroscope (unified)
````

**Real User Monitoring (RUM):**

javascript

````javascript
// Frontend —Ç—Ä–µ–π—Å–∏–Ω–≥
import { WebTracerProvider } from '@opentelemetry/sdk-trace-web';

const provider = new WebTracerProvider();
const tracer = provider.getTracer('frontend-app');

// Track page load
const span = tracer.startSpan('page_load');
span.setAttribute('page.url', window.location.href);

window.addEventListener('load', () => {
  span.end();
});

// Track user interactions
button.addEventListener('click', () => {
  const span = tracer.startSpan('button_click');
  span.setAttribute('button.id', button.id);
  // ... –¥–µ–π—Å—Ç–≤–∏–µ
  span.end();
});
```

**Best practices:**
```
1. ‚úÖ –í—Å–µ–≥–¥–∞ –ø–µ—Ä–µ–¥–∞–≤–∞–π trace context –º–µ–∂–¥—É —Å–µ—Ä–≤–∏—Å–∞–º–∏
2. ‚úÖ –î–æ–±–∞–≤–ª—è–π –ø–æ–ª–µ–∑–Ω—ã–µ attributes (user_id, order_id, etc)
3. ‚úÖ –õ–æ–≥–∏—Ä—É–π trace_id –≤–æ –≤—Å–µ—Ö –ª–æ–≥–∞—Ö
4. ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–π semantic conventions (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –∏–º–µ–Ω–∞)
5. ‚úÖ –ù–∞—Å—Ç—Ä–æ–π –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π sampling
6. ‚úÖ –ù–µ –ª–æ–≥–∏—Ä—É–π sensitive –¥–∞–Ω–Ω—ã–µ –≤ spans
7. ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–π tail sampling –¥–ª—è –æ—à–∏–±–æ–∫
8. ‚úÖ –•—Ä–∞–Ω–∏ —Ç—Ä–µ–π—Å—ã –º–∏–Ω–∏–º—É–º 7 –¥–Ω–µ–π
9. ‚úÖ –ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–π —Å –∞–ª–µ—Ä—Ç–∏–Ω–≥–æ–º
10. ‚úÖ –°–æ–∑–¥–∞–π runbook –¥–ª—è —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
````

**Semantic Conventions (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –∏–º–µ–Ω–∞):**

yaml

```yaml
# HTTP
span.name: "GET /api/users"
http.method: "GET"
http.url: "https://api.example.com/users"
http.status_code: 200
http.route: "/api/users"

# Database
span.name: "SELECT users"
db.system: "postgresql"
db.operation: "SELECT"
db.statement: "SELECT * FROM users WHERE id = ?"
db.name: "production"

# RPC
span.name: "UserService.GetUser"
rpc.system: "grpc"
rpc.service: "UserService"
rpc.method: "GetUser"

# Messaging
span.name: "process_order"
messaging.system: "kafka"
messaging.destination: "orders"
messaging.operation: "process"
```

### üíª –ó–∞–¥–∞–Ω–∏–µ

–ù–∞—Å—Ç—Ä–æ–π –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π distributed tracing —Å Jaeger:

1. **–°–æ–∑–¥–∞–π docker-compose.yml –¥–ª—è Jaeger stack**:

yaml

```yaml
version: '3.8'

services:
  # Jaeger all-in-one (–¥–ª—è development)
  jaeger:
    image: jaegertracing/all-in-one:1.52
    container_name: jaeger
    environment:
      - COLLECTOR_ZIPKIN_HOST_PORT=:9411
      - COLLECTOR_OTLP_ENABLED=true
    ports:
      - "5775:5775/udp"   # accept zipkin.thrift (deprecated)
      - "6831:6831/udp"   # accept jaeger.thrift compact
      - "6832:6832/udp"   # accept jaeger.thrift binary
      - "5778:5778"       # serve configs
      - "16686:16686"     # Jaeger UI
      - "14250:14250"     # model.proto
      - "14268:14268"     # jaeger.thrift
      - "14269:14269"     # Admin port: health, metrics
      - "4317:4317"       # OTLP gRPC
      - "4318:4318"       # OTLP HTTP
      - "9411:9411"       # Zipkin compatible
    restart: unless-stopped

  # OpenTelemetry Collector (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏)
  otel-collector:
    image: otel/opentelemetry-collector-contrib:0.91.0
    container_name: otel-collector
    command: ["--config=/etc/otel-collector-config.yml"]
    volumes:
      - ./otel-collector-config.yml:/etc/otel-collector-config.yml
    ports:
      - "4317:4317"   # OTLP gRPC
      - "4318:4318"   # OTLP HTTP
      - "8888:8888"   # Prometheus metrics
      - "8889:8889"   # Prometheus exporter metrics
    restart: unless-stopped
    depends_on:
      - jaeger

  # Demo –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ - Frontend
  frontend:
    build: ./demo-app/frontend
    container_name: frontend
    environment:
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4317
      - OTEL_SERVICE_NAME=frontend
      - BACKEND_URL=http://backend:5000
    ports:
      - "8080:8080"
    depends_on:
      - otel-collector
      - backend
    restart: unless-stopped

  # Demo –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ - Backend
  backend:
    build: ./demo-app/backend
    container_name: backend
    environment:
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4317
      - OTEL_SERVICE_NAME=backend
      - DATABASE_URL=postgresql://user:password@postgres:5432/demo
      - REDIS_URL=redis://redis:6379
    ports:
      - "5000:5000"
    depends_on:
      - postgres
      - redis
      - otel-collector
    restart: unless-stopped

  # PostgreSQL database
  postgres:
    image: postgres:16-alpine
    container_name: postgres
    environment:
      - POSTGRES_USER=user
      - POSTGRES_PASSWORD=password
      - POSTGRES_DB=demo
    ports:
      - "5432:5432"
    volumes:
      - postgres-data:/var/lib/postgresql/data
    restart: unless-stopped

  # Redis cache
  redis:
    image: redis:7-alpine
    container_name: redis
    ports:
      - "6379:6379"
    restart: unless-stopped

  # Grafana –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
  grafana:
    image: grafana/grafana:10.2.3
    container_name: grafana-tracing
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_FEATURE_TOGGLES_ENABLE=traceqlEditor
    volumes:
      - grafana-data:/var/lib/grafana
      - ./grafana-datasources.yml:/etc/grafana/provisioning/datasources/datasources.yml
    restart: unless-stopped
    depends_on:
      - jaeger

volumes:
  postgres-data:
  grafana-data:
```

2. **–°–æ–∑–¥–∞–π otel-collector-config.yml**:

yaml

```yaml
receivers:
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317
      http:
        endpoint: 0.0.0.0:4318

  # Prometheus metrics receiver
  prometheus:
    config:
      scrape_configs:
        - job_name: 'otel-collector'
          scrape_interval: 10s
          static_configs:
            - targets: ['0.0.0.0:8888']

processors:
  # Batch processor –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
  batch:
    timeout: 10s
    send_batch_size: 1024

  # Memory limiter
  memory_limiter:
    check_interval: 1s
    limit_mib: 512

  # Tail sampling - —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ –æ—à–∏–±–∫–∏ –∏ –º–µ–¥–ª–µ–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
  tail_sampling:
    decision_wait: 10s
    num_traces: 100
    expected_new_traces_per_sec: 10
    policies:
      # –í—Å–µ–≥–¥–∞ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –æ—à–∏–±–∫–∏
      - name: error-traces
        type: status_code
        status_code:
          status_codes: [ERROR]
      
      # –ú–µ–¥–ª–µ–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã (> 1s)
      - name: slow-traces
        type: latency
        latency:
          threshold_ms: 1000
      
      # 10% –æ—Å—Ç–∞–ª—å–Ω—ã—Ö
      - name: probabilistic-policy
        type: probabilistic
        probabilistic:
          sampling_percentage: 10

  # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–Ω—ã—Ö –∞—Ç—Ä–∏–±—É—Ç–æ–≤
  resource:
    attributes:
      - key: environment
        value: development
        action: insert

  # Attributes processor
  attributes:
    actions:
      - key: db.statement
        action: delete  # –£–¥–∞–ª—è–µ–º SQL –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)

exporters:
  # Jaeger exporter
  jaeger:
    endpoint: jaeger:14250
    tls:
      insecure: true

  # Logging exporter (–¥–ª—è –æ—Ç–ª–∞–¥–∫–∏)
  logging:
    loglevel: info

  # Prometheus exporter –¥–ª—è –º–µ—Ç—Ä–∏–∫
  prometheus:
    endpoint: "0.0.0.0:8889"

service:
  pipelines:
    # Traces pipeline
    traces:
      receivers: [otlp]
      processors: [memory_limiter, tail_sampling, batch, resource, attributes]
      exporters: [jaeger, logging]
    
    # Metrics pipeline
    metrics:
      receivers: [otlp, prometheus]
      processors: [memory_limiter, batch]
      exporters: [prometheus, logging]
```

3. **–°–æ–∑–¥–∞–π demo-app/backend (Python Flask)**:

`demo-app/backend/Dockerfile`:

dockerfile

````dockerfile
FROM python:3.11-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD ["python", "app.py"]
```

`demo-app/backend/requirements.txt`:
```
flask==3.0.0
psycopg2-binary==2.9.9
redis==5.0.1
requests==2.31.0
opentelemetry-api==1.21.0
opentelemetry-sdk==1.21.0
opentelemetry-instrumentation-flask==0.42b0
opentelemetry-instrumentation-requests==0.42b0
opentelemetry-instrumentation-psycopg2==0.42b0
opentelemetry-instrumentation-redis==0.42b0
opentelemetry-exporter-otlp==1.21.0
````

`demo-app/backend/app.py`:

python

```python
from flask import Flask, jsonify, request
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter
from opentelemetry.instrumentation.flask import FlaskInstrumentor
from opentelemetry.instrumentation.requests import RequestsInstrumentor
from opentelemetry.instrumentation.psycopg2 import Psycopg2Instrumentor
from opentelemetry.instrumentation.redis import RedisInstrumentor
from opentelemetry.sdk.resources import Resource
from opentelemetry.trace.status import Status, StatusCode

import psycopg2
import redis
import time
import random
import os
import json

# ---------------------------------------------------------------------
# OpenTelemetry configuration
# ---------------------------------------------------------------------

resource = Resource.create(
    {
        "service.name": os.getenv("OTEL_SERVICE_NAME", "backend"),
        "service.version": "1.0.0",
        "deployment.environment": "development",
    }
)

provider = TracerProvider(resource=resource)
processor = BatchSpanProcessor(
    OTLPSpanExporter(
        endpoint=os.getenv("OTEL_EXPORTER_OTLP_ENDPOINT", "http://localhost:4317"),
        insecure=True,
    )
)
provider.add_span_processor(processor)
trace.set_tracer_provider(provider)

tracer = trace.get_tracer(__name__)

# ---------------------------------------------------------------------
# Flask app
# ---------------------------------------------------------------------

app = Flask(__name__)

FlaskInstrumentor().instrument_app(app)
RequestsInstrumentor().instrument()
Psycopg2Instrumentor().instrument()
RedisInstrumentor().instrument()

# ---------------------------------------------------------------------
# Connections
# ---------------------------------------------------------------------

DATABASE_URL = os.getenv(
    "DATABASE_URL", "postgresql://user:password@localhost:5432/demo"
)
REDIS_URL = os.getenv("REDIS_URL", "redis://localhost:6379")


def get_db_connection():
    return psycopg2.connect(DATABASE_URL)


def get_redis_connection():
    return redis.from_url(REDIS_URL)


# ---------------------------------------------------------------------
# Database init
# ---------------------------------------------------------------------

def init_db():
    with get_db_connection() as conn:
        with conn.cursor() as cur:
            cur.execute(
                """
                CREATE TABLE IF NOT EXISTS users (
                    id SERIAL PRIMARY KEY,
                    name VARCHAR(100),
                    email VARCHAR(100),
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
                """
            )
            cur.execute(
                """
                CREATE TABLE IF NOT EXISTS orders (
                    id SERIAL PRIMARY KEY,
                    user_id INTEGER REFERENCES users(id),
                    product VARCHAR(100),
                    amount DECIMAL(10, 2),
                    status VARCHAR(20),
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
                """
            )
            conn.commit()


# ---------------------------------------------------------------------
# Routes
# ---------------------------------------------------------------------

@app.route("/health")
def health():
    return jsonify({"status": "healthy"}), 200


@app.route("/api/users", methods=["GET"])
def get_users():
    with tracer.start_as_current_span("get_users") as span:
        time.sleep(random.uniform(0.01, 0.1))

        r = get_redis_connection()
        cached = r.get("users:all")

        if cached:
            span.set_attribute("cache.hit", True)
            return jsonify(json.loads(cached)), 200

        span.set_attribute("cache.hit", False)

        with get_db_connection() as conn:
            with conn.cursor() as cur:
                cur.execute("SELECT id, name, email FROM users")
                users = [
                    {"id": r[0], "name": r[1], "email": r[2]}
                    for r in cur.fetchall()
                ]

        r.setex("users:all", 60, json.dumps(users))
        return jsonify(users), 200


@app.route("/api/users/<int:user_id>", methods=["GET"])
def get_user(user_id):
    with tracer.start_as_current_span("get_user_by_id") as span:
        span.set_attribute("user.id", user_id)
        time.sleep(random.uniform(0.01, 0.05))

        with get_db_connection() as conn:
            with conn.cursor() as cur:
                cur.execute(
                    "SELECT id, name, email FROM users WHERE id = %s",
                    (user_id,),
                )
                row = cur.fetchone()

        if not row:
            return jsonify({"error": "User not found"}), 404

        return jsonify(
            {"id": row[0], "name": row[1], "email": row[2]}
        ), 200


@app.route("/api/users", methods=["POST"])
def create_user():
    with tracer.start_as_current_span("create_user") as span:
        data = request.json or {}

        if not data.get("name") or not data.get("email"):
            span.set_status(Status(StatusCode.ERROR))
            return jsonify({"error": "Name and email required"}), 400

        time.sleep(random.uniform(0.05, 0.15))

        with get_db_connection() as conn:
            with conn.cursor() as cur:
                cur.execute(
                    "INSERT INTO users (name, email) VALUES (%s, %s) RETURNING id",
                    (data["name"], data["email"]),
                )
                user_id = cur.fetchone()[0]
                conn.commit()

        get_redis_connection().delete("users:all")

        return jsonify(
            {"id": user_id, "name": data["name"], "email": data["email"]}
        ), 201


@app.route("/api/orders", methods=["POST"])
def create_order():
    with tracer.start_as_current_span("create_order") as span:
        data = request.json or {}

        user_id = data.get("user_id")
        product = data.get("product")
        amount = data.get("amount")

        if not all([user_id, product, amount]):
            span.set_status(Status(StatusCode.ERROR))
            return jsonify({"error": "Missing required fields"}), 400

        # Check user
        with get_db_connection() as conn:
            with conn.cursor() as cur:
                cur.execute("SELECT id FROM users WHERE id = %s", (user_id,))
                if not cur.fetchone():
                    return jsonify({"error": "User not found"}), 404

        # Payment simulation
        time.sleep(random.uniform(0.1, 0.5))

        # Save order
        with get_db_connection() as conn:
            with conn.cursor() as cur:
                cur.execute(
                    """
                    INSERT INTO orders (user_id, product, amount, status)
                    VALUES (%s, %s, %s, %s)
                    RETURNING id
                    """,
                    (user_id, product, amount, "completed"),
                )
                order_id = cur.fetchone()[0]
                conn.commit()

        return jsonify(
            {
                "id": order_id,
                "user_id": user_id,
                "product": product,
                "amount": amount,
                "status": "completed",
            }
        ), 201


@app.route("/api/orders/<int:order_id>", methods=["GET"])
def get_order(order_id):
    with tracer.start_as_current_span("get_order") as span:
        time.sleep(random.uniform(0.01, 0.05))

        with get_db_connection() as conn:
            with conn.cursor() as cur:
                cur.execute(
                    """
                    SELECT id, user_id, product, amount, status
                    FROM orders WHERE id = %s
                    """,
                    (order_id,),
                )
                row = cur.fetchone()

        if not row:
            return jsonify({"error": "Order not found"}), 404

        return jsonify(
            {
                "id": row[0],
                "user_id": row[1],
                "product": row[2],
                "amount": float(row[3]),
                "status": row[4],
            }
        ), 200


@app.route("/api/slow")
def slow_endpoint():
    with tracer.start_as_current_span("slow_endpoint") as span:
        delay = random.uniform(2, 5)
        time.sleep(delay)
        return jsonify({"delay": delay}), 200


@app.route("/api/error")
def error_endpoint():
    with tracer.start_as_current_span("error_endpoint") as span:
        try:
            raise RuntimeError("Simulated error for testing")
        except Exception as e:
            span.record_exception(e)
            span.set_status(Status(StatusCode.ERROR, str(e)))
            return jsonify({"error": str(e)}), 500


# ---------------------------------------------------------------------
# Entry point - –ó–∞–ø—É—Å–∫ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
# ---------------------------------------------------------------------

if __name__ == "__main__":
    init_db()
    app.run(host="0.0.0.0", port=5000)
```
```
````

4. **–°–æ–∑–¥–∞–π demo-app/frontend (–ø—Ä–æ—Å—Ç–æ–π HTML + JS)**:

`demo-app/frontend/Dockerfile`:
```dockerfile
FROM nginx:alpine

COPY index.html /usr/share/nginx/html/
COPY nginx.conf /etc/nginx/conf.d/default.conf

EXPOSE 8080
```

`demo-app/frontend/index.html`:
```html
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Tracing Demo App</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 800px;
            margin: 50px auto;
            padding: 20px;
            background-color: #f5f5f5;
        }
        .container {
            background: white;
            padding: 30px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        h1 {
            color: #333;
            border-bottom: 2px solid #007bff;
            padding-bottom: 10px;
        }
        .section {
            margin: 20px 0;
            padding: 20px;
            border: 1px solid #ddd;
            border-radius: 4px;
        }
        button {
            background-color: #007bff;
            color: white;
            padding: 10px 20px;
            border: none;
            border-radius: 4px;
            cursor: pointer;
            margin: 5px;
        }
        button:hover {
            background-color: #0056b3;
        }
        .error {
            background-color: #dc3545;
        }
        .error:hover {
            background-color: #c82333;
        }
        .slow {
            background-color: #ffc107;
        }
        .slow:hover {
            background-color: #e0a800;
        }
        #output {
            margin-top: 20px;
            padding: 15px;
            background-color: #f8f9fa;
            border-radius: 4px;
            min-height: 100px;
            white-space: pre-wrap;
            font-family: monospace;
        }
        input {
            padding: 8px;
            margin: 5px;
            border: 1px solid #ddd;
            border-radius: 4px;
            width: 200px;
        }
        .links {
            margin-top: 30px;
            padding: 20px;
            background-color: #e9ecef;
            border-radius: 4px;
        }
        .links a {
            display: block;
            margin: 10px 0;
            color: #007bff;
            text-decoration: none;
        }
        .links a:hover {
            text-decoration: underline;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>üîç Distributed Tracing Demo</h1>
        
        <div class="section">
            <h2>User Operations</h2>
            <button onclick="getUsers()">Get All Users</button>
            <button onclick="createUser()">Create Random User</button>
            <br>
            <input type="number" id="userId" placeholder="User ID">
            <button onclick="getUser()">Get User by ID</button>
        </div>
        
        <div class="section">
            <h2>Order Operations</h2>
            <input type="number" id="orderUserId" placeholder="User ID">
            <input type="text" id="product" placeholder="Product">
            <input type="number" id="amount" placeholder="Amount">
            <button onclick="createOrder()">Create Order</button>
            <br><br>
            <input type="number" id="orderId" placeholder="Order ID">
            <button onclick="getOrder()">Get Order by ID</button>
        </div>
        
        <div class="section">
            <h2>Test Scenarios</h2>
            <button class="slow" onclick="testSlow()">Test Slow Endpoint (2-5s)</button>
            <button class="error" onclick="testError()">Test Error Endpoint</button>
            <button onclick="stressTest()">Stress Test (10 requests)</button>
        </div>
        
        <div id="output">Response will appear here...</div>
        
        <div class="links">
            <h3>üìä Monitoring Links</h3>
            <a href="http://localhost:16686" target="_blank">üîç Jaeger UI - View Traces</a>
            <a href="http://localhost:3000" target="_blank">üìà Grafana - Metrics & Traces</a>
            <a href="http://localhost:5000/health" target="_blank">üíö Backend Health Check</a>
        </div>
    </div>

    <script>
        const API_URL = 'http://localhost:5000/api';
        const output = document.getElementById('output');

        function log(message, data = null) {
            const timestamp = new Date().toISOString();
            let logMessage = `[${timestamp}] ${message}`;
            if (data) {
                logMessage += '\n' + JSON.stringify(data, null, 2);
            }
            output.textContent = logMessage;
            console.log(message, data);
        }

        async function getUsers() {
            try {
                log('Fetching all users...');
                const response = await fetch(`${API_URL}/users`);
                const data = await response.json();
                log('‚úÖ Users retrieved:', data);
            } catch (error) {
                log('‚ùå Error:', error.message);
            }
        }

        async function getUser() {
            const userId = document.getElementById('userId').value;
            if (!userId) {
                log('‚ùå Please enter a user ID');
                return;
            }
            
            try {
                log(`Fetching user ${userId}...`);
                const response = await fetch(`${API_URL}/users/${userId}`);
                const data = await response.json();
                
                if (response.ok) {
                    log('‚úÖ User retrieved:', data);
                } else {
                    log('‚ùå Error:', data);
                }
            } catch (error) {
                log('‚ùå Error:', error.message);
            }
        }

        async function createUser() {
            const randomNum = Math.floor(Math.random() * 1000);
            const userData = {
                name: `User ${randomNum}`,
                email: `user${randomNum}@example.com`
            };
            
            try {
                log('Creating user...', userData);
                const response = await fetch(`${API_URL}/users`, {
                    method: 'POST',
                    headers: {'Content-Type': 'application/json'},
                    body: JSON.stringify(userData)
                });
                const data = await response.json();
                log('‚úÖ User created:', data);
            } catch (error) {
                log('‚ùå Error:', error.message);
            }
        }

        async function createOrder() {
            const orderData = {
                user_id: parseInt(document.getElementById('orderUserId').value),
                product: document.getElementById('product').value || 'Product',
                amount: parseFloat(document.getElementById('amount').value) || 99.99
            };
            
            if (!orderData.user_id) {
                log('‚ùå Please enter a user ID');
                return;
            }
            
            try {
                log('Creating order...', orderData);
                const response = await fetch(`${API_URL}/orders`, {
                    method: 'POST',
                    headers: {'Content-Type': 'application/json'},
                    body: JSON.stringify(orderData)
                });
                const data = await response.json();
                
                if (response.ok) {
                    log('‚úÖ Order created:', data);
                } else {
                    log('‚ùå Error:', data);
                }
            } catch (error) {
                log('‚ùå Error:', error.message);
            }
        }

        async function getOrder() {
            const orderId = document.getElementById('orderId').value;
            if (!orderId) {
                log('‚ùå Please enter an order ID');
                return;
            }
            
            try {
                log(`Fetching order ${orderId}...`);
                const response = await fetch(`${API_URL}/orders/${orderId}`);
                const data = await response.json();
                
                if (response.ok) {
                    log('‚úÖ Order retrieved:', data);
                } else {
                    log('‚ùå Error:', data);
                }
            } catch (error) {
                log('‚ùå Error:', error.message);
            }
        }

        async function testSlow() {
            try {
                log('‚è≥ Testing slow endpoint (this will take 2-5 seconds)...');
                const start = Date.now();
                const response = await fetch(`${API_URL}/slow`);
                const data = await response.json();
                const duration = ((Date.now() - start) / 1000).toFixed(2);
                log(`‚úÖ Slow endpoint completed in ${duration}s:`, data);
            } catch (error) {
                log('‚ùå Error:', error.message);
            }
        }

        async function testError() {
            try {
                log('üí• Testing error endpoint...');
                const response = await fetch(`${API_URL}/error`);
                const data = await response.json();
                log('‚ùå Expected error:', data);
            } catch (error) {
                log('‚ùå Error:', error.message);
            }
        }

        async function stressTest() {
            log('üî• Starting stress test with 10 parallel requests...');
            const promises = [];
            
            for (let i = 0; i < 10; i++) {
                promises.push(fetch(`${API_URL}/users`));
            }
            
            try {
                const start = Date.now();
                await Promise.all(promises);
                const duration = ((Date.now() - start) / 1000).toFixed(2);
                log(`‚úÖ Stress test completed in ${duration}s (10 requests)`);
            } catch (error) {
                log('‚ùå Stress test failed:', error.message);
            }
        }

        // Initial message
        log('üëã Welcome! Click any button to start generating traces.');
    </script>
</body>
</html>
```

`demo-app/frontend/nginx.conf`:
```nginx
server {
    listen 8080;
    server_name localhost;
    
    location / {
        root /usr/share/nginx/html;
        index index.html;
    }
    
    # CORS –¥–ª—è API –∑–∞–ø—Ä–æ—Å–æ–≤
    location /api {
        if ($request_method = 'OPTIONS') {
            add_header 'Access-Control-Allow-Origin' '*';
            add_header 'Access-Control-Allow-Methods' 'GET, POST, OPTIONS';
            add_header 'Access-Control-Allow-Headers' 'Content-Type';
            return 204;
        }
    }
}
```

5. **–°–æ–∑–¥–∞–π grafana-datasources.yml**:
```yaml
apiVersion: 1

datasources:
  - name: Jaeger
    type: jaeger
    access: proxy
    url: http://jaeger:16686
    isDefault: true
    editable: true
    jsonData:
      tracesToLogsV2:
        datasourceUid: 'loki'
        spanStartTimeShift: '-1h'
        spanEndTimeShift: '1h'
        filterByTraceID: true
        filterBySpanID: false
```

6. **–ó–∞–ø—É—Å—Ç–∏ –∏ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–π**:
```bash
# –°–æ–∑–¥–∞–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
mkdir -p demo-app/frontend demo-app/backend

# –ó–∞–ø—É—Å—Ç–∏ stack
docker-compose up -d

# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞
docker-compose ps

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –ª–æ–≥–æ–≤
docker-compose logs -f backend

# –ü—Ä–æ–≤–µ—Ä–∫–∞ Jaeger
curl http://localhost:16686

# –ü—Ä–æ–≤–µ—Ä–∫–∞ backend health
curl http://localhost:5000/health
```

7. **–û—Ç–∫—Ä–æ–π UI –∏ —Ç–µ—Å—Ç–∏—Ä—É–π**:
```bash
# Frontend demo app
open http://localhost:8080

# Jaeger UI
open http://localhost:16686

# Grafana
open http://localhost:3000

# –°–æ–∑–¥–∞–π —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
curl -X POST http://localhost:5000/api/users \
  -H "Content-Type: application/json" \
  -d '{"name": "Test User", "email": "test@example.com"}'

curl -X POST http://localhost:5000/api/orders \
  -H "Content-Type: application/json" \
  -d '{"user_id": 1, "product": "Test Product", "amount": 99.99}'
```

8. **–ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–π—Å–æ–≤ –≤ Jaeger**:
````

1. –û—Ç–∫—Ä–æ–π Jaeger UI: [http://localhost:16686](http://localhost:16686)
2. Search traces:
    - Service: backend
    - Operation: create_order
    - Min Duration: 1s (–¥–ª—è –º–µ–¥–ª–µ–Ω–Ω—ã—Ö)
    - Tags: error=true (–¥–ª—è –æ—à–∏–±–æ–∫)
3. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–π:
    - Timeline view - –≥–¥–µ –≤—Ä–µ–º—è —Ç—Ä–∞—Ç–∏—Ç—Å—è
    - Span details - –∞—Ç—Ä–∏–±—É—Ç—ã, —Å–æ–±—ã—Ç–∏—è, –æ—à–∏–±–∫–∏
    - Service graph - –∫–∞—Ä—Ç–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
    - Trace comparison - —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –±—ã—Å—Ç—Ä—ã—Ö –∏ –º–µ–¥–ª–µ–Ω–Ω—ã—Ö

````

### üöÄ –ë–æ–Ω—É—Å (–Ω–æ–≤–æ–µ)

**1. –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è Tempo (–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ Jaeger)**:

–î–æ–±–∞–≤—å –≤ `docker-compose.yml`:
```yaml
  tempo:
    image: grafana/tempo:2.3.1
    container_name: tempo
    command: ["-config.file=/etc/tempo.yaml"]
    volumes:
      - ./tempo.yaml:/etc/tempo.yaml
      - tempo-data:/tmp/tempo
    ports:
      - "3200:3200"   # Tempo UI
      - "4317:4317"   # OTLP gRPC
      - "4318:4318"   # OTLP HTTP
    restart: unless-stopped

volumes:
  tempo-data:
```

`tempo.yaml`:
```yaml
server:
  http_listen_port: 3200

distributor:
  receivers:
    otlp:
      protocols:
        grpc:
          endpoint: 0.0.0.0:4317
        http:
          endpoint: 0.0.0.0:4318

ingester:
  max_block_duration: 5m

compactor:
  compaction:
    block_retention: 168h  # 7 days

storage:
  trace:
    backend: local
    local:
      path: /tmp/tempo/blocks
    wal:
      path: /tmp/tempo/wal

metrics_generator:
  registry:
    external_labels:
      source: tempo
  storage:
    path: /tmp/tempo/generator/wal
  traces_storage:
    path: /tmp/tempo/generator/traces
```

**2. –°–æ–∑–¥–∞–π Python —Å–∫—Ä–∏–ø—Ç –¥–ª—è load testing —Å —Ç—Ä–µ–π—Å–∏–Ω–≥–æ–º**:

`load_test.py`:
```python
#!/usr/bin/env python3
"""
Load testing —Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π —Ç—Ä–µ–π—Å–æ–≤
"""
import concurrent.futures
import requests
import time
import random
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter
from opentelemetry.sdk.resources import Resource

# Setup tracing
resource = Resource.create({"service.name": "load-tester"})
provider = TracerProvider(resource=resource)
processor = BatchSpanProcessor(
    OTLPSpanExporter(endpoint="http://localhost:4317", insecure=True)
)
provider.add_span_processor(processor)
trace.set_tracer_provider(provider)
tracer = trace.get_tracer(__name__)

API_URL = "http://localhost:5000/api"

def make_request(endpoint, method="GET", data=None):
    """–î–µ–ª–∞–µ—Ç –∑–∞–ø—Ä–æ—Å —Å —Ç—Ä–µ–π—Å–∏–Ω–≥–æ–º"""
    with tracer.start_as_current_span(f"{method} {endpoint}") as span:
        span.set_attribute("http.method", method)
        span.set_attribute("http.url", f"{API_URL}{endpoint}")
        
        try:
            if method == "GET":
                response = requests.get(f"{API_URL}{endpoint}")
            else:
                response = requests.post(
                    f"{API_URL}{endpoint}",
                    json=data,
                    headers={"Content-Type": "application/json"}
                )
            
            span.set_attribute("http.status_code", response.status_code)
            
            if response.status_code >= 400:
                span.set_attribute("error", True)
                
            return response
            
        except Exception as e:
            span.record_exception(e)
            span.set_attribute("error", True)
            raise

def user_flow():
    """–°–∏–º—É–ª–∏—Ä—É–µ—Ç —Ç–∏–ø–∏—á–Ω—ã–π user flow"""
    with tracer.start_as_current_span("user_flow") as span:
        # 1. –°–æ–∑–¥–∞–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        user_data = {
            "name": f"LoadTest User {random.randint(1, 1000)}",
            "email": f"test{random.randint(1, 1000)}@example.com"
        }
        response = make_request("/users", "POST", user_data)
        
        if response.status_code != 201:
            span.set_attribute("flow.failed", True)
            return
        
        user_id = response.json()["id"]
        span.set_attribute("user.id", user_id)
        
        # 2. –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        time.sleep(random.uniform(0.1, 0.5))
        make_request(f"/users/{user_id}")
        
        # 3. –°–æ–∑–¥–∞–µ–º –∑–∞–∫–∞–∑
        time.sleep(random.uniform(0.1, 0.5))
        order_data = {
            "user_id": user_id,
            "product": f"Product {random.randint(1, 100)}",
            "amount": round(random.uniform(10, 500), 2)
        }
        response = make_request("/orders", "POST", order_data)
        
        if response.status_code == 201:
            order_id = response.json()["id"]
            span.set_attribute("order.id", order_id)
            
            # 4. –ü–æ–ª—É—á–∞–µ–º –∑–∞–∫–∞–∑
            time.sleep(random.uniform(0.1, 0.5))
            make_request(f"/orders/{order_id}")
        
        span.set_attribute("flow.completed", True)

def run_load_test(num_users=10, concurrent=5):
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç load test"""
    print(f"Starting load test: {num_users} users, {concurrent} concurrent")
    
    start_time = time.time()
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=concurrent) as executor:
        futures = [executor.submit(user_flow) for _ in range(num_users)]
        
        for future in concurrent.futures.as_completed(futures):
            try:
                future.result()
            except Exception as e:
                print(f"Error: {e}")
    
    duration = time.time() - start_time
    print(f"Load test completed in {duration:.2f}s")
    print(f"Average: {duration/num_users:.2f}s per user")
    print(f"Throughput: {num_users/duration:.2f} users/sec")

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='Load testing with tracing')
    parser.add_argument('--users', type=int, default=10, help='Number of users')
    parser.add_argument('--concurrent', type=int, default=5, help='Concurrent requests')
    
    args = parser.parse_args()
    
    run_load_test(num_users=args.users, concurrent=args.concurrent)
```

**3. –°–æ–∑–¥–∞–π dashboard –¥–ª—è APM –≤ Grafana**:

`grafana-dashboards/apm-dashboard.json`:
```json
{
  "dashboard": {
    "title": "Application Performance Monitoring",
    "panels": [
      {
        "id": 1,
        "title": "Request Rate",
        "targets": [
          {
            "expr": "sum(rate(traces_spanmetrics_calls_total[5m])) by (service_name)"
          }
        ],
        "type": "timeseries"
      },
      {
        "id": 2,
        "title": "Error Rate",
        "targets": [
          {
            "expr": "sum(rate(traces_spanmetrics_calls_total{status_code=\"STATUS_CODE_ERROR\"}[5m])) by (service_name)"
          }
        ],
        "type": "timeseries"
      },
      {
        "id": 3,
        "title": "Latency (p95)",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, sum(rate(traces_spanmetrics_latency_bucket[5m])) by (le, service_name))"
          }
        ],
        "type": "timeseries"
      },
      {
        "id": 4,
        "title": "Service Map",
        "type": "nodeGraph",
        "targets": [
          {
            "queryType": "serviceMap"
          }
        ]
      }
    ]
  }
}
```

**4. Continuous Profiling —Å Pyroscope**:

–î–æ–±–∞–≤—å –≤ `docker-compose.yml`:
```yaml
  pyroscope:
    image: grafana/pyroscope:latest
    container_name: pyroscope
    ports:
      - "4040:4040"
    restart: unless-stopped
```

–û–±–Ω–æ–≤–∏ Python app –¥–ª—è –ø—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏—è:
```python
import pyroscope

pyroscope.configure(
    application_name="backend",
    server_address="http://pyroscope:4040",
    tags={
        "environment": "development",
    }
)
```

---

## –ò—Ç–æ–≥–∏ –º–æ–¥—É–ª—è 6

–ü–æ—Å–ª–µ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏—è —ç—Ç–æ–≥–æ –º–æ–¥—É–ª—è —Ç—ã –¥–æ–ª–∂–µ–Ω —É–º–µ—Ç—å:

‚úÖ –ü–æ–Ω–∏–º–∞—Ç—å –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ distributed tracing (trace, span, context)
‚úÖ –ù–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å OpenTelemetry –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è—Ö
‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Jaeger –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç—Ä–µ–π—Å–æ–≤
‚úÖ –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç—Ä–µ–π—Å—ã —Å –ª–æ–≥–∞–º–∏ –∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏
‚úÖ –ù–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å sampling –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ö—Ä–∞–Ω–µ–Ω–∏—è
‚úÖ –ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å performance bottlenecks
‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Service Map –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
‚úÖ –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å continuous profiling
‚úÖ –°–æ–∑–¥–∞–≤–∞—Ç—å APM dashboards
‚úÖ –û—Ç–ª–∞–∂–∏–≤–∞—Ç—å –ø—Ä–æ–±–ª–µ–º—ã –≤ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö

**–ö–ª—é—á–µ–≤—ã–µ takeaways:**
1. –¢—Ä–µ–π—Å–∏–Ω–≥ –∫—Ä–∏—Ç–∏—á–µ–Ω –¥–ª—è –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å–æ–≤ - –±–µ–∑ –Ω–µ–≥–æ –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ –æ—Ç–ª–∞–¥–∏—Ç—å –ø—Ä–æ–±–ª–µ–º—ã
2. OpenTelemetry - —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç, –∏—Å–ø–æ–ª—å–∑—É–π –µ–≥–æ
3. –í—Å–µ–≥–¥–∞ —Å–æ—Ö—Ä–∞–Ω—è–π –æ—à–∏–±–∫–∏ –∏ –º–µ–¥–ª–µ–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã (tail sampling)
4. –°–≤—è–∑—ã–≤–∞–π —Ç—Ä–µ–π—Å—ã —Å –ª–æ–≥–∞–º–∏ —á–µ—Ä–µ–∑ trace_id
5. –ò—Å–ø–æ–ª—å–∑—É–π semantic conventions –¥–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏
6. Service Map –ø–æ–º–æ–≥–∞–µ—Ç –ø–æ–Ω—è—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É —Å–∏—Å—Ç–µ–º—ã
7. –ü—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–æ–ø–æ–ª–Ω—è–µ—Ç —Ç—Ä–µ–π—Å–∏–Ω–≥ –¥–ª—è deep analysis
8. –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π sampling —ç–∫–æ–Ω–æ–º–∏—Ç –¥–µ–Ω—å–≥–∏ –∏ storage
````
