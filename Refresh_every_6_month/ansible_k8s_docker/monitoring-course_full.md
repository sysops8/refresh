# –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –¥–ª—è DevOps: –ï–∂–µ–≥–æ–¥–Ω—ã–π/–ü–æ–ª—É–≥–æ–¥–æ–≤–æ–π –∫—É—Ä—Å-–æ—Å–≤–µ–∂–∏—Ç–µ–ª—å

**–¶–µ–ª—å:** –û—Å–≤–µ–∂–∏—Ç—å –≤ –ø–∞–º—è—Ç–∏ –∫–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∑–∞ 2-3 —á–∞—Å–∞ –ø—Ä–∞–∫—Ç–∏–∫–∏ –∏ —É–∑–Ω–∞—Ç—å 1-2 –Ω–æ–≤—ã–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏.

**–§–æ—Ä–º–∞—Ç:** –ö–∞–∂–¥—ã–π —Ä–∞–∑–¥–µ–ª —Å–æ—Å—Ç–æ–∏—Ç –∏–∑:
1. **–ö—Ä–∞—Ç–∫–æ–π —Ç–µ–æ—Ä–∏–∏ (–ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞)**: –°–∞–º–æ–µ –≥–ª–∞–≤–Ω–æ–µ, —á—Ç–æ –≤—ã –º–æ–≥–ª–∏ –∑–∞–±—ã—Ç—å
2. **–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–≥–æ –∑–∞–¥–∞–Ω–∏—è**: –†–µ–∞–ª—å–Ω–∞—è –∑–∞–¥–∞—á–∞, –∫–æ—Ç–æ—Ä—É—é –Ω—É–∂–Ω–æ —Ä–µ—à–∏—Ç—å
3. **–ë–æ–Ω—É—Å–Ω–æ–≥–æ –∑–∞–¥–∞–Ω–∏—è (–¥–ª—è —Ä–æ—Å—Ç–∞)**: –ó–∞–¥–∞—á–∞ –ø–æ—Å–ª–æ–∂–Ω–µ–µ –∏–ª–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –Ω–æ–≤–æ–π —Ñ–∏—á–∏

**–ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è:**
- –ë–∞–∑–æ–≤–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ Linux/Unix
- –î–æ—Å—Ç—É–ø –∫ —Å–µ—Ä–≤–µ—Ä—É –∏–ª–∏ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–π –º–∞—à–∏–Ω–µ
- Docker —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω (–¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ –∑–∞–¥–∞–Ω–∏–π)
- –ë–∞–∑–æ–≤—ã–µ –∑–Ω–∞–Ω–∏—è –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏

---

## –ú–æ–¥—É–ª—å 1: –û—Å–Ω–æ–≤—ã –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∏ –º–µ—Ç—Ä–∏–∫–∏ (20 –º–∏–Ω—É—Ç)

### üéØ –ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞

**–ß–µ—Ç—ã—Ä–µ –∑–æ–ª–æ—Ç—ã—Ö —Å–∏–≥–Ω–∞–ª–∞ (Four Golden Signals):**
```
1. Latency (–ó–∞–¥–µ—Ä–∂–∫–∞)      - –í—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –∑–∞–ø—Ä–æ—Å—ã
2. Traffic (–¢—Ä–∞—Ñ–∏–∫)        - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø—Ä–æ—Å–æ–≤
3. Errors (–û—à–∏–±–∫–∏)         - –ü—Ä–æ—Ü–µ–Ω—Ç –Ω–µ—É–¥–∞—á–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
4. Saturation (–ù–∞—Å—ã—â–µ–Ω–∏–µ)  - –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤ (CPU, –ø–∞–º—è—Ç—å, –¥–∏—Å–∫)
```

**–¢–∏–ø—ã –º–µ—Ç—Ä–∏–∫:**
```
Counter   - –ú–æ–Ω–æ—Ç–æ–Ω–Ω–æ –≤–æ–∑—Ä–∞—Å—Ç–∞—é—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ (–∑–∞–ø—Ä–æ—Å—ã, –æ—à–∏–±–∫–∏)
Gauge     - –¢–µ–∫—É—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ (CPU, –ø–∞–º—è—Ç—å, —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞)
Histogram - –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π (latency buckets)
Summary   - –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∑–∞ –ø–µ—Ä–∏–æ–¥ (percentiles)
```

**USE Method (–¥–ª—è —Ä–µ—Å—É—Ä—Å–æ–≤):**
```
Utilization - –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –∑–∞–Ω—è—Ç–æ—Å—Ç–∏ —Ä–µ—Å—É—Ä—Å–∞
Saturation  - –°—Ç–µ–ø–µ–Ω—å –ø–µ—Ä–µ–≥—Ä—É–∑–∫–∏
Errors      - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—à–∏–±–æ–∫
```

**RED Method (–¥–ª—è —Å–µ—Ä–≤–∏—Å–æ–≤):**
```
Rate     - –ó–∞–ø—Ä–æ—Å–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É
Errors   - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—à–∏–±–æ–∫
Duration - –í—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞
```

**–£—Ä–æ–≤–Ω–∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Application (APM)             ‚îÇ  - –ö–æ–¥, —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ   Service/Container             ‚îÇ  - Docker, K8s
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ   Operating System              ‚îÇ  - CPU, RAM, Disk
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ   Infrastructure                ‚îÇ  - Network, Hardware
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**–ö–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ Linux:**
```bash
# CPU
top, htop
mpstat -P ALL 1

# Memory
free -m
vmstat 1

# Disk I/O
iostat -x 1
iotop

# Network
iftop
nethogs
ss -s

# Process
ps aux --sort=-%mem | head
ps aux --sort=-%cpu | head
```

**–ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π:**
```
- Request rate (req/s)
- Error rate (%)
- Response time (ms) - p50, p95, p99
- Active connections
- Queue depth
- Database query time
- Cache hit ratio
```

### üíª –ó–∞–¥–∞–Ω–∏–µ

–ù–∞—Å—Ç—Ä–æ–π –±–∞–∑–æ–≤—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Å–∏—Å—Ç–µ–º—ã:

1. **–£—Å—Ç–∞–Ω–æ–≤–∏ –∏ –∑–∞–ø—É—Å—Ç–∏ Node Exporter** (–¥–ª—è —Å–±–æ—Ä–∞ –º–µ—Ç—Ä–∏–∫ —Ö–æ—Å—Ç–∞):
```bash
# –ß–µ—Ä–µ–∑ Docker
docker run -d \
  --name node-exporter \
  --net="host" \
  --pid="host" \
  -v "/:/host:ro,rslave" \
  prom/node-exporter:latest \
  --path.rootfs=/host

# –ü—Ä–æ–≤–µ—Ä–∫–∞
curl http://localhost:9100/metrics | head -20
```

2. **–ò–∑—É—á–∏ –æ—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏**:
```bash
# CPU
curl -s http://localhost:9100/metrics | grep node_cpu_seconds_total

# Memory
curl -s http://localhost:9100/metrics | grep node_memory

# Disk
curl -s http://localhost:9100/metrics | grep node_disk

# Network
curl -s http://localhost:9100/metrics | grep node_network
```

3. **–°–æ–∑–¥–∞–π –ø—Ä–æ—Å—Ç–æ–π bash —Å–∫—Ä–∏–ø—Ç** –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ (`monitor.sh`):
```bash
#!/bin/bash

echo "=== System Monitoring Report ==="
echo "Date: $(date)"
echo ""

# CPU Usage
echo "CPU Usage:"
top -bn1 | grep "Cpu(s)" | awk '{print "  User: " $2 ", System: " $4 ", Idle: " $8}'

# Memory Usage
echo ""
echo "Memory Usage:"
free -h | awk 'NR==2{printf "  Total: %s, Used: %s (%.2f%%)\n", $2, $3, $3*100/$2}'

# Disk Usage
echo ""
echo "Disk Usage:"
df -h / | awk 'NR==2{printf "  Total: %s, Used: %s (%s)\n", $2, $3, $5}'

# Load Average
echo ""
echo "Load Average:"
uptime | awk -F'load average:' '{print "  " $2}'

# Top 5 processes by CPU
echo ""
echo "Top 5 processes by CPU:"
ps aux --sort=-%cpu | head -6 | tail -5 | awk '{printf "  %s: %.1f%%\n", $11, $3}'

# Top 5 processes by Memory
echo ""
echo "Top 5 processes by Memory:"
ps aux --sort=-%mem | head -6 | tail -5 | awk '{printf "  %s: %.1f%%\n", $11, $4}'
```

4. –ó–∞–ø—É—Å—Ç–∏ —Å–∫—Ä–∏–ø—Ç:
```bash
chmod +x monitor.sh
./monitor.sh
```

### üöÄ –ë–æ–Ω—É—Å (–Ω–æ–≤–æ–µ)

**–ù–∞—Å—Ç—Ä–æ–π cAdvisor** –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ Docker –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤:
```bash
docker run -d \
  --name=cadvisor \
  --volume=/:/rootfs:ro \
  --volume=/var/run:/var/run:ro \
  --volume=/sys:/sys:ro \
  --volume=/var/lib/docker/:/var/lib/docker:ro \
  --publish=8080:8080 \
  --detach=true \
  gcr.io/cadvisor/cadvisor:latest

# –û—Ç–∫—Ä–æ–π –≤ –±—Ä–∞—É–∑–µ—Ä–µ
http://localhost:8080
```

**–°–æ–∑–¥–∞–π —Å–≤–æ–π custom exporter** –Ω–∞ Python:
```python
# custom_exporter.py
from prometheus_client import start_http_server, Gauge, Counter
import time
import random

# –°–æ–∑–¥–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
request_gauge = Gauge('app_requests_in_progress', 'Number of requests in progress')
request_counter = Counter('app_requests_total', 'Total number of requests')
error_counter = Counter('app_errors_total', 'Total number of errors')

def process_request():
    """–°–∏–º—É–ª–∏—Ä—É–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –∑–∞–ø—Ä–æ—Å–∞"""
    request_gauge.inc()
    request_counter.inc()
    
    # –°–ª—É—á–∞–π–Ω–∞—è –æ—à–∏–±–∫–∞
    if random.random() < 0.1:
        error_counter.inc()
    
    time.sleep(random.uniform(0.1, 0.5))
    request_gauge.dec()

if __name__ == '__main__':
    start_http_server(8000)
    print("Exporter started on port 8000")
    
    while True:
        process_request()
        time.sleep(random.uniform(0.5, 2))
```

---

## –ú–æ–¥—É–ª—å 2: Prometheus - —Å–±–æ—Ä –∏ —Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ (30 –º–∏–Ω—É—Ç)

### üéØ –ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞

**–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ Prometheus:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Targets   ‚îÇ ‚Üê HTTP Pull (scrape)
‚îÇ  (Metrics)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
   ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ Prom-  ‚îÇ
   ‚îÇ etheus ‚îÇ ‚Üê Time Series DB (TSDB)
   ‚îÇ Server ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
   ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ Alert- ‚îÇ
   ‚îÇ manager‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Prometheus config structure:**
```yaml
global:
  scrape_interval: 15s      # –ö–∞–∫ —á–∞—Å—Ç–æ —Å–æ–±–∏—Ä–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏
  evaluation_interval: 15s  # –ö–∞–∫ —á–∞—Å—Ç–æ –ø—Ä–æ–≤–µ—Ä—è—Ç—å –ø—Ä–∞–≤–∏–ª–∞

scrape_configs:
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']
```

**PromQL –æ—Å–Ω–æ–≤—ã:**
```promql
# Instant vector - —Ç–µ–∫—É—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
node_cpu_seconds_total

# Range vector - –∑–Ω–∞—á–µ–Ω–∏—è –∑–∞ –ø–µ—Ä–∏–æ–¥
node_cpu_seconds_total[5m]

# –§–∏–ª—å—Ç—Ä—ã
node_cpu_seconds_total{mode="idle"}
node_cpu_seconds_total{mode!="idle"}
node_cpu_seconds_total{mode=~"user|system"}

# –ê–≥—Ä–µ–≥–∞—Ü–∏—è
sum(node_cpu_seconds_total)
avg(node_cpu_seconds_total)
max(node_cpu_seconds_total)
min(node_cpu_seconds_total)
count(node_cpu_seconds_total)

# –ü–æ label
sum(node_cpu_seconds_total) by (mode)
sum(node_cpu_seconds_total) by (cpu)

# –§—É–Ω–∫—Ü–∏–∏
rate(node_cpu_seconds_total[5m])           # –°–∫–æ—Ä–æ—Å—Ç—å –∏–∑–º–µ–Ω–µ–Ω–∏—è
irate(node_cpu_seconds_total[5m])          # –ú–≥–Ω–æ–≤–µ–Ω–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å
increase(node_cpu_seconds_total[5m])       # –£–≤–µ–ª–∏—á–µ–Ω–∏–µ –∑–∞ –ø–µ—Ä–∏–æ–¥
delta(node_cpu_seconds_total[5m])          # –ò–∑–º–µ–Ω–µ–Ω–∏–µ
```

**–†–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã:**
```promql
# CPU utilization
100 - (avg by (instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)

# Memory usage %
(1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100

# Disk usage %
100 - ((node_filesystem_avail_bytes / node_filesystem_size_bytes) * 100)

# Network traffic
rate(node_network_receive_bytes_total[5m])
rate(node_network_transmit_bytes_total[5m])

# HTTP request rate
rate(http_requests_total[5m])

# Error rate
rate(http_requests_total{status=~"5.."}[5m]) / rate(http_requests_total[5m])

# Latency percentiles (–¥–ª—è histogram)
histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))
histogram_quantile(0.99, rate(http_request_duration_seconds_bucket[5m]))
```

**Metric types –≤ –¥–µ—Ç–∞–ª—è—Ö:**
```promql
# Counter - —Ç–æ–ª—å–∫–æ —Ä–∞—Å—Ç–µ—Ç
http_requests_total
# –ò—Å–ø–æ–ª—å–∑—É–π rate() –∏–ª–∏ increase()
rate(http_requests_total[5m])

# Gauge - –º–æ–∂–µ—Ç —Ä–∞—Å—Ç–∏ –∏ –ø–∞–¥–∞—Ç—å
node_memory_MemAvailable_bytes
# –ò—Å–ø–æ–ª—å–∑—É–π –Ω–∞–ø—Ä—è–º—É—é –∏–ª–∏ —Å —Ñ—É–Ω–∫—Ü–∏—è–º–∏ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏
avg(node_memory_MemAvailable_bytes)

# Histogram - —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π
http_request_duration_seconds_bucket
http_request_duration_seconds_sum
http_request_duration_seconds_count
# –ò—Å–ø–æ–ª—å–∑—É–π histogram_quantile()
histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))

# Summary - –ø—Ä–µ–¥—Ä–∞—Å—á–∏—Ç–∞–Ω–Ω—ã–µ –∫–≤–∞–Ω—Ç–∏–ª–∏
http_request_duration_seconds{quantile="0.95"}
```

**Recording rules** (–¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏):
```yaml
groups:
  - name: example
    interval: 30s
    rules:
    - record: job:node_cpu_utilization:avg
      expr: 100 - (avg by (job) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)
```

**Alerting rules:**
```yaml
groups:
  - name: alerts
    rules:
    - alert: HighCPUUsage
      expr: job:node_cpu_utilization:avg > 80
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "High CPU usage on {{ $labels.instance }}"
        description: "CPU usage is {{ $value }}%"
```

### üíª –ó–∞–¥–∞–Ω–∏–µ

–ù–∞—Å—Ç—Ä–æ–π –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π Prometheus:

1. **–°–æ–∑–¥–∞–π docker-compose.yml**:
```yaml
version: '3.8'

services:
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - ./alerts.yml:/etc/prometheus/alerts.yml
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--web.enable-lifecycle'
    restart: unless-stopped

  node-exporter:
    image: prom/node-exporter:latest
    container_name: node-exporter
    ports:
      - "9100:9100"
    command:
      - '--path.rootfs=/host'
    pid: host
    restart: unless-stopped
    volumes:
      - '/:/host:ro,rslave'

  cadvisor:
    image: gcr.io/cadvisor/cadvisor:latest
    container_name: cadvisor
    ports:
      - "8080:8080"
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
      - /dev/disk/:/dev/disk:ro
    privileged: true
    restart: unless-stopped

volumes:
  prometheus-data:
```

2. **–°–æ–∑–¥–∞–π prometheus.yml**:
```yaml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–∞–≤–∏–ª –∞–ª–µ—Ä—Ç–æ–≤
rule_files:
  - "alerts.yml"

scrape_configs:
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  - job_name: 'node-exporter'
    static_configs:
      - targets: ['node-exporter:9100']

  - job_name: 'cadvisor'
    static_configs:
      - targets: ['cadvisor:8080']
```

3. **–°–æ–∑–¥–∞–π alerts.yml**:
```yaml
groups:
  - name: system_alerts
    rules:
    - alert: HighCPUUsage
      expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "High CPU usage detected"
        description: "CPU usage is above 80% (current value: {{ $value }}%)"

    - alert: HighMemoryUsage
      expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 90
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "High memory usage detected"
        description: "Memory usage is above 90% (current value: {{ $value }}%)"

    - alert: DiskSpaceLow
      expr: (1 - (node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"})) * 100 > 85
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Low disk space"
        description: "Disk usage is above 85% (current value: {{ $value }}%)"

    - alert: InstanceDown
      expr: up == 0
      for: 1m
      labels:
        severity: critical
      annotations:
        summary: "Instance {{ $labels.instance }} down"
        description: "{{ $labels.instance }} has been down for more than 1 minute"
```

4. **–ó–∞–ø—É—Å—Ç–∏ stack**:
```bash
docker-compose up -d

# –ü—Ä–æ–≤–µ—Ä–∫–∞
docker-compose ps
curl http://localhost:9090/api/v1/targets
```

5. **–û—Ç–∫—Ä–æ–π Prometheus UI** –∏ –ø–æ–ø—Ä–æ–±—É–π –∑–∞–ø—Ä–æ—Å—ã:
```
–ü–µ—Ä–µ–π–¥–∏: http://localhost:9090

–ü–æ–ø—Ä–æ–±—É–π –∑–∞–ø—Ä–æ—Å—ã:
- node_cpu_seconds_total
- rate(node_cpu_seconds_total[5m])
- 100 - (avg(irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)
- node_memory_MemAvailable_bytes / 1024 / 1024 / 1024
```

### üöÄ –ë–æ–Ω—É—Å (–Ω–æ–≤–æ–µ)

**–ù–∞—Å—Ç—Ä–æ–π Service Discovery** –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è —Ü–µ–ª–µ–π:

**File-based SD** (`file_sd.json`):
```json
[
  {
    "targets": ["node-exporter:9100"],
    "labels": {
      "job": "node",
      "env": "production"
    }
  },
  {
    "targets": ["cadvisor:8080"],
    "labels": {
      "job": "containers",
      "env": "production"
    }
  }
]
```

–î–æ–±–∞–≤—å –≤ `prometheus.yml`:
```yaml
scrape_configs:
  - job_name: 'dynamic-targets'
    file_sd_configs:
      - files:
        - '/etc/prometheus/file_sd.json'
        refresh_interval: 30s
```

**–ù–∞—Å—Ç—Ä–æ–π Pushgateway** –¥–ª—è –º–µ—Ç—Ä–∏–∫ batch jobs:
```bash
docker run -d \
  --name pushgateway \
  -p 9091:9091 \
  prom/pushgateway

# Push –º–µ—Ç—Ä–∏–∫—É
echo "backup_duration_seconds 125.5" | curl --data-binary @- http://localhost:9091/metrics/job/backup/instance/db1

# –î–æ–±–∞–≤—å –≤ prometheus.yml
scrape_configs:
  - job_name: 'pushgateway'
    static_configs:
      - targets: ['pushgateway:9091']
    honor_labels: true
```

**Recording rules –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏**:
```yaml
# recording_rules.yml
groups:
  - name: performance_rules
    interval: 30s
    rules:
    # CPU utilization per instance
    - record: instance:node_cpu_utilization:rate5m
      expr: 100 - (avg by (instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)
    
    # Memory utilization per instance
    - record: instance:node_memory_utilization:ratio
      expr: 1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)
    
    # Request rate per job
    - record: job:http_requests:rate5m
      expr: sum(rate(http_requests_total[5m])) by (job)
```

---

## –ú–æ–¥—É–ª—å 3: Grafana - –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö (30 –º–∏–Ω—É—Ç)

### üéØ –ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞

**–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ Grafana:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Data     ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ Grafana  ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ Users    ‚îÇ
‚îÇ Sources  ‚îÇ      ‚îÇ Server   ‚îÇ      ‚îÇ          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
   ‚îÇ                    ‚îÇ
   ‚îÇ                    ‚îÇ
   ‚ñº                    ‚ñº
Prometheus       Dashboards
InfluxDB         Alerts
Elasticsearch    Users
Loki             Teams
```

**–¢–∏–ø—ã –ø–∞–Ω–µ–ª–µ–π:**
```
Graph        - –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä—è–¥—ã
Stat         - –û–¥–Ω–æ –∑–Ω–∞—á–µ–Ω–∏–µ
Gauge        - –®–∫–∞–ª–∞
Bar Gauge    - –ì–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω—ã–µ –ø–æ–ª–æ—Å–∫–∏
Table        - –¢–∞–±–ª–∏—Ü–∞
Heatmap      - –¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞
Logs         - –õ–æ–≥–∏
```

**–ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–∞—à–±–æ—Ä–¥–∞:**
```
Query      - –ò–∑ –¥–∞–Ω–Ω—ã—Ö (label_values(metric, label))
Custom     - –°–ø–∏—Å–æ–∫ –∑–Ω–∞—á–µ–Ω–∏–π
Constant   - –ö–æ–Ω—Å—Ç–∞–Ω—Ç–∞
Interval   - –í—Ä–µ–º–µ–Ω–Ω–æ–π –∏–Ω—Ç–µ—Ä–≤–∞–ª
Data source - –í—ã–±–æ—Ä –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö
```

**–ü–æ–ª–µ–∑–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ Grafana:**
```
$__interval        - –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –∏–Ω—Ç–µ—Ä–≤–∞–ª
$__rate_interval   - –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª –¥–ª—è rate()
$timeFilter        - –í—Ä–µ–º–µ–Ω–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä
$__from / $__to    - –ù–∞—á–∞–ª–æ/–∫–æ–Ω–µ—Ü –ø–µ—Ä–∏–æ–¥–∞

# –ü—Ä–∏–º–µ—Ä —Å –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
rate(http_requests_total{job="$job"}[$__rate_interval])
```

**Templating examples:**
```promql
# –ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è instance
label_values(node_cpu_seconds_total, instance)

# –ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è job
label_values(up, job)

# –ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è mountpoint
label_values(node_filesystem_size_bytes, mountpoint)

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ –∑–∞–ø—Ä–æ—Å–µ
node_filesystem_avail_bytes{instance="$instance", mountpoint="$mountpoint"}
```

**Alert channels:**
```
Email
Slack
PagerDuty
Webhook
Telegram
Discord
Teams
OpsGenie
```

**Dashboard best practices:**
```
1. –ò—Å–ø–æ–ª—å–∑—É–π Row –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ –ø–∞–Ω–µ–ª–µ–π
2. –î–æ–±–∞–≤–ª—è–π –æ–ø–∏—Å–∞–Ω–∏—è –∫ –ø–∞–Ω–µ–ª—è–º
3. –ò—Å–ø–æ–ª—å–∑—É–π –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –≥–∏–±–∫–æ—Å—Ç–∏
4. –£–∫–∞–∑—ã–≤–∞–π –µ–¥–∏–Ω–∏—Ü—ã –∏–∑–º–µ—Ä–µ–Ω–∏—è
5. –ò—Å–ø–æ–ª—å–∑—É–π —Ü–≤–µ—Ç–æ–≤—ã–µ –ø–æ—Ä–æ–≥–∏
6. –î–æ–±–∞–≤–ª—è–π —Å—Å—ã–ª–∫–∏ –Ω–∞ runbook'–∏
7. –ì—Ä—É–ø–ø–∏—Ä—É–π —Å–≤—è–∑–∞–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
8. –ò—Å–ø–æ–ª—å–∑—É–π consistent naming
```

### üíª –ó–∞–¥–∞–Ω–∏–µ

–ù–∞—Å—Ç—Ä–æ–π Grafana –∏ —Å–æ–∑–¥–∞–π dashboard:

1. **–î–æ–±–∞–≤—å Grafana –≤ docker-compose.yml**:
```yaml
  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - grafana-data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning
    restart: unless-stopped
    depends_on:
      - prometheus

volumes:
  grafana-data:
```

2. **–°–æ–∑–¥–∞–π provisioning –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏** (`grafana/provisioning/datasources/prometheus.yml`):
```yaml
apiVersion: 1

datasources:
  - name: Prometheus
    type: prometheus
    access: proxy
    url: http://prometheus:9090
    isDefault: true
    editable: true
```

3. **–°–æ–∑–¥–∞–π provisioning –¥–ª—è dashboard** (`grafana/provisioning/dashboards/dashboard.yml`):
```yaml
apiVersion: 1

providers:
  - name: 'Default'
    orgId: 1
    folder: ''
    type: file
    disableDeletion: false
    updateIntervalSeconds: 10
    allowUiUpdates: true
    options:
      path: /etc/grafana/provisioning/dashboards
```

4. **–ó–∞–ø—É—Å—Ç–∏ Grafana**:
```bash
# –°–æ–∑–¥–∞–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
mkdir -p grafana/provisioning/datasources
mkdir -p grafana/provisioning/dashboards

docker-compose up -d grafana

# –û—Ç–∫—Ä–æ–π –≤ –±—Ä–∞—É–∑–µ—Ä–µ
http://localhost:3000
# Login: admin
# Password: admin
```

5. **–°–æ–∑–¥–∞–π System Monitoring Dashboard** –≤—Ä—É—á–Ω—É—é:

**Panel 1: CPU Usage**
- Visualization: Time series
- Query: `100 - (avg(irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)`
- Legend: CPU Usage %
- Unit: Percent (0-100)
- Threshold: Yellow at 70, Red at 90

**Panel 2: Memory Usage**
- Visualization: Time series
- Query: `(1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100`
- Legend: Memory Usage %
- Unit: Percent (0-100)

**Panel 3: Disk Usage**
- Visualization: Gauge
- Query: `100 - ((node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"}) * 100)`
- Unit: Percent (0-100)
- Threshold: Green 0-70, Yellow 70-85, Red 85-100

**Panel 4: Network Traffic**
- Visualization: Time series
- Query A: `rate(node_network_receive_bytes_total[5m]) / 1024 / 1024`
- Query B: `rate(node_network_transmit_bytes_total[5m]) / 1024 / 1024`
- Unit: MB/s

**Panel 5: Top Processes by CPU**
- Visualization: Table
- Query: `topk(5, irate(process_cpu_seconds_total[5m]))`

6. **–°–æ–∑–¥–∞–π –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è dashboard**:
- Variable: instance
  - Type: Query
  - Query: `label_values(node_cpu_seconds_total, instance)`
  
–ò–∑–º–µ–Ω–∏ –∑–∞–ø—Ä–æ—Å—ã –Ω–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π:
```promql
100 - (avg(irate(node_cpu_seconds_total{instance="$instance", mode="idle"}[5m])) * 100)
```

### üöÄ –ë–æ–Ω—É—Å (–Ω–æ–≤–æ–µ)

**–°–æ–∑–¥–∞–π JSON dashboard —á–µ—Ä–µ–∑ provisioning** (`grafana/provisioning/dashboards/system-overview.json`):
```json
{
  "dashboard": {
    "title": "System Overview",
    "tags": ["system", "monitoring"],
    "timezone": "browser",
    "panels": [
      {
        "id": 1,
        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 0},
        "type": "timeseries",
        "title": "CPU Usage",
        "targets": [
          {
            "expr": "100 - (avg(irate(node_cpu_seconds_total{instance=\"$instance\",mode=\"idle\"}[5m])) * 100)",
            "legendFormat": "CPU Usage %"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "percent",
            "thresholds": {
              "steps": [
                {"value": 0, "color": "green"},
                {"value": 70, "color": "yellow"},
                {"value": 90, "color": "red"}
              ]
            }
          }
        }
      }
    ],
    "templating": {
      "list": [
        {
          "name": "instance",
          "type": "query",
          "datasource": "Prometheus",
          "query": "label_values(node_cpu_seconds_total, instance)",
          "refresh": 1
        }
      ]
    }
  }
}
```

**–ù–∞—Å—Ç—Ä–æ–π Alerting –≤ Grafana**:
1. Configuration ‚Üí Alerting ‚Üí Contact points
2. –°–æ–∑–¥–∞–π Email contact point
3. –°–æ–∑–¥–∞–π Alert rule:
   - Name: High CPU Alert
   - Query: `avg(irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100 < 20`
   - Condition: WHEN last() OF query(A) IS BELOW 20
   - For: 5m

**–£—Å—Ç–∞–Ω–æ–≤–∏ Grafana plugins**:
```bash
# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ —á–µ—Ä–µ–∑ UI
Configuration ‚Üí Plugins ‚Üí Search

# –ü–æ–ª–µ–∑–Ω—ã–µ –ø–ª–∞–≥–∏–Ω—ã:
- Pie Chart
- Worldmap Panel
- Clock Panel
- Status Panel

# –ß–µ—Ä–µ–∑ CLI (–≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–µ)
docker exec grafana grafana-cli plugins install grafana-piechart-panel
docker restart grafana
```

---
## –ú–æ–¥—É–ª—å 4: –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–∞—Ü–∏—è –ª–æ–≥–æ–≤ (30 –º–∏–Ω—É—Ç)

### üéØ –ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞

**–£—Ä–æ–≤–Ω–∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è:**

```
TRACE   - –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
DEBUG   - –û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
INFO    - –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
WARN    - –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
ERROR   - –û—à–∏–±–∫–∏, –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω—ã–µ –¥–ª—è —Ä–∞–±–æ—Ç—ã
FATAL   - –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏, –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –ø–∞–¥–∞–µ—Ç
```

**Structured logging (JSON):**

json

````json
{
  "timestamp": "2025-01-15T10:30:00Z",
  "level": "ERROR",
  "service": "api",
  "message": "Database connection failed",
  "error": "connection timeout",
  "user_id": "12345",
  "request_id": "abc-123",
  "duration_ms": 5000
}
````

**ELK Stack:**
```
Elasticsearch  - –•—Ä–∞–Ω–µ–Ω–∏–µ –∏ –ø–æ–∏—Å–∫
Logstash       - –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –ø–∞—Ä—Å–∏–Ω–≥
Kibana         - –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
```

**Alternative: Loki Stack:**
```
Loki           - –•—Ä–∞–Ω–µ–Ω–∏–µ –ª–æ–≥–æ–≤ (–∫–∞–∫ Prometheus –¥–ª—è –ª–æ–≥–æ–≤)
Promtail       - –ê–≥–µ–Ω—Ç —Å–±–æ—Ä–∞ (–∫–∞–∫ node-exporter)
Grafana        - –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
```

**Log aggregation patterns:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   App    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
                ‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îú‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ Log     ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ Centralized  ‚îÇ
‚îÇ   App    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚î§    ‚îÇ Shipper ‚îÇ    ‚îÇ Log Storage  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ   App    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
````

**–ü–æ–ª–µ–∑–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã –¥–ª—è –ª–æ–≥–æ–≤:**

bash

```bash
# journalctl (systemd)
journalctl -u nginx                  # –õ–æ–≥–∏ —Å–µ—Ä–≤–∏—Å–∞
journalctl -f                        # Follow –ª–æ–≥–∏
journalctl --since "1 hour ago"
journalctl -p err                    # –¢–æ–ª—å–∫–æ –æ—à–∏–±–∫–∏
journalctl --disk-usage              # –†–∞–∑–º–µ—Ä –ª–æ–≥–æ–≤

# Docker logs
docker logs <container>
docker logs -f <container>
docker logs --tail 100 <container>
docker logs --since 1h <container>

# –¢—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–µ –ª–æ–≥–∏ Linux
tail -f /var/log/syslog
tail -f /var/log/nginx/access.log
grep "ERROR" /var/log/application.log
zgrep "pattern" /var/log/old.log.gz  # –ü–æ–∏—Å–∫ –≤ —Å–∂–∞—Ç—ã—Ö –ª–æ–≥–∞—Ö

# –õ–æ–≥–∏ —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏
tail -f /var/log/app.log | ts '%Y-%m-%d %H:%M:%S'

# –ú–Ω–æ–≥–æ—Ñ–∞–π–ª–æ–≤—ã–π tail
multitail /var/log/nginx/access.log /var/log/nginx/error.log

# –ê–Ω–∞–ª–∏–∑ –ª–æ–≥–æ–≤
awk '{print $1}' access.log | sort | uniq -c | sort -rn | head -10  # Top 10 IP
grep "500" access.log | wc -l  # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ 500 –æ—à–∏–±–æ–∫
```

**Log rotation:**

bash

```bash
# logrotate –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (/etc/logrotate.d/app)
/var/log/app/*.log {
    daily                # –†–æ—Ç–∞—Ü–∏—è –∫–∞–∂–¥—ã–π –¥–µ–Ω—å
    rotate 7             # –•—Ä–∞–Ω–∏—Ç—å 7 –∞—Ä—Ö–∏–≤–æ–≤
    compress             # –°–∂–∏–º–∞—Ç—å —Å—Ç–∞—Ä—ã–µ
    delaycompress        # –ù–µ —Å–∂–∏–º–∞—Ç—å –ø–æ—Å–ª–µ–¥–Ω–∏–π
    missingok            # –ù–µ –æ—à–∏–±–∞—Ç—å—Å—è –µ—Å–ª–∏ —Ñ–∞–π–ª–∞ –Ω–µ—Ç
    notifempty           # –ù–µ —Ä–æ—Ç–∏—Ä–æ–≤–∞—Ç—å –ø—É—Å—Ç—ã–µ
    create 0640 app app  # –°–æ–∑–¥–∞—Ç—å —Å –ø—Ä–∞–≤–∞–º–∏
    sharedscripts
    postrotate
        systemctl reload app > /dev/null
    endscript
}

# –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
logrotate -d /etc/logrotate.d/app    # Dry run
logrotate -f /etc/logrotate.d/app    # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —Ä–æ—Ç–∞—Ü–∏—è
```

**–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è—Ö:**

**Python (structured logging):**

python

```python
import logging
import json
from datetime import datetime

class JSONFormatter(logging.Formatter):
    def format(self, record):
        log_data = {
            "timestamp": datetime.utcnow().isoformat(),
            "level": record.levelname,
            "service": "my-api",
            "message": record.getMessage(),
            "module": record.module,
            "function": record.funcName,
            "line": record.lineno
        }
        if record.exc_info:
            log_data["exception"] = self.formatException(record.exc_info)
        return json.dumps(log_data)

logging.basicConfig(level=logging.INFO)
handler = logging.StreamHandler()
handler.setFormatter(JSONFormatter())
logger = logging.getLogger()
logger.handlers = [handler]

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
logger.info("User logged in", extra={"user_id": "123", "ip": "192.168.1.1"})
logger.error("Database error", extra={"query": "SELECT *", "duration_ms": 5000})
```

**Node.js (Winston):**

javascript

```javascript
const winston = require('winston');

const logger = winston.createLogger({
  level: 'info',
  format: winston.format.combine(
    winston.format.timestamp(),
    winston.format.json()
  ),
  defaultMeta: { service: 'api-service' },
  transports: [
    new winston.transports.File({ filename: 'error.log', level: 'error' }),
    new winston.transports.File({ filename: 'combined.log' }),
    new winston.transports.Console({
      format: winston.format.simple()
    })
  ]
});

// –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
logger.info('User action', { user_id: '123', action: 'login' });
logger.error('Database error', { error: err.message, query: sql });
```

**Loki query patterns (LogQL):**

logql

```logql
# –ë–∞–∑–æ–≤—ã–π –ø–æ–∏—Å–∫
{job="varlogs"}

# –§–∏–ª—å—Ç—Ä—ã
{job="varlogs"} |= "error"                    # –°–æ–¥–µ—Ä–∂–∏—Ç "error"
{job="varlogs"} != "debug"                    # –ù–µ —Å–æ–¥–µ—Ä–∂–∏—Ç "debug"
{job="varlogs"} |~ "error|ERROR"              # Regex
{job="varlogs"} !~ "info|INFO"                # –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–π regex

# JSON parsing
{job="varlogs"} | json | level="error"
{job="varlogs"} | json | response_time > 1000

# –ê–≥—Ä–µ–≥–∞—Ü–∏—è
rate({job="varlogs"}[5m])                     # –õ–æ–≥-–∑–∞–ø–∏—Å–µ–π –≤ —Å–µ–∫—É–Ω–¥—É
sum(rate({job="varlogs"}[5m])) by (level)     # –ü–æ —É—Ä–æ–≤–Ω—é
count_over_time({job="varlogs"}[1h])          # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞ —á–∞—Å

# Pattern extraction
{job="varlogs"} | pattern `<_> level=<level> <_>`
{job="varlogs"} | regexp `status=(?P<status>\d+)`

# –ú–µ—Ç—Ä–∏–∫–∏ –∏–∑ –ª–æ–≥–æ–≤
sum(rate({job="api"} | json | status="500" [5m]))
```

**Elasticsearch query patterns:**

json

```json
// –ë–∞–∑–æ–≤—ã–π –ø–æ–∏—Å–∫
GET /logs-*/_search
{
  "query": {
    "match": {
      "message": "error"
    }
  }
}

// –í—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏–∞–ø–∞–∑–æ–Ω
GET /logs-*/_search
{
  "query": {
    "range": {
      "@timestamp": {
        "gte": "now-1h",
        "lte": "now"
      }
    }
  }
}

// –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å
GET /logs-*/_search
{
  "query": {
    "bool": {
      "must": [
        { "match": { "level": "ERROR" }},
        { "match": { "service": "api" }}
      ],
      "filter": [
        { "range": { "@timestamp": { "gte": "now-1h" }}}
      ]
    }
  },
  "aggs": {
    "errors_by_service": {
      "terms": { "field": "service.keyword" }
    }
  }
}
```

**Fluentd/Fluent Bit basics:**

conf

````conf
# Fluentd –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (fluent.conf)
<source>
  @type tail
  path /var/log/nginx/access.log
  pos_file /var/log/td-agent/nginx-access.log.pos
  tag nginx.access
  <parse>
    @type nginx
  </parse>
</source>

<filter nginx.access>
  @type record_transformer
  <record>
    hostname "#{Socket.gethostname}"
    service "nginx"
  </record>
</filter>

<match nginx.access>
  @type elasticsearch
  host elasticsearch
  port 9200
  index_name nginx-access
  type_name _doc
</match>

# Fluent Bit –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (–±–æ–ª–µ–µ –ª–µ–≥–∫–æ–≤–µ—Å–Ω–∞—è –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞)
[INPUT]
    Name              tail
    Path              /var/log/containers/*.log
    Parser            docker
    Tag               kube.*

[FILTER]
    Name                kubernetes
    Match               kube.*
    Kube_URL            https://kubernetes.default.svc:443
    Kube_CA_File        /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
    Kube_Token_File     /var/run/secrets/kubernetes.io/serviceaccount/token

[OUTPUT]
    Name              loki
    Match             *
    Host              loki
    Port              3100
````

**Log best practices:**
````
1. –í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–π structured logging (JSON)
2. –í–∫–ª—é—á–∞–π –∫–æ–Ω—Ç–µ–∫—Å—Ç: request_id, user_id, trace_id
3. –õ–æ–≥–∏—Ä—É–π –Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º —É—Ä–æ–≤–Ω–µ:
   - DEBUG: –¥–µ—Ç–∞–ª–∏ –¥–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏
   - INFO: –Ω–æ—Ä–º–∞–ª—å–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏
   - WARN: –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã
   - ERROR: –æ—à–∏–±–∫–∏ —Ç—Ä–µ–±—É—é—â–∏–µ –≤–Ω–∏–º–∞–Ω–∏—è
4. –ù–µ –ª–æ–≥–∏—Ä—É–π sensitive data (–ø–∞—Ä–æ–ª–∏, —Ç–æ–∫–µ–Ω—ã, PII)
5. –ò—Å–ø–æ–ª—å–∑—É–π correlation IDs –¥–ª—è —Ç—Ä–µ–π—Å–∏–Ω–≥–∞
6. –†–æ—Ç–∏—Ä—É–π –ª–æ–≥–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
7. –¶–µ–Ω—Ç—Ä–∞–ª–∏–∑—É–π –ª–æ–≥–∏ —Å–æ –≤—Å–µ—Ö —Å–∏—Å—Ç–µ–º
8. –ù–∞—Å—Ç—Ä–æ–π –∞–ª–µ—Ä—Ç—ã –Ω–∞ –∫—Ä–∏—Ç–∏—á–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
````

### üíª –ó–∞–¥–∞–Ω–∏–µ

–ù–∞—Å—Ç—Ä–æ–π —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å Loki:

1. **–°–æ–∑–¥–∞–π docker-compose.yml –¥–ª—è Loki stack**:

yaml

```yaml
version: '3.8'

services:
  loki:
    image: grafana/loki:2.9.3
    container_name: loki
    ports:
      - "3100:3100"
    volumes:
      - ./loki-config.yml:/etc/loki/local-config.yaml
      - loki-data:/loki
    command: -config.file=/etc/loki/local-config.yaml
    restart: unless-stopped

  promtail:
    image: grafana/promtail:2.9.3
    container_name: promtail
    volumes:
      - ./promtail-config.yml:/etc/promtail/config.yml
      - /var/log:/var/log:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
    command: -config.file=/etc/promtail/config.yml
    restart: unless-stopped
    depends_on:
      - loki

  grafana:
    image: grafana/grafana:10.2.3
    container_name: grafana-logs
    ports:
      - "3001:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - grafana-logs-data:/var/lib/grafana
      - ./grafana-datasources.yml:/etc/grafana/provisioning/datasources/datasources.yml
    restart: unless-stopped
    depends_on:
      - loki

  # –¢–µ—Å—Ç–æ–≤–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∏—Ä—É—é—â–µ–µ –ª–æ–≥–∏
  log-generator:
    image: mingrammer/flog
    container_name: log-generator
    command: -f json -l -d 1 -s 1
    restart: unless-stopped

volumes:
  loki-data:
  grafana-logs-data:
```

2. **–°–æ–∑–¥–∞–π loki-config.yml**:

yaml

```yaml
auth_enabled: false

server:
  http_listen_port: 3100
  grpc_listen_port: 9096

common:
  path_prefix: /loki
  storage:
    filesystem:
      chunks_directory: /loki/chunks
      rules_directory: /loki/rules
  replication_factor: 1
  ring:
    instance_addr: 127.0.0.1
    kvstore:
      store: inmemory

query_range:
  results_cache:
    cache:
      embedded_cache:
        enabled: true
        max_size_mb: 100

schema_config:
  configs:
    - from: 2020-10-24
      store: boltdb-shipper
      object_store: filesystem
      schema: v11
      index:
        prefix: index_
        period: 24h

ruler:
  alertmanager_url: http://localhost:9093

# Retention (—É–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö –ª–æ–≥–æ–≤)
limits_config:
  retention_period: 168h  # 7 –¥–Ω–µ–π
```

3. **–°–æ–∑–¥–∞–π promtail-config.yml**:

yaml

```yaml
server:
  http_listen_port: 9080
  grpc_listen_port: 0

positions:
  filename: /tmp/positions.yaml

clients:
  - url: http://loki:3100/loki/api/v1/push

scrape_configs:
  # Docker –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã
  - job_name: docker
    docker_sd_configs:
      - host: unix:///var/run/docker.sock
        refresh_interval: 5s
    relabel_configs:
      - source_labels: ['__meta_docker_container_name']
        regex: '/(.*)'
        target_label: 'container'
      - source_labels: ['__meta_docker_container_log_stream']
        target_label: 'stream'
    pipeline_stages:
      - json:
          expressions:
            level: level
            message: message
            timestamp: timestamp
      - labels:
          level:
          stream:

  # –°–∏—Å—Ç–µ–º–Ω—ã–µ –ª–æ–≥–∏
  - job_name: system
    static_configs:
      - targets:
          - localhost
        labels:
          job: varlogs
          __path__: /var/log/*.log

  # Application logs (—Å –ø–∞—Ä—Å–∏–Ω–≥–æ–º JSON)
  - job_name: app-logs
    static_configs:
      - targets:
          - localhost
        labels:
          job: app
          __path__: /var/log/app/*.log
    pipeline_stages:
      - json:
          expressions:
            timestamp: timestamp
            level: level
            service: service
            message: message
            user_id: user_id
      - timestamp:
          source: timestamp
          format: RFC3339
      - labels:
          level:
          service:
```

4. **–°–æ–∑–¥–∞–π grafana-datasources.yml**:

yaml

```yaml
apiVersion: 1

datasources:
  - name: Loki
    type: loki
    access: proxy
    url: http://loki:3100
    isDefault: true
    editable: true
    jsonData:
      maxLines: 1000
```

5. **–ó–∞–ø—É—Å—Ç–∏ stack**:

bash

```bash
# –°–æ–∑–¥–∞–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
mkdir -p logs/app

# –ó–∞–ø—É—Å—Ç–∏
docker-compose up -d

# –ü—Ä–æ–≤–µ—Ä—å —Å—Ç–∞—Ç—É—Å
docker-compose ps
curl http://localhost:3100/ready

# –ü—Ä–æ–≤–µ—Ä—å –ª–æ–≥–∏
curl http://localhost:3100/loki/api/v1/label
```

6. **–°–æ–∑–¥–∞–π Python —Å–∫—Ä–∏–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ—Å—Ç–æ–≤—ã—Ö –ª–æ–≥–æ–≤** (`generate_logs.py`):

python

````python
#!/usr/bin/env python3
import json
import random
import time
from datetime import datetime

levels = ['DEBUG', 'INFO', 'WARN', 'ERROR']
services = ['api', 'frontend', 'database', 'cache']
messages = {
    'DEBUG': ['Query executed', 'Cache hit', 'Function called'],
    'INFO': ['User logged in', 'Request processed', 'Task completed'],
    'WARN': ['Slow query detected', 'High memory usage', 'Rate limit approaching'],
    'ERROR': ['Database connection failed', 'Timeout occurred', '500 Internal Server Error']
}

def generate_log():
    level = random.choices(levels, weights=[10, 60, 20, 10])[0]
    service = random.choice(services)
    message = random.choice(messages[level])
    
    log_entry = {
        "timestamp": datetime.utcnow().isoformat() + "Z",
        "level": level,
        "service": service,
        "message": message,
        "request_id": f"req-{random.randint(1000, 9999)}",
        "user_id": f"user-{random.randint(1, 100)}",
        "duration_ms": random.randint(10, 5000) if level in ['WARN', 'ERROR'] else random.randint(10, 500)
    }
    
    return json.dumps(log_entry)

if __name__ == "__main__":
    print("Starting log generation...")
    while True:
        log = generate_log()
        print(log)
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Ñ–∞–π–ª
        with open('/var/log/app/application.log', 'a') as f:
            f.write(log + '\n')
        time.sleep(random.uniform(0.1, 2))
````

7. **–û—Ç–∫—Ä–æ–π Grafana –∏ —Å–æ–∑–¥–∞–π dashboard**:
````
URL: http://localhost:3001
Login: admin
Password: admin

–ü—Ä–∏–º–µ—Ä—ã –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è –ø–∞–Ω–µ–ª–µ–π:

# –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–æ–≥–æ–≤ –ø–æ —É—Ä–æ–≤–Ω—é
sum(rate({job="docker"}[1m])) by (level)

# –õ–æ–≥–∏ —Å –æ—à–∏–±–∫–∞–º–∏
{job="docker"} |= "ERROR"

# Top services –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –ª–æ–≥–æ–≤
topk(5, sum(rate({job="docker"}[5m])) by (container))

# –õ–æ–≥–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Å–µ—Ä–≤–∏—Å–∞
{job="docker", container="log-generator"}

# –ú–µ–¥–ª–µ–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã (–µ—Å–ª–∏ duration > 1000ms)
{job="docker"} | json | duration_ms > 1000
````

8. **–ü—Ä–æ–≤–µ—Ä—å —Ä–∞–±–æ—Ç—É**:

bash

```bash
# –õ–æ–≥–∏ –≤ Loki
curl -G -s "http://localhost:3100/loki/api/v1/query" \
  --data-urlencode 'query={job="docker"}' | jq

# –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–æ–≥–æ–≤
curl -G -s "http://localhost:3100/loki/api/v1/query" \
  --data-urlencode 'query=count_over_time({job="docker"}[1h])' | jq

# –ú–µ—Ç—Ä–∏–∫–∏ Promtail
curl http://localhost:9080/metrics
```

### üöÄ –ë–æ–Ω—É—Å (–Ω–æ–≤–æ–µ)

**1. –ù–∞—Å—Ç—Ä–æ–π ELK Stack –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è**:

`docker-compose-elk.yml`:

yaml

```yaml
version: '3.8'

services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.3
    container_name: elasticsearch
    environment:
      - discovery.type=single-node
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
      - xpack.security.enabled=false
    ports:
      - "9200:9200"
    volumes:
      - elasticsearch-data:/usr/share/elasticsearch/data
    restart: unless-stopped

  logstash:
    image: docker.elastic.co/logstash/logstash:8.11.3
    container_name: logstash
    volumes:
      - ./logstash.conf:/usr/share/logstash/pipeline/logstash.conf
    ports:
      - "5000:5000"
      - "9600:9600"
    environment:
      - "LS_JAVA_OPTS=-Xmx256m -Xms256m"
    depends_on:
      - elasticsearch
    restart: unless-stopped

  kibana:
    image: docker.elastic.co/kibana/kibana:8.11.3
    container_name: kibana
    ports:
      - "5601:5601"
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    depends_on:
      - elasticsearch
    restart: unless-stopped

  filebeat:
    image: docker.elastic.co/beats/filebeat:8.11.3
    container_name: filebeat
    user: root
    volumes:
      - ./filebeat.yml:/usr/share/filebeat/filebeat.yml:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
    command: filebeat -e -strict.perms=false
    depends_on:
      - elasticsearch
    restart: unless-stopped

volumes:
  elasticsearch-data:
```

`logstash.conf`:

conf

```conf
input {
  beats {
    port => 5000
  }
}

filter {
  if [message] =~ /^\{.*\}$/ {
    json {
      source => "message"
    }
  }
  
  date {
    match => ["timestamp", "ISO8601"]
    target => "@timestamp"
  }
  
  mutate {
    remove_field => ["message"]
  }
}

output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "logs-%{+YYYY.MM.dd}"
  }
  
  stdout {
    codec => rubydebug
  }
}
```

`filebeat.yml`:

yaml

```yaml
filebeat.inputs:
  - type: container
    paths:
      - '/var/lib/docker/containers/*/*.log'
    processors:
      - add_docker_metadata:
          host: "unix:///var/run/docker.sock"

output.logstash:
  hosts: ["logstash:5000"]

logging.level: info
```

**2. –°–æ–∑–¥–∞–π log alerting rules**:

–î–ª—è Loki (—á–µ—Ä–µ–∑ Grafana Alerting):

yaml

```yaml
# Alert: High Error Rate
groups:
  - name: log_alerts
    interval: 1m
    rules:
      - alert: HighErrorRate
        expr: |
          sum(rate({job="docker"} |= "ERROR" [5m])) > 10
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value }} errors/sec"

      - alert: ServiceDown
        expr: |
          absent(rate({job="docker", container="api"}[5m]))
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Service {{ $labels.container }} is down"
```

**3. –ù–∞—Å—Ç—Ä–æ–π log parsing –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤**:

Nginx access log parsing –≤ Promtail:

yaml

````yaml
- job_name: nginx
  static_configs:
    - targets:
        - localhost
      labels:
        job: nginx
        __path__: /var/log/nginx/access.log
  pipeline_stages:
    - regex:
        expression: '^(?P<remote_addr>[\w\.]+) - (?P<remote_user>[^ ]*) \[(?P<time_local>.*)\] "(?P<method>[^ ]*) (?P<request>[^ ]*) (?P<protocol>[^ ]*)" (?P<status>[\d]+) (?P<body_bytes_sent>[\d]+) "(?P<http_referer>[^"]*)" "(?P<http_user_agent>[^"]*)"'
    - labels:
        method:
        status:
    - timestamp:
        source: time_local
        format: 02/Jan/2006:15:04:05 -0700
````

**4. –°–æ–∑–¥–∞–π log analysis dashboard**:

Grafana panels –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ª–æ–≥–æ–≤:
````
Panel 1: Log volume over time
Query: sum(rate({job="docker"}[1m])) by (level)
Visualization: Time series

Panel 2: Top error messages
Query: topk(10, sum(rate({job="docker"} |= "ERROR" [5m])) by (message))
Visualization: Bar chart

Panel 3: Logs table
Query: {job="docker"}
Visualization: Logs

Panel 4: Response time distribution
Query: quantile_over_time(0.95, {job="docker"} | json | unwrap duration_ms [5m])
Visualization: Gauge

Panel 5: Service health
Query: count(rate({job="docker"}[1m])) by (container)
Visualization: Stat
````

**5. –ù–∞—Å—Ç—Ä–æ–π log sampling –¥–ª—è –≤—ã—Å–æ–∫–æ–Ω–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —Å–∏—Å—Ç–µ–º**:

yaml

```yaml
# Promtail sampling configuration
scrape_configs:
  - job_name: high-volume-app
    static_configs:
      - targets:
          - localhost
        labels:
          job: app
          __path__: /var/log/app/*.log
    pipeline_stages:
      # –°–æ—Ö—Ä–∞–Ω—è–π —Ç–æ–ª—å–∫–æ ERROR –∏ WARN + sample INFO/DEBUG
      - match:
          selector: '{job="app"}'
          stages:
            - json:
                expressions:
                  level: level
            - drop:
                expression: "level == 'DEBUG' and __sample__ > 0.1"  # 10% DEBUG
            - drop:
                expression: "level == 'INFO' and __sample__ > 0.5"   # 50% INFO
```

**6. Log retention –∏ archiving**:

yaml

```yaml
# Loki retention config
limits_config:
  retention_period: 168h  # 7 –¥–Ω–µ–π

# Compactor –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ —Å—Ç–∞—Ä—ã—Ö –ª–æ–≥–æ–≤
compactor:
  working_directory: /loki/compactor
  shared_store: filesystem
  compaction_interval: 10m
  retention_enabled: true
  retention_delete_delay: 2h
  retention_delete_worker_count: 150
```

**7. –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Alertmanager**:

yaml

```yaml
# Loki ruler config –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –∞–ª–µ—Ä—Ç–æ–≤
ruler:
  storage:
    type: local
    local:
      directory: /loki/rules
  rule_path: /tmp/rules
  alertmanager_url: http://alertmanager:9093
  ring:
    kvstore:
      store: inmemory
  enable_api: true
```

Rules file (`/loki/rules/alerts.yml`):

yaml

````yaml
groups:
  - name: logs
    interval: 1m
    rules:
      - alert: HighErrorRate
        expr: |
          sum(rate({job="docker"} |= "ERROR" [5m])) > 1
        for: 5m
        labels:
          severity: warning
          team: backend
        annotations:
          summary: "High error rate in {{ $labels.container }}"
          description: "Error rate: {{ $value }} errors/sec"
          dashboard: "http://grafana:3000/d/logs"
````

**8. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ Loki vs ELK**:
```
Loki –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:
‚úÖ –õ–µ–≥–∫–æ–≤–µ—Å–Ω—ã–π (–º–µ–Ω—å—à–µ —Ä–µ—Å—É—Ä—Å–æ–≤)
‚úÖ –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Prometheus/Grafana
‚úÖ –ü—Ä–æ—Å—Ç–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
‚úÖ –•–æ—Ä–æ—à–æ –¥–ª—è Kubernetes
‚úÖ –î–µ—à–µ–≤–ª–µ –≤ —ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏–∏

ELK –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:
‚úÖ –ú–æ—â–Ω—ã–π –ø–æ–ª–Ω–æ—Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫
‚úÖ –ë–æ–≥–∞—Ç—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏
‚úÖ Advanced analytics
‚úÖ –ë–æ–ª—å—à–µ –ø–ª–∞–≥–∏–Ω–æ–≤ –∏ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–π
‚úÖ Mature ecosystem

–í—ã–±–æ—Ä:
- Loki: –¥–ª—è –º–µ—Ç—Ä–∏–∫-–æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø–æ–¥—Ö–æ–¥–∞, K8s
- ELK: –¥–ª—è —Å–ª–æ–∂–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –ª–æ–≥–æ–≤, compliance
````

---

## –ò—Ç–æ–≥–∏ –º–æ–¥—É–ª—è 4

–ü–æ—Å–ª–µ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏—è —ç—Ç–æ–≥–æ –º–æ–¥—É–ª—è —Ç—ã –¥–æ–ª–∂–µ–Ω —É–º–µ—Ç—å:

‚úÖ –ü–æ–Ω–∏–º–∞—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã –∫ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—é 
‚úÖ –ù–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å Loki + Promtail + Grafana 
‚úÖ –ü–∏—Å–∞—Ç—å LogQL –∑–∞–ø—Ä–æ—Å—ã 
‚úÖ –ü–∞—Ä—Å–∏—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –ª–æ–≥–æ–≤ 
‚úÖ –°–æ–∑–¥–∞–≤–∞—Ç—å –¥–∞—à–±–æ—Ä–¥—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ª–æ–≥–æ–≤ 
‚úÖ –ù–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å –∞–ª–µ—Ä—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –ª–æ–≥–æ–≤ 
‚úÖ –£–ø—Ä–∞–≤–ª—è—Ç—å retention –∏ rotation 
‚úÖ –°—Ä–∞–≤–Ω–∏–≤–∞—Ç—å Loki –∏ ELK —Å—Ç–µ–∫–∏


## –ú–æ–¥—É–ª—å 5: Alerting –∏ Notification - —É–º–Ω—ã–µ –∞–ª–µ—Ä—Ç—ã –±–µ–∑ alert fatigue (35 –º–∏–Ω—É—Ç)

### üéØ –ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞

**–§–∏–ª–æ—Å–æ—Ñ–∏—è –∞–ª–µ—Ä—Ç–∏–Ω–≥–∞:**

```
–•–æ—Ä–æ—à–∏–π –∞–ª–µ—Ä—Ç = Actionable + Urgent + Real Problem

‚ùå –ü–ª–æ—Ö–æ–π –∞–ª–µ—Ä—Ç: "CPU usage > 80%"
‚úÖ –•–æ—Ä–æ—à–∏–π –∞–ª–µ—Ä—Ç: "API response time > 1s for 5min, affecting users"

–ü—Ä–∞–≤–∏–ª–æ: –ï—Å–ª–∏ –∞–ª–µ—Ä—Ç –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –¥–µ–π—Å—Ç–≤–∏—è –ø—Ä—è–º–æ —Å–µ–π—á–∞—Å - —ç—Ç–æ –Ω–µ –∞–ª–µ—Ä—Ç, —ç—Ç–æ –º–µ—Ç—Ä–∏–∫–∞
```

**–£—Ä–æ–≤–Ω–∏ severity:**

```
CRITICAL (P1)  - –ü–æ–ª–Ω—ã–π outage, —Ç—Ä–µ–±—É–µ—Ç –Ω–µ–º–µ–¥–ª–µ–Ω–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π
                 –ü—Ä–∏–º–µ—Ä: —Å–µ—Ä–≤–∏—Å –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –ø–æ—Ç–µ—Ä—è –¥–∞–Ω–Ω—ã—Ö

WARNING (P2)   - –î–µ–≥—Ä–∞–¥–∞—Ü–∏—è —Å–µ—Ä–≤–∏—Å–∞, —Ç—Ä–µ–±—É–µ—Ç –¥–µ–π—Å—Ç–≤–∏–π –≤ –±–ª–∏–∂–∞–π—à–µ–µ –≤—Ä–µ–º—è
                 –ü—Ä–∏–º–µ—Ä: –≤—ã—Å–æ–∫–∞—è latency, —Å–∫–æ—Ä–æ –∑–∞–∫–æ–Ω—á–∏—Ç—Å—è –º–µ—Å—Ç–æ

INFO (P3)      - –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–µ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ, –Ω–µ —Ç—Ä–µ–±—É–µ—Ç —Å—Ä–æ—á–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏–π
                 –ü—Ä–∏–º–µ—Ä: deployment –∑–∞–≤–µ—Ä—à–µ–Ω, –ø–ª–∞–Ω–æ–≤–æ–µ –æ–±—Å–ª—É–∂–∏–≤–∞–Ω–∏–µ
```

**Alertmanager –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Prometheus  ‚îÇ‚îÄ‚îê
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
                ‚îú‚îÄ‚îÄ‚ñ∫ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ    ‚îÇ Alertmanager ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ Receivers   ‚îÇ
‚îÇ    Loki     ‚îÇ‚îÄ‚î§    ‚îÇ              ‚îÇ     ‚îÇ (Slack/etc) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ    ‚îÇ - Grouping   ‚îÇ     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚îÇ    ‚îÇ - Inhibition ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ    ‚îÇ - Silencing  ‚îÇ
‚îÇ   Custom    ‚îÇ‚îÄ‚îò    ‚îÇ - Routing    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Alert states:**

```
Inactive  ‚îÄ‚îÄ‚ñ∫ Pending  ‚îÄ‚îÄ‚ñ∫ Firing  ‚îÄ‚îÄ‚ñ∫ Resolved
               (for)         ‚îÇ
                            ‚Üì
                         Silenced
```

**–ö–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏:**

**1. Grouping** - –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø–æ—Ö–æ–∂–∏—Ö –∞–ª–µ—Ä—Ç–æ–≤:

yaml

```yaml
# –í–º–µ—Å—Ç–æ 100 –∞–ª–µ—Ä—Ç–æ–≤ –æ down –Ω–æ–¥–∞—Ö
# –û–¥–∏–Ω grouped –∞–ª–µ—Ä—Ç: "50 nodes are down in cluster-prod"
route:
  group_by: ['alertname', 'cluster']
  group_wait: 30s
  group_interval: 5m
```

**2. Inhibition** - –ø–æ–¥–∞–≤–ª–µ–Ω–∏–µ –∑–∞–≤–∏—Å–∏–º—ã—Ö –∞–ª–µ—Ä—Ç–æ–≤:

yaml

```yaml
# –ï—Å–ª–∏ –∫–ª–∞—Å—Ç–µ—Ä down, –Ω–µ —Å–ª–∞—Ç—å –∞–ª–µ—Ä—Ç—ã –æ –∫–∞–∂–¥–æ–º —Å–µ—Ä–≤–∏—Å–µ –≤ –Ω–µ–º
inhibit_rules:
  - source_match:
      alertname: ClusterDown
    target_match:
      cluster: production
    equal: ['cluster']
```

**3. Silencing** - –≤—Ä–µ–º–µ–Ω–Ω–æ–µ –æ—Ç–∫–ª—é—á–µ–Ω–∏–µ –∞–ª–µ—Ä—Ç–æ–≤:

bash

```bash
# –í–æ –≤—Ä–µ–º—è maintenance window
amtool silence add alertname=HighCPU --duration=2h --comment="Planned maintenance"
```

**4. Routing** - –º–∞—Ä—à—Ä—É—Ç–∏–∑–∞—Ü–∏—è –ø–æ –∫–æ–º–∞–Ω–¥–∞–º/–∫–∞–Ω–∞–ª–∞–º:

yaml

```yaml
route:
  routes:
    - match:
        team: backend
      receiver: backend-team
    - match:
        severity: critical
      receiver: pagerduty
```

**Prometheus alerting rules —Å—Ç—Ä—É–∫—Ç—É—Ä–∞:**

yaml

```yaml
groups:
  - name: example
    interval: 30s
    rules:
    - alert: HighErrorRate
      expr: |
        rate(http_requests_total{status=~"5.."}[5m]) 
        / 
        rate(http_requests_total[5m]) 
        > 0.05
      for: 5m
      labels:
        severity: warning
        team: backend
        service: api
      annotations:
        summary: "High error rate on {{ $labels.instance }}"
        description: "Error rate is {{ $value | humanizePercentage }}"
        dashboard: "https://grafana.com/d/api-dashboard"
        runbook: "https://wiki.com/runbooks/high-error-rate"
```

**Alert best practices:**

**1. –ù–∞–∑–≤–∞–Ω–∏–µ –∞–ª–µ—Ä—Ç–∞ (–≥–æ–≤–æ—Ä—è—â–µ–µ):**

yaml

```yaml
‚ùå alert: HighCPU
‚úÖ alert: InstanceHighCPUUsage

‚ùå alert: Error
‚úÖ alert: APIHighErrorRate5xx
```

**2. For clause (–∏–∑–±–µ–≥–∞–µ–º flapping):**

yaml

```yaml
# –ù–µ –∞–ª–µ—Ä—Ç–∏—Ç—å –Ω–∞ –∫—Ä–∞—Ç–∫–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Å–ø–∞–π–∫–∏
for: 5m  # –ê–ª–µ—Ä—Ç —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —É—Å–ª–æ–≤–∏–µ true 5 –º–∏–Ω—É—Ç –ø–æ–¥—Ä—è–¥
```

**3. –ê–Ω–Ω–æ—Ç–∞—Ü–∏–∏ (–ø–æ–ª–µ–∑–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç):**

yaml

```yaml
annotations:
  summary: "–ö—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º—ã"
  description: "{{ $labels.instance }} has {{ $value }}% CPU usage"
  dashboard: "–°—Å—ã–ª–∫–∞ –Ω–∞ dashboard"
  runbook: "–°—Å—ã–ª–∫–∞ –Ω–∞ runbook —Å —Ä–µ—à–µ–Ω–∏–µ–º"
  impact: "Users experiencing slow response times"
```

**4. Labels –¥–ª—è routing:**

yaml

```yaml
labels:
  severity: critical|warning|info
  team: backend|frontend|data
  service: api|web|worker
  environment: prod|staging|dev
```

**–¢–∏–ø–∏—á–Ω—ã–µ –∞–ª–µ—Ä—Ç—ã –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã:**

yaml

```yaml
# Instance down
- alert: InstanceDown
  expr: up == 0
  for: 5m
  labels:
    severity: critical
  annotations:
    summary: "Instance {{ $labels.instance }} down"

# High CPU
- alert: HighCPUUsage
  expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
  for: 10m
  labels:
    severity: warning

# High Memory
- alert: HighMemoryUsage
  expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 90
  for: 5m
  labels:
    severity: warning

# Disk space low
- alert: DiskSpaceLow
  expr: (1 - (node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"})) * 100 > 85
  for: 5m
  labels:
    severity: warning

# High disk I/O
- alert: HighDiskIO
  expr: rate(node_disk_io_time_seconds_total[5m]) > 0.9
  for: 10m
  labels:
    severity: warning
```

**–¢–∏–ø–∏—á–Ω—ã–µ –∞–ª–µ—Ä—Ç—ã –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π:**

yaml

````yaml
# High error rate
- alert: HighErrorRate
  expr: |
    sum(rate(http_requests_total{status=~"5.."}[5m])) by (service)
    /
    sum(rate(http_requests_total[5m])) by (service)
    > 0.05
  for: 5m
  labels:
    severity: critical

# Slow response time
- alert: SlowResponseTime
  expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 1
  for: 10m
  labels:
    severity: warning

# High request rate (DDoS?)
- alert: UnusuallyHighTraffic
  expr: sum(rate(http_requests_total[5m])) > 1000
  for: 5m
  labels:
    severity: warning

# Database connection pool exhausted
- alert: DatabaseConnectionPoolNearLimit
  expr: database_connections_active / database_connections_max > 0.9
  for: 5m
  labels:
    severity: warning

# Queue backed up
- alert: QueueBacklog
  expr: queue_depth > 1000
  for: 10m
  labels:
    severity: warning

# Certificate expiring soon
- alert: CertificateExpiringSoon
  expr: (ssl_certificate_expiry_timestamp - time()) / 86400 < 30
  for: 1h
  labels:
    severity: warning
````

**Alert fatigue - –∫–∞–∫ –∏–∑–±–µ–∂–∞—Ç—å:**
```
–ü—Ä–æ–±–ª–µ–º–∞: –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –∞–ª–µ—Ä—Ç–æ–≤ ‚Üí –∏–≥–Ω–æ—Ä–∏—Ä—É—é—Ç—Å—è ‚Üí –ø—Ä–æ–ø—É—â–µ–Ω—ã —Ä–µ–∞–ª—å–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã

–†–µ—à–µ–Ω–∏—è:
1. ‚úÖ –ê–ª–µ—Ä—Ç–∏—Ç—å —Ç–æ–ª—å–∫–æ –Ω–∞ —Å–∏–º–ø—Ç–æ–º—ã, –∞ –Ω–µ –ø—Ä–∏—á–∏–Ω—ã
   ‚ùå CPU high, Memory high, Disk full (–ø—Ä–∏—á–∏–Ω—ã)
   ‚úÖ Users can't login, API is slow (—Å–∏–º–ø—Ç–æ–º—ã)

2. ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–π –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ threshold
   ‚ùå CPU > 50% (—Å–ª–∏—à–∫–æ–º —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ)
   ‚úÖ CPU > 80% for 10 minutes (—Ä–∞–∑—É–º–Ω–æ)

3. ‚úÖ –ì—Ä—É–ø–ø–∏—Ä—É–π –ø–æ—Ö–æ–∂–∏–µ –∞–ª–µ—Ä—Ç—ã
   ‚ùå 50 –∞–ª–µ—Ä—Ç–æ–≤ "pod X down"
   ‚úÖ 1 –∞–ª–µ—Ä—Ç "50 pods down in namespace Y"

4. ‚úÖ Inhibition rules –¥–ª—è –∑–∞–≤–∏—Å–∏–º—ã—Ö –∞–ª–µ—Ä—Ç–æ–≤
   –ï—Å–ª–∏ –∫–ª–∞—Å—Ç–µ—Ä down ‚Üí –Ω–µ —Å–ª–∞—Ç—å –∞–ª–µ—Ä—Ç—ã –æ —Å–µ—Ä–≤–∏—Å–∞—Ö

5. ‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω–æ–µ –≤—Ä–µ–º—è —Å—É—Ç–æ–∫
   Non-critical –∞–ª–µ—Ä—Ç—ã —Ç–æ–ª—å–∫–æ –≤ —Ä–∞–±–æ—á–µ–µ –≤—Ä–µ–º—è

6. ‚úÖ SLO-based alerting
   –ê–ª–µ—Ä—Ç–∏—Ç—å –∫–æ–≥–¥–∞ error budget –∏—Å—á–µ—Ä–ø—ã–≤–∞–µ—Ç—Å—è

7. ‚úÖ –†–µ–≥—É–ª—è—Ä–Ω—ã–π review –∏ cleanup
   –£–¥–∞–ª—è–π –Ω–µ–∞–∫—Ç—É–∞–ª—å–Ω—ã–µ –∞–ª–µ—Ä—Ç—ã
```

**Notification channels:**
````
–ö—Ä–∏—Ç–∏—á–Ω–æ—Å—Ç—å    –ö–∞–Ω–∞–ª           –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å
‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
Critical       PagerDuty       Production outage, —Ç—Ä–µ–±—É–µ—Ç –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è
               OpsGenie        
               
Warning        Slack           –¢—Ä–µ–±—É–µ—Ç –≤–Ω–∏–º–∞–Ω–∏—è, –Ω–æ –Ω–µ —Å—Ä–æ—á–Ω–æ
               Teams           
               
Info           Email           FYI, —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞, –æ—Ç—á–µ—Ç—ã
               Webhook         –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –¥—Ä—É–≥–∏–º–∏ —Å–∏—Å—Ç–µ–º–∞–º–∏
               
–í—Å–µ —É—Ä–æ–≤–Ω–∏     Grafana         –î–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ –∞–Ω–∞–ª–∏–∑–∞
````

**Alertmanager –∫–æ–º–∞–Ω–¥—ã:**

bash

```bash
# –°—Ç–∞—Ç—É—Å
amtool config show
amtool config routes
amtool alert query

# Silences
amtool silence add alertname=HighCPU --duration=2h --comment="Maintenance"
amtool silence query
amtool silence expire <silence-id>

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–Ω—Ñ–∏–≥–∞
amtool check-config alertmanager.yml

# –û—Ç–ø—Ä–∞–≤–∫–∞ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –∞–ª–µ—Ä—Ç–∞
amtool alert add alertname=Test severity=warning

# API –∑–∞–ø—Ä–æ—Å—ã
curl -X GET http://localhost:9093/api/v2/alerts
curl -X GET http://localhost:9093/api/v2/silences
curl -X GET http://localhost:9093/api/v2/status
```

### üíª –ó–∞–¥–∞–Ω–∏–µ

–ù–∞—Å—Ç—Ä–æ–π –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—É—é —Å–∏—Å—Ç–µ–º—É –∞–ª–µ—Ä—Ç–∏–Ω–≥–∞:

1. **–î–æ–±–∞–≤—å Alertmanager –≤ docker-compose.yml**:

yaml

```yaml
version: '3.8'

services:
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - ./alerts.yml:/etc/prometheus/alerts.yml
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.enable-lifecycle'
    restart: unless-stopped

  alertmanager:
    image: prom/alertmanager:latest
    container_name: alertmanager
    ports:
      - "9093:9093"
    volumes:
      - ./alertmanager.yml:/etc/alertmanager/alertmanager.yml
      - alertmanager-data:/alertmanager
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
    restart: unless-stopped

  node-exporter:
    image: prom/node-exporter:latest
    container_name: node-exporter
    ports:
      - "9100:9100"
    command:
      - '--path.rootfs=/host'
    pid: host
    restart: unless-stopped
    volumes:
      - '/:/host:ro,rslave'

  # Webhook receiver –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
  webhook-receiver:
    image: ghcr.io/tarampampam/webhook-tester:latest
    container_name: webhook-receiver
    ports:
      - "8080:8080"
    restart: unless-stopped

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_UNIFIED_ALERTING_ENABLED=true
    volumes:
      - grafana-data:/var/lib/grafana
      - ./grafana-datasources.yml:/etc/grafana/provisioning/datasources/datasources.yml
    restart: unless-stopped
    depends_on:
      - prometheus

volumes:
  prometheus-data:
  alertmanager-data:
  grafana-data:
```

2. **–°–æ–∑–¥–∞–π prometheus.yml —Å –∞–ª–µ—Ä—Ç–∏–Ω–≥–æ–º**:

yaml

```yaml
global:
  scrape_interval: 15s
  evaluation_interval: 15s
  external_labels:
    cluster: 'local'
    environment: 'dev'

# Alertmanager configuration
alerting:
  alertmanagers:
    - static_configs:
        - targets:
            - alertmanager:9093

# Load rules
rule_files:
  - "alerts.yml"

scrape_configs:
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  - job_name: 'node-exporter'
    static_configs:
      - targets: ['node-exporter:9100']

  - job_name: 'alertmanager'
    static_configs:
      - targets: ['alertmanager:9093']
```

3. **–°–æ–∑–¥–∞–π alerts.yml —Å –ø—Ä–∞–≤–∏–ª–∞–º–∏**:

yaml

```yaml
groups:
  - name: infrastructure
    interval: 30s
    rules:
    # Instance down
    - alert: InstanceDown
      expr: up == 0
      for: 2m
      labels:
        severity: critical
        team: infrastructure
      annotations:
        summary: "Instance {{ $labels.instance }} is down"
        description: "{{ $labels.job }} on {{ $labels.instance }} has been down for more than 2 minutes."
        dashboard: "http://localhost:3000/d/node-exporter"
        runbook: "https://runbooks.example.com/InstanceDown"

    # High CPU
    - alert: HighCPUUsage
      expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
      for: 5m
      labels:
        severity: warning
        team: infrastructure
      annotations:
        summary: "High CPU usage on {{ $labels.instance }}"
        description: "CPU usage is {{ $value | humanize }}% on {{ $labels.instance }}"
        dashboard: "http://localhost:3000/d/node-exporter"

    # High Memory
    - alert: HighMemoryUsage
      expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 85
      for: 5m
      labels:
        severity: warning
        team: infrastructure
      annotations:
        summary: "High memory usage on {{ $labels.instance }}"
        description: "Memory usage is {{ $value | humanize }}% on {{ $labels.instance }}"

    # Disk space critical
    - alert: DiskSpaceCritical
      expr: (1 - (node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"})) * 100 > 90
      for: 5m
      labels:
        severity: critical
        team: infrastructure
      annotations:
        summary: "Critical disk space on {{ $labels.instance }}"
        description: "Disk usage is {{ $value | humanize }}% on {{ $labels.instance }}"
        impact: "System may become unresponsive if disk fills up"

    # Disk space warning
    - alert: DiskSpaceWarning
      expr: (1 - (node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"})) * 100 > 80
      for: 10m
      labels:
        severity: warning
        team: infrastructure
      annotations:
        summary: "Low disk space on {{ $labels.instance }}"
        description: "Disk usage is {{ $value | humanize }}% on {{ $labels.instance }}"

  - name: alertmanager
    interval: 30s
    rules:
    # Alertmanager down
    - alert: AlertmanagerDown
      expr: up{job="alertmanager"} == 0
      for: 2m
      labels:
        severity: critical
        team: monitoring
      annotations:
        summary: "Alertmanager is down"
        description: "Alertmanager has been down for more than 2 minutes. Alerts may not be delivered!"

    # Too many alerts firing
    - alert: TooManyAlerts
      expr: count(ALERTS{alertstate="firing"}) > 10
      for: 5m
      labels:
        severity: warning
        team: monitoring
      annotations:
        summary: "Too many alerts firing"
        description: "There are {{ $value }} alerts currently firing. This may indicate a systemic issue."

  - name: prometheus
    interval: 30s
    rules:
    # Prometheus target missing
    - alert: PrometheusTargetMissing
      expr: up == 0
      for: 2m
      labels:
        severity: critical
        team: monitoring
      annotations:
        summary: "Prometheus target missing"
        description: "A Prometheus target has disappeared. Instance: {{ $labels.instance }}"

    # Prometheus config reload failed
    - alert: PrometheusConfigReloadFailed
      expr: prometheus_config_last_reload_successful == 0
      for: 5m
      labels:
        severity: critical
        team: monitoring
      annotations:
        summary: "Prometheus config reload failed"
        description: "Prometheus config reload has failed on {{ $labels.instance }}"

  - name: deadman
    interval: 30s
    rules:
    # Deadman switch - –∞–ª–µ—Ä—Ç –∫–æ—Ç–æ—Ä—ã–π –≤—Å–µ–≥–¥–∞ –¥–æ–ª–∂–µ–Ω firing
    - alert: DeadMansSwitch
      expr: vector(1)
      labels:
        severity: info
        team: monitoring
      annotations:
        summary: "Monitoring system is alive"
        description: "This is a deadman switch. It should always be firing. If you don't receive this, monitoring is broken."
```

4. **–°–æ–∑–¥–∞–π alertmanager.yml —Å routing –∏ receivers**:

yaml

```yaml
global:
  resolve_timeout: 5m
  # Slack (—Ä–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π –∏ –Ω–∞—Å—Ç—Ä–æ–π –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏)
  # slack_api_url: 'https://hooks.slack.com/services/YOUR/WEBHOOK/URL'

# Templates –¥–ª—è –∫—Ä–∞—Å–∏–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
templates:
  - '/etc/alertmanager/templates/*.tmpl'

# Route tree
route:
  receiver: 'default'
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 10s
  group_interval: 5m
  repeat_interval: 4h
  
  routes:
    # Critical –∞–ª–µ—Ä—Ç—ã ‚Üí webhook + log
    - match:
        severity: critical
      receiver: critical-alerts
      group_wait: 10s
      repeat_interval: 1h
      continue: true

    # Infrastructure team
    - match:
        team: infrastructure
      receiver: infrastructure-team
      group_wait: 30s
      repeat_interval: 4h

    # Monitoring team
    - match:
        team: monitoring
      receiver: monitoring-team

    # Deadman switch (–¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —á—Ç–æ alerting —Ä–∞–±–æ—Ç–∞–µ—Ç)
    - match:
        alertname: DeadMansSwitch
      receiver: deadman
      repeat_interval: 5m

# Inhibition rules (–ø–æ–¥–∞–≤–ª–µ–Ω–∏–µ –∑–∞–≤–∏—Å–∏–º—ã—Ö –∞–ª–µ—Ä—Ç–æ–≤)
inhibit_rules:
  # –ï—Å–ª–∏ instance down, –Ω–µ —Å–ª–∞—Ç—å –¥—Ä—É–≥–∏–µ –∞–ª–µ—Ä—Ç—ã —Å —Ç–æ–≥–æ –∂–µ instance
  - source_match:
      severity: critical
      alertname: InstanceDown
    target_match:
      severity: warning
    equal: ['instance']

  # –ï—Å–ª–∏ –¥–∏—Å–∫ –∫—Ä–∏—Ç–∏—á–µ–Ω, –Ω–µ —Å–ª–∞—Ç—å warning –æ –¥–∏—Å–∫–µ
  - source_match:
      alertname: DiskSpaceCritical
    target_match:
      alertname: DiskSpaceWarning
    equal: ['instance', 'mountpoint']

# Receivers (–∫–∞–Ω–∞–ª—ã —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π)
receivers:
  - name: 'default'
    webhook_configs:
      - url: 'http://webhook-receiver:8080/webhook/default'
        send_resolved: true

  - name: 'critical-alerts'
    webhook_configs:
      - url: 'http://webhook-receiver:8080/webhook/critical'
        send_resolved: true
    # Uncomment for Slack
    # slack_configs:
    #   - channel: '#alerts-critical'
    #     title: 'üö® CRITICAL ALERT'
    #     text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
    #     send_resolved: true

  - name: 'infrastructure-team'
    webhook_configs:
      - url: 'http://webhook-receiver:8080/webhook/infrastructure'
        send_resolved: true
    # Uncomment for Slack
    # slack_configs:
    #   - channel: '#team-infrastructure'
    #     title: '‚ö†Ô∏è Infrastructure Alert'
    #     text: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'

  - name: 'monitoring-team'
    webhook_configs:
      - url: 'http://webhook-receiver:8080/webhook/monitoring'
        send_resolved: true

  - name: 'deadman'
    webhook_configs:
      - url: 'http://webhook-receiver:8080/webhook/deadman'
        send_resolved: false
```

5. **–°–æ–∑–¥–∞–π grafana-datasources.yml**:

yaml

```yaml
apiVersion: 1

datasources:
  - name: Prometheus
    type: prometheus
    access: proxy
    url: http://prometheus:9090
    isDefault: true
    editable: true
    jsonData:
      httpMethod: POST
      
  - name: Alertmanager
    type: alertmanager
    access: proxy
    url: http://alertmanager:9093
    editable: true
    jsonData:
      implementation: prometheus
```

6. **–ó–∞–ø—É—Å—Ç–∏ –∏ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–π**:

bash

```bash
# –ó–∞–ø—É—Å–∫
docker-compose up -d

# –ü—Ä–æ–≤–µ—Ä–∫–∞ Prometheus
curl http://localhost:9090/api/v1/rules

# –ü—Ä–æ–≤–µ—Ä–∫–∞ Alertmanager
curl http://localhost:9093/api/v2/status

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –∞–ª–µ—Ä—Ç–æ–≤ –≤ Prometheus
curl http://localhost:9090/api/v1/alerts | jq

# –°–ø–∏—Å–æ–∫ firing –∞–ª–µ—Ä—Ç–æ–≤
curl http://localhost:9093/api/v2/alerts | jq '.[] | select(.status.state == "active")'
```

7. **–°–æ–∑–¥–∞–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ—Å—Ç–æ–≤–æ–π –Ω–∞–≥—Ä—É–∑–∫–∏** (`stress_test.sh`):

bash

```bash
#!/bin/bash

echo "Starting stress test to trigger alerts..."

# CPU stress (—Ç—Ä–∏–≥–≥–µ—Ä–Ω–µ—Ç HighCPUUsage)
echo "Generating CPU load..."
docker run --rm --name cpu-stress \
  polinux/stress \
  stress --cpu 4 --timeout 300s &

# –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –¥–∏—Å–∫–∞ (–¥–ª—è DiskSpaceWarning)
# –í–ù–ò–ú–ê–ù–ò–ï: –ë—É–¥—å –æ—Å—Ç–æ—Ä–æ–∂–µ–Ω —Å —ç—Ç–∏–º –Ω–∞ –ø—Ä–æ–¥–µ!
# echo "Filling disk space..."
# dd if=/dev/zero of=/tmp/largefile bs=1M count=10000

echo "Stress test running. Check alerts in:"
echo "- Prometheus: http://localhost:9090/alerts"
echo "- Alertmanager: http://localhost:9093"
echo "- Webhook receiver: http://localhost:8080"
echo ""
echo "Wait 5-10 minutes for alerts to fire..."
```

8. **–ü—Ä–æ–≤–µ—Ä—å UI –∏ –∞–ª–µ—Ä—Ç—ã**:

bash

```bash
# Prometheus Alerts UI
open http://localhost:9090/alerts

# Alertmanager UI
open http://localhost:9093

# Grafana Alerting
open http://localhost:3000/alerting/list

# Webhook receiver (–ø—Ä–æ–≤–µ—Ä—å –ø–æ–ª—É—á–µ–Ω–Ω—ã–µ –∞–ª–µ—Ä—Ç—ã)
open http://localhost:8080
```

9. **–ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–π silencing**:

bash

```bash
# –£—Å—Ç–∞–Ω–æ–≤–∏ amtool
go install github.com/prometheus/alertmanager/cmd/amtool@latest
# –∏–ª–∏
brew install amtool

# –ù–∞—Å—Ç—Ä–æ–π amtool
cat > ~/.config/amtool/config.yml <<EOF
alertmanager.url: http://localhost:9093
EOF

# –°–æ–∑–¥–∞–π silence –Ω–∞ –≤—Ä–µ–º—è —Ç–µ—Å—Ç–∞
amtool silence add \
  alertname=HighCPUUsage \
  --duration=1h \
  --comment="Testing alert system" \
  --author="devops@example.com"

# –ü—Ä–æ–≤–µ—Ä—å silences
amtool silence query

# –£–¥–∞–ª–∏ silence
amtool silence expire <silence-id>
```

### üöÄ –ë–æ–Ω—É—Å (–Ω–æ–≤–æ–µ)

**1. –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å–æ Slack**:

–û–±–Ω–æ–≤–∏ `alertmanager.yml`:

yaml

```yaml
global:
  slack_api_url: 'https://hooks.slack.com/services/YOUR/WEBHOOK/URL'

receivers:
  - name: 'slack-critical'
    slack_configs:
      - channel: '#alerts-critical'
        username: 'Alertmanager'
        icon_emoji: ':fire:'
        title: 'üö® {{ .GroupLabels.alertname }}'
        text: |
          {{ range .Alerts }}
          *Alert:* {{ .Labels.alertname }}
          *Severity:* {{ .Labels.severity }}
          *Instance:* {{ .Labels.instance }}
          *Description:* {{ .Annotations.description }}
          *Dashboard:* {{ .Annotations.dashboard }}
          {{ end }}
        send_resolved: true
        color: '{{ if eq .Status "firing" }}danger{{ else }}good{{ end }}'
```

**2. Custom notification template**:

–°–æ–∑–¥–∞–π `templates/slack.tmpl`:

gotmpl

```gotmpl
{{ define "slack.title" }}
[{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ .GroupLabels.alertname }}
{{ end }}

{{ define "slack.text" }}
{{ range .Alerts }}
*Alert:* {{ .Labels.alertname }} - `{{ .Labels.severity }}`
*Instance:* {{ .Labels.instance }}
*Summary:* {{ .Annotations.summary }}
*Description:* {{ .Annotations.description }}
{{ if .Annotations.runbook }}*Runbook:* {{ .Annotations.runbook }}{{ end }}
{{ if .Annotations.dashboard }}*Dashboard:* {{ .Annotations.dashboard }}{{ end }}
*Started:* {{ .StartsAt.Format "2006-01-02 15:04:05 MST" }}
{{ if .EndsAt }}*Ended:* {{ .EndsAt.Format "2006-01-02 15:04:05 MST" }}{{ end }}
{{ end }}
{{ end }}

{{ define "slack.color" }}
{{ if eq .Status "firing" }}
  {{ if eq .CommonLabels.severity "critical" }}danger{{ else }}warning{{ end }}
{{ else }}
good
{{ end }}
{{ end }}
```

**3. PagerDuty –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è** (–¥–ª—è critical alerts):

yaml

```yaml
receivers:
  - name: 'pagerduty-critical'
    pagerduty_configs:
      - service_key: 'YOUR_PAGERDUTY_SERVICE_KEY'
        description: '{{ .GroupLabels.alertname }}: {{ .Annotations.summary }}'
        severity: '{{ .CommonLabels.severity }}'
        details:
          firing: '{{ .Alerts.Firing | len }}'
          resolved: '{{ .Alerts.Resolved | len }}'
          instance: '{{ .CommonLabels.instance }}'
        client: 'Alertmanager'
        client_url: 'http://alertmanager:9093'
        send_resolved: true
```

**4. Email notifications —Å HTML template**:

yaml

```yaml
receivers:
  - name: 'email-team'
    email_configs:
      - to: 'team@example.com'
        from: 'alertmanager@example.com'
        smarthost: 'smtp.gmail.com:587'
        auth_username: 'alertmanager@example.com'
        auth_password: 'your-app-password'
        headers:
          Subject: '{{ if eq .Status "firing" }}üö®{{ else }}‚úÖ{{ end }} [{{ .Status | toUpper }}] {{ .GroupLabels.alertname }}'
        html: |
          <!DOCTYPE html>
          <html>
          <body>
            <h2 style="color: {{ if eq .Status "firing" }}#d9534f{{ else }}#5cb85c{{ end }}">
              {{ if eq .Status "firing" }}üö® Firing Alerts{{ else }}‚úÖ Resolved{{ end }}
            </h2>
            {{ range .Alerts }}
            <div style="border-left: 4px solid {{ if eq .Status "firing" }}#d9534f{{ else }}#5cb85c{{ end }}; padding: 10px; margin: 10px 0;">
              <h3>{{ .Labels.alertname }}</h3>
              <p><strong>Severity:</strong> {{ .Labels.severity }}</p>
              <p><strong>Instance:</strong> {{ .Labels.instance }}</p>
              <p><strong>Description:</strong> {{ .Annotations.description }}</p>
              {{ if .Annotations.runbook }}
              <p><a href="{{ .Annotations.runbook }}">üìñ Runbook</a></p>
              {{ end }}
              {{ if .Annotations.dashboard }}
              <p><a href="{{ .Annotations.dashboard }}">üìä Dashboard</a></p>
              {{ end }}
            </div>
            {{ end }}
          </body>
          </html>
        send_resolved: true
```

**5. Webhook –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å Jira/ServiceNow**:

–°–æ–∑–¥–∞–π `webhook_handler.py`:

python
```python
#!/usr/bin/env python3

from flask import Flask, request, jsonify
import requests

app = Flask(__name__)

@app.route("/webhook/jira", methods=["POST"])
def jira_webhook():
    """
    –°–æ–∑–¥–∞–µ—Ç Jira ticket –¥–ª—è –∫—Ä–∏—Ç–∏—á–Ω—ã—Ö –∞–ª–µ—Ä—Ç–æ–≤
    (status=firing, severity=critical)
    """
    data = request.json or {}

    if data.get("status") == "firing":
        for alert in data.get("alerts", []):
            if alert.get("labels", {}).get("severity") == "critical":
                create_jira_ticket(alert)

    return jsonify({"status": "ok"}), 200

def create_jira_ticket(alert):
    """–°–æ–∑–¥–∞–µ—Ç Jira ticket —á–µ—Ä–µ–∑ API"""

    jira_url = "https://your-jira.atlassian.net/rest/api/2/issue"

    ticket = {
        "fields": {
            "project": {"key": "OPS"},
            "summary": f"[ALERT] {alert['labels'].get('alertname', 'Unknown alert')}",
            "description": alert.get("annotations", {}).get(
                "description", "No description provided"
            ),
            "issuetype": {"name": "Incident"},
            "priority": {"name": "Critical"},
            "labels": ["alert", "monitoring"],
        }
    }

    response = requests.post(
        jira_url,
        json=ticket,
        auth=("user@example.com", "jira-api-token"),
        headers={"Content-Type": "application/json"},
        timeout=10,
    )

    if response.ok:
        print(f"Jira ticket created: {response.json().get('key')}")
    else:
        print(f"Failed to create Jira ticket: {response.text}")

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000)
```

**6. SLO-based alerting** (–ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ø–æ–¥—Ö–æ–¥):
```yaml
groups:
  - name: slo_alerts
    interval: 30s
    rules:
    # Error budget burn rate
    - alert: ErrorBudgetBurnRateTooHigh
      expr: |
        (
          sum(rate(http_requests_total{status=~"5.."}[1h]))
          /
          sum(rate(http_requests_total[1h]))
        ) > (1 - 0.999) * 10  # 10x SLO burn rate
      for: 5m
      labels:
        severity: critical
        team: sre
      annotations:
        summary: "Error budget burning too fast"
        description: "Current error rate is {{ $value | humanizePercentage }}. At this rate, monthly error budget will be exhausted in {{ with printf \"(1-0.999)*730/%f\" $value }}{{ . }}{{ end }} hours."
        dashboard: "http://localhost:3000/d/slo-dashboard"

    # SLO violation
    - alert: SLOViolation
      expr: |
        (
          1 - (
            sum(rate(http_requests_total{status!~"5.."}[30d]))
            /
            sum(rate(http_requests_total[30d]))
          )
        ) > 0.001  # –ù–∞—Ä—É—à–µ–Ω–∏–µ 99.9% SLO
      for: 1h
      labels:
        severity: warning
        team: sre
      annotations:
        summary: "SLO violation detected"
        description: "30-day error rate is {{ $value | humanizePercentage }}, violating 99.9% SLO"
```

**7. Multi-window multi-burn-rate alerts** (Google SRE –ø–æ–¥—Ö–æ–¥):
```yaml
groups:
  - name: multiwindow_multiburn_alerts
    interval: 30s
    rules:
    # Fast burn (–Ω—É–∂–Ω–æ –¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ)
    - alert: ErrorBudgetFastBurn
      expr: |
        (
          sum(rate(http_requests_total{status=~"5.."}[5m]))
          /
          sum(rate(http_requests_total[5m]))
        ) > 14.4 * (1 - 0.999)  # 14.4x burn rate
        and
        (
          sum(rate(http_requests_total{status=~"5.."}[1h]))
          /
          sum(rate(http_requests_total[1h]))
        ) > 14.4 * (1 - 0.999)
      for: 2m
      labels:
        severity: critical
        burn_rate: fast
      annotations:
        summary: "Fast error budget burn"
        description: "Error budget will be exhausted in 2 hours at current rate"

    # Slow burn (—Ç—Ä–µ–±—É–µ—Ç –≤–Ω–∏–º–∞–Ω–∏—è –≤ –±–ª–∏–∂–∞–π—à–µ–µ –≤—Ä–µ–º—è)
    - alert: ErrorBudgetSlowBurn
      expr: |
        (
          sum(rate(http_requests_total{status=~"5.."}[30m]))
          /
          sum(rate(http_requests_total[30m]))
        ) > 6 * (1 - 0.999)  # 6x burn rate
        and
        (
          sum(rate(http_requests_total{status=~"5.."}[6h]))
          /
          sum(rate(http_requests_total[6h]))
        ) > 6 * (1 - 0.999)
      for: 15m
      labels:
        severity: warning
        burn_rate: slow
      annotations:
        summary: "Slow error budget burn"
        description: "Error budget will be exhausted in 5 days at current rate"
```

**8. Alert aggregation dashboard**:

–°–æ–∑–¥–∞–π Python —Å–∫—Ä–∏–ø—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∞–ª–µ—Ä—Ç–æ–≤ (`alert_analysis.py`):
```python
#!/usr/bin/env python3
import requests
from collections import Counter
from datetime import datetime, timedelta

ALERTMANAGER_URL = "http://localhost:9093"

def get_alerts():
    """–ü–æ–ª—É—á–∏—Ç—å –≤—Å–µ –∞–ª–µ—Ä—Ç—ã –∏–∑ Alertmanager"""
    response = requests.get(f"{ALERTMANAGER_URL}/api/v2/alerts")
    return response.json()

def analyze_alerts():
    """–ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –∞–ª–µ—Ä—Ç–æ–≤"""
    alerts = get_alerts()
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    total_alerts = len(alerts)
    firing_alerts = [a for a in alerts if a['status']['state'] == 'active']
    
    # –ü–æ severity
    severity_counter = Counter(
        alert['labels'].get('severity', 'unknown') 
        for alert in firing_alerts
    )
    
    # –ü–æ team
    team_counter = Counter(
        alert['labels'].get('team', 'unknown') 
        for alert in firing_alerts
    )
    
    # –°–∞–º—ã–µ —á–∞—Å—Ç—ã–µ –∞–ª–µ—Ä—Ç—ã
    alert_counter = Counter(
        alert['labels']['alertname'] 
        for alert in firing_alerts
    )
    
    # –í—ã–≤–æ–¥ –æ—Ç—á–µ—Ç–∞
    print("=" * 60)
    print("ALERT ANALYSIS REPORT")
    print("=" * 60)
    print(f"Total alerts: {total_alerts}")
    print(f"Firing alerts: {len(firing_alerts)}")
    print()
    
    print("By Severity:")
    for severity, count in severity_counter.most_common():
        print(f"  {severity}: {count}")
    print()
    
    print("By Team:")
    for team, count in team_counter.most_common():
        print(f"  {team}: {count}")
    print()
    
    print("Top 5 Most Frequent Alerts:")
    for alertname, count in alert_counter.most_common(5):
        print(f"  {alertname}: {count}")
    print("=" * 60)

if __name__ == "__main__":
    analyze_alerts()
```

**9. Alert testing framework**:

–°–æ–∑–¥–∞–π `alert_test.py`:
```python
#!/usr/bin/env python3
"""
–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∞–ª–µ—Ä—Ç–æ–≤ - –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º
—á—Ç–æ –∞–ª–µ—Ä—Ç—ã —Å—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç
"""
import requests
import time
from prometheus_client import CollectorRegistry, Gauge, push_to_gateway

def test_high_cpu_alert():
    """–¢–µ—Å—Ç –∞–ª–µ—Ä—Ç–∞ HighCPUUsage"""
    print("Testing HighCPUUsage alert...")
    
    registry = CollectorRegistry()
    cpu_gauge = Gauge('node_cpu_seconds_total', 
                      'CPU time', 
                      ['mode', 'instance'], 
                      registry=registry)
    
    # –°–∏–º—É–ª–∏—Ä—É–µ–º –≤—ã—Å–æ–∫—É—é CPU –Ω–∞–≥—Ä—É–∑–∫—É
    cpu_gauge.labels(mode='idle', instance='test-instance').set(0.1)
    cpu_gauge.labels(mode='user', instance='test-instance').set(0.8)
    
    # Push –≤ Pushgateway
    push_to_gateway('localhost:9091', job='test', registry=registry)
    
    print("Metrics pushed. Wait 5 minutes and check alerts...")
    print("http://localhost:9090/alerts")

def test_disk_space_alert():
    """–¢–µ—Å—Ç –∞–ª–µ—Ä—Ç–∞ DiskSpaceCritical"""
    print("Testing DiskSpaceCritical alert...")
    
    registry = CollectorRegistry()
    disk_total = Gauge('node_filesystem_size_bytes',
                       'Filesystem size',
                       ['mountpoint', 'instance'],
                       registry=registry)
    disk_avail = Gauge('node_filesystem_avail_bytes',
                       'Available space',
                       ['mountpoint', 'instance'],
                       registry=registry)
    
    # –°–∏–º—É–ª–∏—Ä—É–µ–º 95% –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –¥–∏—Å–∫–∞
    disk_total.labels(mountpoint='/', instance='test-instance').set(100e9)  # 100GB
    disk_avail.labels(mountpoint='/', instance='test-instance').set(5e9)    # 5GB
    
    push_to_gateway('localhost:9091', job='test', registry=registry)
    
    print("Metrics pushed. Check alerts...")

if __name__ == "__main__":
    print("Starting alert tests...")
    test_high_cpu_alert()
    time.sleep(2)
    test_disk_space_alert()
    print("\nTests completed. Monitor alerts for next 10 minutes.")
```

**10. Alert maintenance calendar integration**:
```python
#!/usr/bin/env python3
"""
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ silences –≤–æ –≤—Ä–µ–º—è maintenance windows
"""
import requests
from datetime import datetime, timedelta

ALERTMANAGER_URL = "http://localhost:9093"

def create_maintenance_silence(service, duration_hours, comment):
    """–°–æ–∑–¥–∞—Ç—å silence –Ω–∞ –≤—Ä–µ–º—è maintenance"""
    
    now = datetime.utcnow()
    starts_at = now.isoformat() + "Z"
    ends_at = (now + timedelta(hours=duration_hours)).isoformat() + "Z"
    
    silence = {
        "matchers": [
            {
                "name": "service",
                "value": service,
                "isRegex": False
            }
        ],
        "startsAt": starts_at,
        "endsAt": ends_at,
        "createdBy": "maintenance-script",
        "comment": comment
    }
    
    response = requests.post(
        f"{ALERTMANAGER_URL}/api/v2/silences",
        json=silence
    )
    
    if response.status_code == 200:
        silence_id = response.json()['silenceID']
        print(f"‚úÖ Silence created: {silence_id}")
        print(f"   Service: {service}")
        print(f"   Duration: {duration_hours} hours")
        print(f"   Ends at: {ends_at}")
        return silence_id
    else:
        print(f"‚ùå Failed to create silence: {response.text}")
        return None

if __name__ == "__main__":
    # –ü—Ä–∏–º–µ—Ä: Maintenance –Ω–∞ API —Å–µ—Ä–≤–∏—Å–µ –Ω–∞ 2 —á–∞—Å–∞
    create_maintenance_silence(
        service="api",
        duration_hours=2,
        comment="Planned database migration"
    )
```

---

## –ò—Ç–æ–≥–∏ –º–æ–¥—É–ª—è 5

–ü–æ—Å–ª–µ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏—è —ç—Ç–æ–≥–æ –º–æ–¥—É–ª—è —Ç—ã –¥–æ–ª–∂–µ–Ω —É–º–µ—Ç—å:

‚úÖ –ù–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å Alertmanager —Å routing –∏ inhibition
‚úÖ –ü–∏—Å–∞—Ç—å –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ alert rules –≤ Prometheus
‚úÖ –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –∫–∞–Ω–∞–ª–∞–º–∏ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π (Slack, PagerDuty, Email)
‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å grouping, inhibition –∏ silencing
‚úÖ –°–æ–∑–¥–∞–≤–∞—Ç—å SLO-based alerts
‚úÖ –ò–∑–±–µ–≥–∞—Ç—å alert fatigue —á–µ—Ä–µ–∑ –ø—Ä–∞–≤–∏–ª—å–Ω—É—é –Ω–∞—Å—Ç—Ä–æ–π–∫—É
‚úÖ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –∏ –æ—Ç–ª–∞–∂–∏–≤–∞—Ç—å alerts
‚úÖ –°–æ–∑–¥–∞–≤–∞—Ç—å custom notification templates
‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å maintenance windows

**–ö–ª—é—á–µ–≤—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã –∞–ª–µ—Ä—Ç–∏–Ω–≥–∞:**
1. Alert –Ω–∞ —Å–∏–º–ø—Ç–æ–º—ã, –∞ –Ω–µ –Ω–∞ –ø—Ä–∏—á–∏–Ω—ã
2. –ö–∞–∂–¥—ã–π –∞–ª–µ—Ä—Ç –¥–æ–ª–∂–µ–Ω —Ç—Ä–µ–±–æ–≤–∞—Ç—å –¥–µ–π—Å—Ç–≤–∏—è
3. –ò—Å–ø–æ–ª—å–∑—É–π –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ severity —É—Ä–æ–≤–Ω–∏
4. –ì—Ä—É–ø–ø–∏—Ä—É–π –∏ –ø–æ–¥–∞–≤–ª—è–π –∑–∞–≤–∏—Å–∏–º—ã–µ –∞–ª–µ—Ä—Ç—ã
5. –†–µ–≥—É–ª—è—Ä–Ω–æ review –∏ cleanup –∞–ª–µ—Ä—Ç–æ–≤
6. –î–æ–∫—É–º–µ–Ω—Ç–∏—Ä—É–π runbooks –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∞–ª–µ—Ä—Ç–∞
7. –¢–µ—Å—Ç–∏—Ä—É–π –∞–ª–µ—Ä—Ç—ã —Ä–µ–≥—É–ª—è—Ä–Ω–æ


## –ú–æ–¥—É–ª—å 6: Distributed Tracing –∏ Application Performance Monitoring (40 –º–∏–Ω—É—Ç)

### üéØ –ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞

**–¢—Ä–∏ —Å—Ç–æ–ª–ø–∞ Observability:**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   METRICS   ‚îÇ  - –ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç? (CPU, memory, requests/sec)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ    LOGS     ‚îÇ  - –ß—Ç–æ –ø—Ä–æ–∏–∑–æ—à–ª–æ? (—Å–æ–±—ã—Ç–∏—è, –æ—à–∏–±–∫–∏)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   TRACES    ‚îÇ  - –ü–æ—á–µ–º—É —ç—Ç–æ –ø—Ä–æ–∏–∑–æ—à–ª–æ? (–ø—É—Ç—å –∑–∞–ø—Ä–æ—Å–∞ —á–µ—Ä–µ–∑ —Å–∏—Å—Ç–µ–º—É)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Distributed Tracing - –∑–∞—á–µ–º –Ω—É–∂–µ–Ω:**

```
–ü—Ä–æ–±–ª–µ–º–∞ –≤ –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å–∞—Ö:
User Request ‚Üí API Gateway ‚Üí Auth Service ‚Üí Order Service ‚Üí Payment Service ‚Üí Database
                                                                   ‚Üì
                                              ‚ùå SLOW RESPONSE (5 seconds)

–í–æ–ø—Ä–æ—Å: –ì–¥–µ bottleneck?
- API Gateway: 50ms
- Auth Service: 100ms
- Order Service: 200ms
- Payment Service: 4500ms ‚Üê –ù–ê–ô–î–ï–ù–û!
- Database: 150ms
```

**–û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏:**

**Trace** - –ø–æ–ª–Ω—ã–π –ø—É—Ç—å –æ–¥–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ —á–µ—Ä–µ–∑ —Å–∏—Å—Ç–µ–º—É:

```
Trace ID: abc123
‚îú‚îÄ Span 1: API Gateway (50ms)
‚îú‚îÄ Span 2: Auth Service (100ms)
‚îú‚îÄ Span 3: Order Service (200ms)
‚îÇ  ‚îú‚îÄ Span 4: DB Query (50ms)
‚îÇ  ‚îî‚îÄ Span 5: Cache Check (10ms)
‚îî‚îÄ Span 6: Payment Service (4500ms)
   ‚îî‚îÄ Span 7: External API Call (4400ms) ‚Üê –ü—Ä–æ–±–ª–µ–º–∞!
```

**Span** - –µ–¥–∏–Ω–∏—Ü–∞ —Ä–∞–±–æ—Ç—ã –≤ —Å–∏—Å—Ç–µ–º–µ:

yaml

````yaml
Span:
  trace_id: "abc123"
  span_id: "span456"
  parent_span_id: "span789"
  operation_name: "POST /api/orders"
  start_time: "2025-01-15T10:00:00Z"
  duration: 200ms
  tags:
    http.method: "POST"
    http.status_code: 200
    service.name: "order-service"
    db.statement: "SELECT * FROM orders"
  logs:
    - timestamp: "2025-01-15T10:00:00.050Z"
      message: "Order validated"
````

**–ü–æ–ø—É–ª—è—Ä–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã —Ç—Ä–µ–π—Å–∏–Ω–≥–∞:**
```
Jaeger       - CNCF –ø—Ä–æ–µ–∫—Ç, –æ—Ç Uber, Go
Zipkin       - –û—Ç Twitter, Java
Tempo        - –û—Ç Grafana Labs, –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å Loki
OpenTelemetry - –°—Ç–∞–Ω–¥–∞—Ä—Ç (–æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ OpenTracing + OpenCensus)
AWS X-Ray    - Managed —Å–µ—Ä–≤–∏—Å –æ—Ç AWS
Datadog APM  - Commercial
New Relic    - Commercial
```

**OpenTelemetry (OTel) - —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ     Your Application             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ  OpenTelemetry SDK         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Auto-instrumentation    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  - Manual instrumentation  ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ OTel Collector ‚îÇ - –û–±—Ä–∞–±–æ—Ç–∫–∞, —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ                       ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îê
‚îÇ Jaeger ‚îÇ            ‚îÇ Tempo  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Sampling (–≤—ã–±–æ—Ä–∫–∞ —Ç—Ä–µ–π—Å–æ–≤):**
```
–ü—Ä–æ–±–ª–µ–º–∞: –ù–µ–ª—å–∑—è —Ö—Ä–∞–Ω–∏—Ç—å 100% —Ç—Ä–µ–π—Å–æ–≤ (—Å–ª–∏—à–∫–æ–º –¥–æ—Ä–æ–≥–æ)

–í–∏–¥—ã sampling:
1. Head sampling (—Ä–µ—à–µ–Ω–∏–µ –≤ –Ω–∞—á–∞–ª–µ)
   - Probabilistic: 10% –≤—Å–µ—Ö —Ç—Ä–µ–π—Å–æ–≤
   - Rate limiting: 100 —Ç—Ä–µ–π—Å–æ–≤/—Å–µ–∫
   
2. Tail sampling (—Ä–µ—à–µ–Ω–∏–µ –≤ –∫–æ–Ω—Ü–µ)
   - –í—Å–µ –º–µ–¥–ª–µ–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã (> 1s)
   - –í—Å–µ –∑–∞–ø—Ä–æ—Å—ã —Å –æ—à–∏–±–∫–∞–º–∏
   - 1% –Ω–æ—Ä–º–∞–ª—å–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤

–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: Tail sampling + –≤—Å–µ–≥–¥–∞ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –æ—à–∏–±–∫–∏
```

**APM (Application Performance Monitoring) - —á—Ç–æ –≤–∫–ª—é—á–∞–µ—Ç:**
```
1. –¢—Ä–µ–π—Å–∏–Ω–≥ (Distributed Tracing)
2. –ü—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏–µ (CPU, Memory profiling)
3. Error tracking
4. Real User Monitoring (RUM)
5. Database query analysis
6. External services monitoring
```

**–ö–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ APM:**

**RED –º–µ—Ç—Ä–∏–∫–∏ (–¥–ª—è —Å–µ—Ä–≤–∏—Å–æ–≤):**
```
Rate     - Requests per second
Error    - Error rate (%)
Duration - Request latency (p50, p95, p99)
```

**USE –º–µ—Ç—Ä–∏–∫–∏ (–¥–ª—è —Ä–µ—Å—É—Ä—Å–æ–≤):**
```
Utilization - % –≤—Ä–µ–º–µ–Ω–∏ –∑–∞–Ω—è—Ç–æ—Å—Ç–∏
Saturation  - –î–ª–∏–Ω–∞ –æ—á–µ—Ä–µ–¥–∏
Errors      - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—à–∏–±–æ–∫
```

**Service metrics:**
````
Apdex Score = (Satisfied + Tolerating/2) / Total Requests
- Satisfied: < 1s
- Tolerating: 1-4s
- Frustrated: > 4s

Throughput = Requests per second
Error Rate = Errors / Total Requests
Availability = Uptime / Total Time
````

**Context Propagation (–∫–∞–∫ –ø–µ—Ä–µ–¥–∞–µ—Ç—Å—è trace_id):**

**HTTP Headers:**

http

````http
# W3C Trace Context (—Å—Ç–∞–Ω–¥–∞—Ä—Ç)
traceparent: 00-abc123def456-span789-01
tracestate: vendor1=value1,vendor2=value2

# Jaeger
uber-trace-id: abc123:span456:0:1

# Zipkin
X-B3-TraceId: abc123
X-B3-SpanId: span456
X-B3-ParentSpanId: parent789
X-B3-Sampled: 1
````

**gRPC Metadata:**
```
grpc-trace-bin: <binary trace context>
````

**Instrumentation –ø–æ–¥—Ö–æ–¥—ã:**

**Auto-instrumentation** (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π):

python

```python
# Python —Å OpenTelemetry
from opentelemetry.instrumentation.flask import FlaskInstrumentor
from opentelemetry.instrumentation.requests import RequestsInstrumentor

FlaskInstrumentor().instrument()      # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ Flask
RequestsInstrumentor().instrument()   # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ requests
```

**Manual instrumentation** (—Ä—É—á–Ω–æ–π):

python

```python
from opentelemetry import trace

tracer = trace.get_tracer(__name__)

@app.route('/api/order')
def create_order():
    with tracer.start_as_current_span("create_order") as span:
        span.set_attribute("order.id", order_id)
        span.set_attribute("user.id", user_id)
        
        # –í–∞—à –∫–æ–¥
        result = process_order(order_id)
        
        span.add_event("Order processed")
        return result
```

**–Ø–∑—ã–∫-—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏:**

**Python:**

python

```python
# OpenTelemetry
opentelemetry-api
opentelemetry-sdk
opentelemetry-instrumentation-flask
opentelemetry-instrumentation-django
opentelemetry-instrumentation-sqlalchemy
opentelemetry-exporter-jaeger
```

**Node.js:**

javascript

```javascript
// OpenTelemetry
@opentelemetry/api
@opentelemetry/sdk-node
@opentelemetry/auto-instrumentations-node
@opentelemetry/exporter-jaeger
```

**Go:**

go

```go
// OpenTelemetry
go.opentelemetry.io/otel
go.opentelemetry.io/otel/trace
go.opentelemetry.io/contrib/instrumentation/net/http/otelhttp
```

**Java:**

java

````java
// OpenTelemetry Java Agent (auto-instrumentation)
java -javaagent:opentelemetry-javaagent.jar \
     -Dotel.service.name=my-service \
     -jar myapp.jar
````

**Jaeger UI - –æ—Å–Ω–æ–≤–Ω—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏:**
```
1. Search traces:
   - –ü–æ service name
   - –ü–æ operation name
   - –ü–æ tags
   - –ü–æ duration
   - –ü–æ –≤—Ä–µ–º–µ–Ω–∏

2. Trace timeline:
   - –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è spans
   - Waterfall view
   - Gantt chart

3. Dependencies graph:
   - –ö–∞—Ä—Ç–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π —Å–µ—Ä–≤–∏—Å–æ–≤
   - –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≤—ã–∑–æ–≤–æ–≤

4. Comparison:
   - –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ç—Ä–µ–π—Å–æ–≤
   - A/B testing —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
```

**Service Map (–∫–∞—Ä—Ç–∞ —Å–µ—Ä–≤–∏—Å–æ–≤):**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   User     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      ‚îÇ HTTP
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ API Gateway‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      ‚îÇ
   ‚îå‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ      ‚îÇ        ‚îÇ
‚îå‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îê ‚îå‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇAuth ‚îÇ ‚îÇOrder‚îÇ ‚îÇUser  ‚îÇ
‚îÇSvc  ‚îÇ ‚îÇSvc  ‚îÇ ‚îÇSvc   ‚îÇ
‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
   ‚îÇ       ‚îÇ
   ‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ   ‚îÇPayment  ‚îÇ
   ‚îÇ   ‚îÇSvc      ‚îÇ
   ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
   ‚îÇ       ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚îÇ         ‚îÇ
   ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îê
   ‚îÇ DB   ‚îÇ  ‚îÇCache ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Error tracking –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è:**
```
–°–≤—è–∑—å —Ç—Ä–µ–π—Å–æ–≤ —Å –æ—à–∏–±–∫–∞–º–∏:

Exception –≤ –∫–æ–¥–µ ‚Üí Trace ID ‚Üí –ü–æ–ª–Ω—ã–π –ø—É—Ç—å –∑–∞–ø—Ä–æ—Å–∞
                               + stack trace
                               + request params
                               + user context
```

**Database query analysis:**
```
–ß–∞—Å—Ç—ã–µ –ø—Ä–æ–±–ª–µ–º—ã:
1. N+1 queries
   - 1 –∑–∞–ø—Ä–æ—Å —Å–ø–∏—Å–∫–∞ + N –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–µ—Ç–∞–ª–µ–π
   
2. Missing indexes
   - Full table scan
   
3. Slow queries
   - –°–ª–æ–∂–Ω—ã–µ JOIN
   - –ë–æ–ª—å—à–∏–µ SELECT *
   
4. Connection pool exhaustion
   - –ù–µ –∑–∞–∫—Ä—ã—Ç—ã–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
```

**–ü—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏–µ (CPU/Memory):**
```
Continuous Profiling:
- Flamegraph –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
- –ö–∞–∫–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –∑–∞–Ω–∏–º–∞—é—Ç –±–æ–ª—å—à–µ –≤—Ä–µ–º–µ–Ω–∏
- Memory allocations
- Goroutines/Threads

–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã:
- pprof (Go)
- py-spy (Python)
- async-profiler (Java)
- Pyroscope (unified)
```

**Real User Monitoring (RUM):**

javascript

````javascript
// Frontend —Ç—Ä–µ–π—Å–∏–Ω–≥
import { WebTracerProvider } from '@opentelemetry/sdk-trace-web';

const provider = new WebTracerProvider();
const tracer = provider.getTracer('frontend-app');

// Track page load
const span = tracer.startSpan('page_load');
span.setAttribute('page.url', window.location.href);

window.addEventListener('load', () => {
  span.end();
});

// Track user interactions
button.addEventListener('click', () => {
  const span = tracer.startSpan('button_click');
  span.setAttribute('button.id', button.id);
  // ... –¥–µ–π—Å—Ç–≤–∏–µ
  span.end();
});
````

**Best practices:**
````
1. ‚úÖ –í—Å–µ–≥–¥–∞ –ø–µ—Ä–µ–¥–∞–≤–∞–π trace context –º–µ–∂–¥—É —Å–µ—Ä–≤–∏—Å–∞–º–∏
2. ‚úÖ –î–æ–±–∞–≤–ª—è–π –ø–æ–ª–µ–∑–Ω—ã–µ attributes (user_id, order_id, etc)
3. ‚úÖ –õ–æ–≥–∏—Ä—É–π trace_id –≤–æ –≤—Å–µ—Ö –ª–æ–≥–∞—Ö
4. ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–π semantic conventions (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –∏–º–µ–Ω–∞)
5. ‚úÖ –ù–∞—Å—Ç—Ä–æ–π –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π sampling
6. ‚úÖ –ù–µ –ª–æ–≥–∏—Ä—É–π sensitive –¥–∞–Ω–Ω—ã–µ –≤ spans
7. ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–π tail sampling –¥–ª—è –æ—à–∏–±–æ–∫
8. ‚úÖ –•—Ä–∞–Ω–∏ —Ç—Ä–µ–π—Å—ã –º–∏–Ω–∏–º—É–º 7 –¥–Ω–µ–π
9. ‚úÖ –ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–π —Å –∞–ª–µ—Ä—Ç–∏–Ω–≥–æ–º
10. ‚úÖ –°–æ–∑–¥–∞–π runbook –¥–ª—è —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
````

**Semantic Conventions (—Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –∏–º–µ–Ω–∞):**

yaml

```yaml
# HTTP
span.name: "GET /api/users"
http.method: "GET"
http.url: "https://api.example.com/users"
http.status_code: 200
http.route: "/api/users"

# Database
span.name: "SELECT users"
db.system: "postgresql"
db.operation: "SELECT"
db.statement: "SELECT * FROM users WHERE id = ?"
db.name: "production"

# RPC
span.name: "UserService.GetUser"
rpc.system: "grpc"
rpc.service: "UserService"
rpc.method: "GetUser"

# Messaging
span.name: "process_order"
messaging.system: "kafka"
messaging.destination: "orders"
messaging.operation: "process"
```

### üíª –ó–∞–¥–∞–Ω–∏–µ

–ù–∞—Å—Ç—Ä–æ–π –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π distributed tracing —Å Jaeger:

1. **–°–æ–∑–¥–∞–π docker-compose.yml –¥–ª—è Jaeger stack**:

yaml

```yaml
version: '3.8'

services:
  # Jaeger all-in-one (–¥–ª—è development)
  jaeger:
    image: jaegertracing/all-in-one:1.52
    container_name: jaeger
    environment:
      - COLLECTOR_ZIPKIN_HOST_PORT=:9411
      - COLLECTOR_OTLP_ENABLED=true
    ports:
      - "5775:5775/udp"   # accept zipkin.thrift (deprecated)
      - "6831:6831/udp"   # accept jaeger.thrift compact
      - "6832:6832/udp"   # accept jaeger.thrift binary
      - "5778:5778"       # serve configs
      - "16686:16686"     # Jaeger UI
      - "14250:14250"     # model.proto
      - "14268:14268"     # jaeger.thrift
      - "14269:14269"     # Admin port: health, metrics
      - "4317:4317"       # OTLP gRPC
      - "4318:4318"       # OTLP HTTP
      - "9411:9411"       # Zipkin compatible
    restart: unless-stopped

  # OpenTelemetry Collector (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏)
  otel-collector:
    image: otel/opentelemetry-collector-contrib:0.91.0
    container_name: otel-collector
    command: ["--config=/etc/otel-collector-config.yml"]
    volumes:
      - ./otel-collector-config.yml:/etc/otel-collector-config.yml
    ports:
      - "4317:4317"   # OTLP gRPC
      - "4318:4318"   # OTLP HTTP
      - "8888:8888"   # Prometheus metrics
      - "8889:8889"   # Prometheus exporter metrics
    restart: unless-stopped
    depends_on:
      - jaeger

  # Demo –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ - Frontend
  frontend:
    build: ./demo-app/frontend
    container_name: frontend
    environment:
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4317
      - OTEL_SERVICE_NAME=frontend
      - BACKEND_URL=http://backend:5000
    ports:
      - "8080:8080"
    depends_on:
      - otel-collector
      - backend
    restart: unless-stopped

  # Demo –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ - Backend
  backend:
    build: ./demo-app/backend
    container_name: backend
    environment:
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4317
      - OTEL_SERVICE_NAME=backend
      - DATABASE_URL=postgresql://user:password@postgres:5432/demo
      - REDIS_URL=redis://redis:6379
    ports:
      - "5000:5000"
    depends_on:
      - postgres
      - redis
      - otel-collector
    restart: unless-stopped

  # PostgreSQL database
  postgres:
    image: postgres:16-alpine
    container_name: postgres
    environment:
      - POSTGRES_USER=user
      - POSTGRES_PASSWORD=password
      - POSTGRES_DB=demo
    ports:
      - "5432:5432"
    volumes:
      - postgres-data:/var/lib/postgresql/data
    restart: unless-stopped

  # Redis cache
  redis:
    image: redis:7-alpine
    container_name: redis
    ports:
      - "6379:6379"
    restart: unless-stopped

  # Grafana –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
  grafana:
    image: grafana/grafana:10.2.3
    container_name: grafana-tracing
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_FEATURE_TOGGLES_ENABLE=traceqlEditor
    volumes:
      - grafana-data:/var/lib/grafana
      - ./grafana-datasources.yml:/etc/grafana/provisioning/datasources/datasources.yml
    restart: unless-stopped
    depends_on:
      - jaeger

volumes:
  postgres-data:
  grafana-data:
```

2. **–°–æ–∑–¥–∞–π otel-collector-config.yml**:

yaml

```yaml
receivers:
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317
      http:
        endpoint: 0.0.0.0:4318

  # Prometheus metrics receiver
  prometheus:
    config:
      scrape_configs:
        - job_name: 'otel-collector'
          scrape_interval: 10s
          static_configs:
            - targets: ['0.0.0.0:8888']

processors:
  # Batch processor –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
  batch:
    timeout: 10s
    send_batch_size: 1024

  # Memory limiter
  memory_limiter:
    check_interval: 1s
    limit_mib: 512

  # Tail sampling - —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ –æ—à–∏–±–∫–∏ –∏ –º–µ–¥–ª–µ–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
  tail_sampling:
    decision_wait: 10s
    num_traces: 100
    expected_new_traces_per_sec: 10
    policies:
      # –í—Å–µ–≥–¥–∞ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –æ—à–∏–±–∫–∏
      - name: error-traces
        type: status_code
        status_code:
          status_codes: [ERROR]
      
      # –ú–µ–¥–ª–µ–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã (> 1s)
      - name: slow-traces
        type: latency
        latency:
          threshold_ms: 1000
      
      # 10% –æ—Å—Ç–∞–ª—å–Ω—ã—Ö
      - name: probabilistic-policy
        type: probabilistic
        probabilistic:
          sampling_percentage: 10

  # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ä–µ—Å—É—Ä—Å–Ω—ã—Ö –∞—Ç—Ä–∏–±—É—Ç–æ–≤
  resource:
    attributes:
      - key: environment
        value: development
        action: insert

  # Attributes processor
  attributes:
    actions:
      - key: db.statement
        action: delete  # –£–¥–∞–ª—è–µ–º SQL –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)

exporters:
  # Jaeger exporter
  jaeger:
    endpoint: jaeger:14250
    tls:
      insecure: true

  # Logging exporter (–¥–ª—è –æ—Ç–ª–∞–¥–∫–∏)
  logging:
    loglevel: info

  # Prometheus exporter –¥–ª—è –º–µ—Ç—Ä–∏–∫
  prometheus:
    endpoint: "0.0.0.0:8889"

service:
  pipelines:
    # Traces pipeline
    traces:
      receivers: [otlp]
      processors: [memory_limiter, tail_sampling, batch, resource, attributes]
      exporters: [jaeger, logging]
    
    # Metrics pipeline
    metrics:
      receivers: [otlp, prometheus]
      processors: [memory_limiter, batch]
      exporters: [prometheus, logging]
```

3. **–°–æ–∑–¥–∞–π demo-app/backend (Python Flask)**:

`demo-app/backend/Dockerfile`:

dockerfile

````dockerfile
FROM python:3.11-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD ["python", "app.py"]
````

`demo-app/backend/requirements.txt`:
````
flask==3.0.0
psycopg2-binary==2.9.9
redis==5.0.1
requests==2.31.0
opentelemetry-api==1.21.0
opentelemetry-sdk==1.21.0
opentelemetry-instrumentation-flask==0.42b0
opentelemetry-instrumentation-requests==0.42b0
opentelemetry-instrumentation-psycopg2==0.42b0
opentelemetry-instrumentation-redis==0.42b0
opentelemetry-exporter-otlp==1.21.0
````

`demo-app/backend/app.py`:

python

```python
from flask import Flask, jsonify, request
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter
from opentelemetry.instrumentation.flask import FlaskInstrumentor
from opentelemetry.instrumentation.requests import RequestsInstrumentor
from opentelemetry.instrumentation.psycopg2 import Psycopg2Instrumentor
from opentelemetry.instrumentation.redis import RedisInstrumentor
from opentelemetry.sdk.resources import Resource
from opentelemetry.trace.status import Status, StatusCode

import psycopg2
import redis
import time
import random
import os
import json

# ---------------------------------------------------------------------
# OpenTelemetry configuration
# ---------------------------------------------------------------------

resource = Resource.create(
    {
        "service.name": os.getenv("OTEL_SERVICE_NAME", "backend"),
        "service.version": "1.0.0",
        "deployment.environment": "development",
    }
)

provider = TracerProvider(resource=resource)
processor = BatchSpanProcessor(
    OTLPSpanExporter(
        endpoint=os.getenv("OTEL_EXPORTER_OTLP_ENDPOINT", "http://localhost:4317"),
        insecure=True,
    )
)
provider.add_span_processor(processor)
trace.set_tracer_provider(provider)

tracer = trace.get_tracer(__name__)

# ---------------------------------------------------------------------
# Flask app
# ---------------------------------------------------------------------

app = Flask(__name__)

FlaskInstrumentor().instrument_app(app)
RequestsInstrumentor().instrument()
Psycopg2Instrumentor().instrument()
RedisInstrumentor().instrument()

# ---------------------------------------------------------------------
# Connections
# ---------------------------------------------------------------------

DATABASE_URL = os.getenv(
    "DATABASE_URL", "postgresql://user:password@localhost:5432/demo"
)
REDIS_URL = os.getenv("REDIS_URL", "redis://localhost:6379")


def get_db_connection():
    return psycopg2.connect(DATABASE_URL)


def get_redis_connection():
    return redis.from_url(REDIS_URL)


# ---------------------------------------------------------------------
# Database init
# ---------------------------------------------------------------------

def init_db():
    with get_db_connection() as conn:
        with conn.cursor() as cur:
            cur.execute(
                """
                CREATE TABLE IF NOT EXISTS users (
                    id SERIAL PRIMARY KEY,
                    name VARCHAR(100),
                    email VARCHAR(100),
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
                """
            )
            cur.execute(
                """
                CREATE TABLE IF NOT EXISTS orders (
                    id SERIAL PRIMARY KEY,
                    user_id INTEGER REFERENCES users(id),
                    product VARCHAR(100),
                    amount DECIMAL(10, 2),
                    status VARCHAR(20),
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
                """
            )
            conn.commit()


# ---------------------------------------------------------------------
# Routes
# ---------------------------------------------------------------------

@app.route("/health")
def health():
    return jsonify({"status": "healthy"}), 200


@app.route("/api/users", methods=["GET"])
def get_users():
    with tracer.start_as_current_span("get_users") as span:
        time.sleep(random.uniform(0.01, 0.1))

        r = get_redis_connection()
        cached = r.get("users:all")

        if cached:
            span.set_attribute("cache.hit", True)
            return jsonify(json.loads(cached)), 200

        span.set_attribute("cache.hit", False)

        with get_db_connection() as conn:
            with conn.cursor() as cur:
                cur.execute("SELECT id, name, email FROM users")
                users = [
                    {"id": r[0], "name": r[1], "email": r[2]}
                    for r in cur.fetchall()
                ]

        r.setex("users:all", 60, json.dumps(users))
        return jsonify(users), 200


@app.route("/api/users/<int:user_id>", methods=["GET"])
def get_user(user_id):
    with tracer.start_as_current_span("get_user_by_id") as span:
        span.set_attribute("user.id", user_id)
        time.sleep(random.uniform(0.01, 0.05))

        with get_db_connection() as conn:
            with conn.cursor() as cur:
                cur.execute(
                    "SELECT id, name, email FROM users WHERE id = %s",
                    (user_id,),
                )
                row = cur.fetchone()

        if not row:
            return jsonify({"error": "User not found"}), 404

        return jsonify(
            {"id": row[0], "name": row[1], "email": row[2]}
        ), 200


@app.route("/api/users", methods=["POST"])
def create_user():
    with tracer.start_as_current_span("create_user") as span:
        data = request.json or {}

        if not data.get("name") or not data.get("email"):
            span.set_status(Status(StatusCode.ERROR))
            return jsonify({"error": "Name and email required"}), 400

        time.sleep(random.uniform(0.05, 0.15))

        with get_db_connection() as conn:
            with conn.cursor() as cur:
                cur.execute(
                    "INSERT INTO users (name, email) VALUES (%s, %s) RETURNING id",
                    (data["name"], data["email"]),
                )
                user_id = cur.fetchone()[0]
                conn.commit()

        get_redis_connection().delete("users:all")

        return jsonify(
            {"id": user_id, "name": data["name"], "email": data["email"]}
        ), 201


@app.route("/api/orders", methods=["POST"])
def create_order():
    with tracer.start_as_current_span("create_order") as span:
        data = request.json or {}

        user_id = data.get("user_id")
        product = data.get("product")
        amount = data.get("amount")

        if not all([user_id, product, amount]):
            span.set_status(Status(StatusCode.ERROR))
            return jsonify({"error": "Missing required fields"}), 400

        # Check user
        with get_db_connection() as conn:
            with conn.cursor() as cur:
                cur.execute("SELECT id FROM users WHERE id = %s", (user_id,))
                if not cur.fetchone():
                    return jsonify({"error": "User not found"}), 404

        # Payment simulation
        time.sleep(random.uniform(0.1, 0.5))

        # Save order
        with get_db_connection() as conn:
            with conn.cursor() as cur:
                cur.execute(
                    """
                    INSERT INTO orders (user_id, product, amount, status)
                    VALUES (%s, %s, %s, %s)
                    RETURNING id
                    """,
                    (user_id, product, amount, "completed"),
                )
                order_id = cur.fetchone()[0]
                conn.commit()

        return jsonify(
            {
                "id": order_id,
                "user_id": user_id,
                "product": product,
                "amount": amount,
                "status": "completed",
            }
        ), 201


@app.route("/api/orders/<int:order_id>", methods=["GET"])
def get_order(order_id):
    with tracer.start_as_current_span("get_order") as span:
        time.sleep(random.uniform(0.01, 0.05))

        with get_db_connection() as conn:
            with conn.cursor() as cur:
                cur.execute(
                    """
                    SELECT id, user_id, product, amount, status
                    FROM orders WHERE id = %s
                    """,
                    (order_id,),
                )
                row = cur.fetchone()

        if not row:
            return jsonify({"error": "Order not found"}), 404

        return jsonify(
            {
                "id": row[0],
                "user_id": row[1],
                "product": row[2],
                "amount": float(row[3]),
                "status": row[4],
            }
        ), 200


@app.route("/api/slow")
def slow_endpoint():
    with tracer.start_as_current_span("slow_endpoint") as span:
        delay = random.uniform(2, 5)
        time.sleep(delay)
        return jsonify({"delay": delay}), 200


@app.route("/api/error")
def error_endpoint():
    with tracer.start_as_current_span("error_endpoint") as span:
        try:
            raise RuntimeError("Simulated error for testing")
        except Exception as e:
            span.record_exception(e)
            span.set_status(Status(StatusCode.ERROR, str(e)))
            return jsonify({"error": str(e)}), 500


# ---------------------------------------------------------------------
# Entry point - –ó–∞–ø—É—Å–∫ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
# ---------------------------------------------------------------------

if __name__ == "__main__":
    init_db()
    app.run(host="0.0.0.0", port=5000)
```
```
````

4. **–°–æ–∑–¥–∞–π demo-app/frontend (–ø—Ä–æ—Å—Ç–æ–π HTML + JS)**:

`demo-app/frontend/Dockerfile`:
```dockerfile
FROM nginx:alpine

COPY index.html /usr/share/nginx/html/
COPY nginx.conf /etc/nginx/conf.d/default.conf

EXPOSE 8080
```

`demo-app/frontend/index.html`:
```html
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Tracing Demo App</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 800px;
            margin: 50px auto;
            padding: 20px;
            background-color: #f5f5f5;
        }
        .container {
            background: white;
            padding: 30px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        h1 {
            color: #333;
            border-bottom: 2px solid #007bff;
            padding-bottom: 10px;
        }
        .section {
            margin: 20px 0;
            padding: 20px;
            border: 1px solid #ddd;
            border-radius: 4px;
        }
        button {
            background-color: #007bff;
            color: white;
            padding: 10px 20px;
            border: none;
            border-radius: 4px;
            cursor: pointer;
            margin: 5px;
        }
        button:hover {
            background-color: #0056b3;
        }
        .error {
            background-color: #dc3545;
        }
        .error:hover {
            background-color: #c82333;
        }
        .slow {
            background-color: #ffc107;
        }
        .slow:hover {
            background-color: #e0a800;
        }
        #output {
            margin-top: 20px;
            padding: 15px;
            background-color: #f8f9fa;
            border-radius: 4px;
            min-height: 100px;
            white-space: pre-wrap;
            font-family: monospace;
        }
        input {
            padding: 8px;
            margin: 5px;
            border: 1px solid #ddd;
            border-radius: 4px;
            width: 200px;
        }
        .links {
            margin-top: 30px;
            padding: 20px;
            background-color: #e9ecef;
            border-radius: 4px;
        }
        .links a {
            display: block;
            margin: 10px 0;
            color: #007bff;
            text-decoration: none;
        }
        .links a:hover {
            text-decoration: underline;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>üîç Distributed Tracing Demo</h1>
        
        <div class="section">
            <h2>User Operations</h2>
            <button onclick="getUsers()">Get All Users</button>
            <button onclick="createUser()">Create Random User</button>
            <br>
            <input type="number" id="userId" placeholder="User ID">
            <button onclick="getUser()">Get User by ID</button>
        </div>
        
        <div class="section">
            <h2>Order Operations</h2>
            <input type="number" id="orderUserId" placeholder="User ID">
            <input type="text" id="product" placeholder="Product">
            <input type="number" id="amount" placeholder="Amount">
            <button onclick="createOrder()">Create Order</button>
            <br><br>
            <input type="number" id="orderId" placeholder="Order ID">
            <button onclick="getOrder()">Get Order by ID</button>
        </div>
        
        <div class="section">
            <h2>Test Scenarios</h2>
            <button class="slow" onclick="testSlow()">Test Slow Endpoint (2-5s)</button>
            <button class="error" onclick="testError()">Test Error Endpoint</button>
            <button onclick="stressTest()">Stress Test (10 requests)</button>
        </div>
        
        <div id="output">Response will appear here...</div>
        
        <div class="links">
            <h3>üìä Monitoring Links</h3>
            <a href="http://localhost:16686" target="_blank">üîç Jaeger UI - View Traces</a>
            <a href="http://localhost:3000" target="_blank">üìà Grafana - Metrics & Traces</a>
            <a href="http://localhost:5000/health" target="_blank">üíö Backend Health Check</a>
        </div>
    </div>

    <script>
        const API_URL = 'http://localhost:5000/api';
        const output = document.getElementById('output');

        function log(message, data = null) {
            const timestamp = new Date().toISOString();
            let logMessage = `[${timestamp}] ${message}`;
            if (data) {
                logMessage += '\n' + JSON.stringify(data, null, 2);
            }
            output.textContent = logMessage;
            console.log(message, data);
        }

        async function getUsers() {
            try {
                log('Fetching all users...');
                const response = await fetch(`${API_URL}/users`);
                const data = await response.json();
                log('‚úÖ Users retrieved:', data);
            } catch (error) {
                log('‚ùå Error:', error.message);
            }
        }

        async function getUser() {
            const userId = document.getElementById('userId').value;
            if (!userId) {
                log('‚ùå Please enter a user ID');
                return;
            }
            
            try {
                log(`Fetching user ${userId}...`);
                const response = await fetch(`${API_URL}/users/${userId}`);
                const data = await response.json();
                
                if (response.ok) {
                    log('‚úÖ User retrieved:', data);
                } else {
                    log('‚ùå Error:', data);
                }
            } catch (error) {
                log('‚ùå Error:', error.message);
            }
        }

        async function createUser() {
            const randomNum = Math.floor(Math.random() * 1000);
            const userData = {
                name: `User ${randomNum}`,
                email: `user${randomNum}@example.com`
            };
            
            try {
                log('Creating user...', userData);
                const response = await fetch(`${API_URL}/users`, {
                    method: 'POST',
                    headers: {'Content-Type': 'application/json'},
                    body: JSON.stringify(userData)
                });
                const data = await response.json();
                log('‚úÖ User created:', data);
            } catch (error) {
                log('‚ùå Error:', error.message);
            }
        }

        async function createOrder() {
            const orderData = {
                user_id: parseInt(document.getElementById('orderUserId').value),
                product: document.getElementById('product').value || 'Product',
                amount: parseFloat(document.getElementById('amount').value) || 99.99
            };
            
            if (!orderData.user_id) {
                log('‚ùå Please enter a user ID');
                return;
            }
            
            try {
                log('Creating order...', orderData);
                const response = await fetch(`${API_URL}/orders`, {
                    method: 'POST',
                    headers: {'Content-Type': 'application/json'},
                    body: JSON.stringify(orderData)
                });
                const data = await response.json();
                
                if (response.ok) {
                    log('‚úÖ Order created:', data);
                } else {
                    log('‚ùå Error:', data);
                }
            } catch (error) {
                log('‚ùå Error:', error.message);
            }
        }

        async function getOrder() {
            const orderId = document.getElementById('orderId').value;
            if (!orderId) {
                log('‚ùå Please enter an order ID');
                return;
            }
            
            try {
                log(`Fetching order ${orderId}...`);
                const response = await fetch(`${API_URL}/orders/${orderId}`);
                const data = await response.json();
                
                if (response.ok) {
                    log('‚úÖ Order retrieved:', data);
                } else {
                    log('‚ùå Error:', data);
                }
            } catch (error) {
                log('‚ùå Error:', error.message);
            }
        }

        async function testSlow() {
            try {
                log('‚è≥ Testing slow endpoint (this will take 2-5 seconds)...');
                const start = Date.now();
                const response = await fetch(`${API_URL}/slow`);
                const data = await response.json();
                const duration = ((Date.now() - start) / 1000).toFixed(2);
                log(`‚úÖ Slow endpoint completed in ${duration}s:`, data);
            } catch (error) {
                log('‚ùå Error:', error.message);
            }
        }

        async function testError() {
            try {
                log('üí• Testing error endpoint...');
                const response = await fetch(`${API_URL}/error`);
                const data = await response.json();
                log('‚ùå Expected error:', data);
            } catch (error) {
                log('‚ùå Error:', error.message);
            }
        }

        async function stressTest() {
            log('üî• Starting stress test with 10 parallel requests...');
            const promises = [];
            
            for (let i = 0; i < 10; i++) {
                promises.push(fetch(`${API_URL}/users`));
            }
            
            try {
                const start = Date.now();
                await Promise.all(promises);
                const duration = ((Date.now() - start) / 1000).toFixed(2);
                log(`‚úÖ Stress test completed in ${duration}s (10 requests)`);
            } catch (error) {
                log('‚ùå Stress test failed:', error.message);
            }
        }

        // Initial message
        log('üëã Welcome! Click any button to start generating traces.');
    </script>
</body>
</html>
```

`demo-app/frontend/nginx.conf`:
```nginx
server {
    listen 8080;
    server_name localhost;
    
    location / {
        root /usr/share/nginx/html;
        index index.html;
    }
    
    # CORS –¥–ª—è API –∑–∞–ø—Ä–æ—Å–æ–≤
    location /api {
        if ($request_method = 'OPTIONS') {
            add_header 'Access-Control-Allow-Origin' '*';
            add_header 'Access-Control-Allow-Methods' 'GET, POST, OPTIONS';
            add_header 'Access-Control-Allow-Headers' 'Content-Type';
            return 204;
        }
    }
}
```

5. **–°–æ–∑–¥–∞–π grafana-datasources.yml**:
```yaml
apiVersion: 1

datasources:
  - name: Jaeger
    type: jaeger
    access: proxy
    url: http://jaeger:16686
    isDefault: true
    editable: true
    jsonData:
      tracesToLogsV2:
        datasourceUid: 'loki'
        spanStartTimeShift: '-1h'
        spanEndTimeShift: '1h'
        filterByTraceID: true
        filterBySpanID: false
```

6. **–ó–∞–ø—É—Å—Ç–∏ –∏ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–π**:
```bash
# –°–æ–∑–¥–∞–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
mkdir -p demo-app/frontend demo-app/backend

# –ó–∞–ø—É—Å—Ç–∏ stack
docker-compose up -d

# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞
docker-compose ps

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –ª–æ–≥–æ–≤
docker-compose logs -f backend

# –ü—Ä–æ–≤–µ—Ä–∫–∞ Jaeger
curl http://localhost:16686

# –ü—Ä–æ–≤–µ—Ä–∫–∞ backend health
curl http://localhost:5000/health
```

7. **–û—Ç–∫—Ä–æ–π UI –∏ —Ç–µ—Å—Ç–∏—Ä—É–π**:
```bash
# Frontend demo app
open http://localhost:8080

# Jaeger UI
open http://localhost:16686

# Grafana
open http://localhost:3000

# –°–æ–∑–¥–∞–π —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
curl -X POST http://localhost:5000/api/users \
  -H "Content-Type: application/json" \
  -d '{"name": "Test User", "email": "test@example.com"}'

curl -X POST http://localhost:5000/api/orders \
  -H "Content-Type: application/json" \
  -d '{"user_id": 1, "product": "Test Product", "amount": 99.99}'
```

8. **–ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–π—Å–æ–≤ –≤ Jaeger**:
````

1. –û—Ç–∫—Ä–æ–π Jaeger UI: [http://localhost:16686](http://localhost:16686)
2. Search traces:
    - Service: backend
    - Operation: create_order
    - Min Duration: 1s (–¥–ª—è –º–µ–¥–ª–µ–Ω–Ω—ã—Ö)
    - Tags: error=true (–¥–ª—è –æ—à–∏–±–æ–∫)
3. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–π:
    - Timeline view - –≥–¥–µ –≤—Ä–µ–º—è —Ç—Ä–∞—Ç–∏—Ç—Å—è
    - Span details - –∞—Ç—Ä–∏–±—É—Ç—ã, —Å–æ–±—ã—Ç–∏—è, –æ—à–∏–±–∫–∏
    - Service graph - –∫–∞—Ä—Ç–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
    - Trace comparison - —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –±—ã—Å—Ç—Ä—ã—Ö –∏ –º–µ–¥–ª–µ–Ω–Ω—ã—Ö

````

### üöÄ –ë–æ–Ω—É—Å (–Ω–æ–≤–æ–µ)

**1. –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è Tempo (–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ Jaeger)**:

–î–æ–±–∞–≤—å –≤ `docker-compose.yml`:
```yaml
  tempo:
    image: grafana/tempo:2.3.1
    container_name: tempo
    command: ["-config.file=/etc/tempo.yaml"]
    volumes:
      - ./tempo.yaml:/etc/tempo.yaml
      - tempo-data:/tmp/tempo
    ports:
      - "3200:3200"   # Tempo UI
      - "4317:4317"   # OTLP gRPC
      - "4318:4318"   # OTLP HTTP
    restart: unless-stopped

volumes:
  tempo-data:
```

`tempo.yaml`:
```yaml
server:
  http_listen_port: 3200

distributor:
  receivers:
    otlp:
      protocols:
        grpc:
          endpoint: 0.0.0.0:4317
        http:
          endpoint: 0.0.0.0:4318

ingester:
  max_block_duration: 5m

compactor:
  compaction:
    block_retention: 168h  # 7 days

storage:
  trace:
    backend: local
    local:
      path: /tmp/tempo/blocks
    wal:
      path: /tmp/tempo/wal

metrics_generator:
  registry:
    external_labels:
      source: tempo
  storage:
    path: /tmp/tempo/generator/wal
  traces_storage:
    path: /tmp/tempo/generator/traces
```

**2. –°–æ–∑–¥–∞–π Python —Å–∫—Ä–∏–ø—Ç –¥–ª—è load testing —Å —Ç—Ä–µ–π—Å–∏–Ω–≥–æ–º**:

`load_test.py`:
```python
#!/usr/bin/env python3
"""
Load testing —Å –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π —Ç—Ä–µ–π—Å–æ–≤
"""
import concurrent.futures
import requests
import time
import random
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter
from opentelemetry.sdk.resources import Resource

# Setup tracing
resource = Resource.create({"service.name": "load-tester"})
provider = TracerProvider(resource=resource)
processor = BatchSpanProcessor(
    OTLPSpanExporter(endpoint="http://localhost:4317", insecure=True)
)
provider.add_span_processor(processor)
trace.set_tracer_provider(provider)
tracer = trace.get_tracer(__name__)

API_URL = "http://localhost:5000/api"

def make_request(endpoint, method="GET", data=None):
    """–î–µ–ª–∞–µ—Ç –∑–∞–ø—Ä–æ—Å —Å —Ç—Ä–µ–π—Å–∏–Ω–≥–æ–º"""
    with tracer.start_as_current_span(f"{method} {endpoint}") as span:
        span.set_attribute("http.method", method)
        span.set_attribute("http.url", f"{API_URL}{endpoint}")
        
        try:
            if method == "GET":
                response = requests.get(f"{API_URL}{endpoint}")
            else:
                response = requests.post(
                    f"{API_URL}{endpoint}",
                    json=data,
                    headers={"Content-Type": "application/json"}
                )
            
            span.set_attribute("http.status_code", response.status_code)
            
            if response.status_code >= 400:
                span.set_attribute("error", True)
                
            return response
            
        except Exception as e:
            span.record_exception(e)
            span.set_attribute("error", True)
            raise

def user_flow():
    """–°–∏–º—É–ª–∏—Ä—É–µ—Ç —Ç–∏–ø–∏—á–Ω—ã–π user flow"""
    with tracer.start_as_current_span("user_flow") as span:
        # 1. –°–æ–∑–¥–∞–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        user_data = {
            "name": f"LoadTest User {random.randint(1, 1000)}",
            "email": f"test{random.randint(1, 1000)}@example.com"
        }
        response = make_request("/users", "POST", user_data)
        
        if response.status_code != 201:
            span.set_attribute("flow.failed", True)
            return
        
        user_id = response.json()["id"]
        span.set_attribute("user.id", user_id)
        
        # 2. –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        time.sleep(random.uniform(0.1, 0.5))
        make_request(f"/users/{user_id}")
        
        # 3. –°–æ–∑–¥–∞–µ–º –∑–∞–∫–∞–∑
        time.sleep(random.uniform(0.1, 0.5))
        order_data = {
            "user_id": user_id,
            "product": f"Product {random.randint(1, 100)}",
            "amount": round(random.uniform(10, 500), 2)
        }
        response = make_request("/orders", "POST", order_data)
        
        if response.status_code == 201:
            order_id = response.json()["id"]
            span.set_attribute("order.id", order_id)
            
            # 4. –ü–æ–ª—É—á–∞–µ–º –∑–∞–∫–∞–∑
            time.sleep(random.uniform(0.1, 0.5))
            make_request(f"/orders/{order_id}")
        
        span.set_attribute("flow.completed", True)

def run_load_test(num_users=10, concurrent=5):
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç load test"""
    print(f"Starting load test: {num_users} users, {concurrent} concurrent")
    
    start_time = time.time()
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=concurrent) as executor:
        futures = [executor.submit(user_flow) for _ in range(num_users)]
        
        for future in concurrent.futures.as_completed(futures):
            try:
                future.result()
            except Exception as e:
                print(f"Error: {e}")
    
    duration = time.time() - start_time
    print(f"Load test completed in {duration:.2f}s")
    print(f"Average: {duration/num_users:.2f}s per user")
    print(f"Throughput: {num_users/duration:.2f} users/sec")

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='Load testing with tracing')
    parser.add_argument('--users', type=int, default=10, help='Number of users')
    parser.add_argument('--concurrent', type=int, default=5, help='Concurrent requests')
    
    args = parser.parse_args()
    
    run_load_test(num_users=args.users, concurrent=args.concurrent)
```

**3. –°–æ–∑–¥–∞–π dashboard –¥–ª—è APM –≤ Grafana**:

`grafana-dashboards/apm-dashboard.json`:
```json
{
  "dashboard": {
    "title": "Application Performance Monitoring",
    "panels": [
      {
        "id": 1,
        "title": "Request Rate",
        "targets": [
          {
            "expr": "sum(rate(traces_spanmetrics_calls_total[5m])) by (service_name)"
          }
        ],
        "type": "timeseries"
      },
      {
        "id": 2,
        "title": "Error Rate",
        "targets": [
          {
            "expr": "sum(rate(traces_spanmetrics_calls_total{status_code=\"STATUS_CODE_ERROR\"}[5m])) by (service_name)"
          }
        ],
        "type": "timeseries"
      },
      {
        "id": 3,
        "title": "Latency (p95)",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, sum(rate(traces_spanmetrics_latency_bucket[5m])) by (le, service_name))"
          }
        ],
        "type": "timeseries"
      },
      {
        "id": 4,
        "title": "Service Map",
        "type": "nodeGraph",
        "targets": [
          {
            "queryType": "serviceMap"
          }
        ]
      }
    ]
  }
}
```

**4. Continuous Profiling —Å Pyroscope**:

–î–æ–±–∞–≤—å –≤ `docker-compose.yml`:
```yaml
  pyroscope:
    image: grafana/pyroscope:latest
    container_name: pyroscope
    ports:
      - "4040:4040"
    restart: unless-stopped
```

–û–±–Ω–æ–≤–∏ Python app –¥–ª—è –ø—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏—è:
```python
import pyroscope

pyroscope.configure(
    application_name="backend",
    server_address="http://pyroscope:4040",
    tags={
        "environment": "development",
    }
)
```

---

## –ò—Ç–æ–≥–∏ –º–æ–¥—É–ª—è 6

–ü–æ—Å–ª–µ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏—è —ç—Ç–æ–≥–æ –º–æ–¥—É–ª—è —Ç—ã –¥–æ–ª–∂–µ–Ω —É–º–µ—Ç—å:

‚úÖ –ü–æ–Ω–∏–º–∞—Ç—å –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ distributed tracing (trace, span, context)
‚úÖ –ù–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å OpenTelemetry –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è—Ö
‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Jaeger –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç—Ä–µ–π—Å–æ–≤
‚úÖ –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å —Ç—Ä–µ–π—Å—ã —Å –ª–æ–≥–∞–º–∏ –∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏
‚úÖ –ù–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å sampling –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Ö—Ä–∞–Ω–µ–Ω–∏—è
‚úÖ –ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å performance bottlenecks
‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Service Map –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
‚úÖ –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å continuous profiling
‚úÖ –°–æ–∑–¥–∞–≤–∞—Ç—å APM dashboards
‚úÖ –û—Ç–ª–∞–∂–∏–≤–∞—Ç—å –ø—Ä–æ–±–ª–µ–º—ã –≤ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö —Å–∏—Å—Ç–µ–º–∞—Ö

**–ö–ª—é—á–µ–≤—ã–µ takeaways:**
1. –¢—Ä–µ–π—Å–∏–Ω–≥ –∫—Ä–∏—Ç–∏—á–µ–Ω –¥–ª—è –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å–æ–≤ - –±–µ–∑ –Ω–µ–≥–æ –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ –æ—Ç–ª–∞–¥–∏—Ç—å –ø—Ä–æ–±–ª–µ–º—ã
2. OpenTelemetry - —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç, –∏—Å–ø–æ–ª—å–∑—É–π –µ–≥–æ
3. –í—Å–µ–≥–¥–∞ —Å–æ—Ö—Ä–∞–Ω—è–π –æ—à–∏–±–∫–∏ –∏ –º–µ–¥–ª–µ–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã (tail sampling)
4. –°–≤—è–∑—ã–≤–∞–π —Ç—Ä–µ–π—Å—ã —Å –ª–æ–≥–∞–º–∏ —á–µ—Ä–µ–∑ trace_id
5. –ò—Å–ø–æ–ª—å–∑—É–π semantic conventions –¥–ª—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏
6. Service Map –ø–æ–º–æ–≥–∞–µ—Ç –ø–æ–Ω—è—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É —Å–∏—Å—Ç–µ–º—ã
7. –ü—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–æ–ø–æ–ª–Ω—è–µ—Ç —Ç—Ä–µ–π—Å–∏–Ω–≥ –¥–ª—è deep analysis
8. –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π sampling —ç–∫–æ–Ω–æ–º–∏—Ç –¥–µ–Ω—å–≥–∏ –∏ storage


## –ú–æ–¥—É–ª—å 7: Application Performance Monitoring (APM) (30 –º–∏–Ω—É—Ç)

### üéØ –ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞

**APM - —á—Ç–æ –º–æ–Ω–∏—Ç–æ—Ä–∏–º:**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ User Experience                     ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ ‚Ä¢ Page Load Time                    ‚îÇ
‚îÇ ‚Ä¢ Time to First Byte (TTFB)        ‚îÇ
‚îÇ ‚Ä¢ First Contentful Paint (FCP)     ‚îÇ
‚îÇ ‚Ä¢ Largest Contentful Paint (LCP)   ‚îÇ
‚îÇ ‚Ä¢ Cumulative Layout Shift (CLS)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Application Layer                   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ ‚Ä¢ Request Rate                      ‚îÇ
‚îÇ ‚Ä¢ Error Rate                        ‚îÇ
‚îÇ ‚Ä¢ Response Time (p50, p95, p99)    ‚îÇ
‚îÇ ‚Ä¢ Throughput                        ‚îÇ
‚îÇ ‚Ä¢ Apdex Score                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Code Level                          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ ‚Ä¢ Function Execution Time           ‚îÇ
‚îÇ ‚Ä¢ Database Query Performance        ‚îÇ
‚îÇ ‚Ä¢ External API Calls                ‚îÇ
‚îÇ ‚Ä¢ Memory Allocations                ‚îÇ
‚îÇ ‚Ä¢ CPU Profiling                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**–ö–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ APM:**

**1. Apdex Score** (Application Performance Index):

```
Apdex = (Satisfied + Tolerating/2) / Total Requests

Satisfied:   Response time ‚â§ T (target)
Tolerating:  T < Response time ‚â§ 4T
Frustrated:  Response time > 4T

–ü—Ä–∏–º–µ—Ä: T = 0.5s
- 0.3s  ‚Üí Satisfied
- 1.2s  ‚Üí Tolerating
- 3.0s  ‚Üí Frustrated

Score: 0.0-1.0 (1.0 = –∏–¥–µ–∞–ª—å–Ω–æ)
```

**2. Percentiles:**

```
p50 (median)  - 50% requests –±—ã—Å—Ç—Ä–µ–µ
p95           - 95% requests –±—ã—Å—Ç—Ä–µ–µ (—Ö–æ—Ä–æ—à–æ –¥–ª—è SLA)
p99           - 99% requests –±—ã—Å—Ç—Ä–µ–µ (tail latency)
p99.9         - –¥–ª—è –∫—Ä–∏—Ç–∏—á–Ω—ã—Ö —Å–∏—Å—Ç–µ–º

–ü–æ—á–µ–º—É –Ω–µ —Å—Ä–µ–¥–Ω–µ–µ?
Average: [10ms, 10ms, 10ms, 5000ms] = 1257ms
p95:     [10ms, 10ms, 10ms, 5000ms] = 10ms
```

**3. Service Level Objectives (SLO):**

yaml

```yaml
SLI (Indicator):   Availability = successful_requests / total_requests
SLO (Objective):   99.9% availability
SLA (Agreement):   99.9% or credit

Error Budget:      0.1% = 43 minutes/month downtime
```

**4. Golden Signals –¥–ª—è APM:**

yaml

````yaml
Latency:      How long to process requests
Traffic:      How many requests
Errors:       Rate of failed requests  
Saturation:   How "full" your service is
````

**–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã APM:**
````
Commercial:
- New Relic
- Datadog APM
- Dynatrace
- AppDynamics

Open Source:
- Elastic APM
- SigNoz
- Grafana Tempo + Prometheus
- Jaeger
````

### üíª –ó–∞–¥–∞–Ω–∏–µ

–ù–∞—Å—Ç—Ä–æ–π –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π APM —Å Elastic Stack:

1. **–î–æ–±–∞–≤—å Elastic APM –≤ docker-compose.yml**:

yaml

```yaml
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    container_name: elasticsearch
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    ports:
      - "9200:9200"
    volumes:
      - elasticsearch-data:/usr/share/elasticsearch/data
    restart: unless-stopped

  kibana:
    image: docker.elastic.co/kibana/kibana:8.11.0
    container_name: kibana
    ports:
      - "5601:5601"
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    depends_on:
      - elasticsearch
    restart: unless-stopped

  apm-server:
    image: docker.elastic.co/apm/apm-server:8.11.0
    container_name: apm-server
    ports:
      - "8200:8200"
    command: >
      apm-server -e
        -E apm-server.rum.enabled=true
        -E apm-server.host=0.0.0.0:8200
        -E output.elasticsearch.hosts=["elasticsearch:9200"]
        -E apm-server.kibana.enabled=true
        -E apm-server.kibana.host=kibana:5601
    depends_on:
      - elasticsearch
      - kibana
    restart: unless-stopped

volumes:
  elasticsearch-data:
```

2. **–°–æ–∑–¥–∞–π –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ —Å Elastic APM instrumentation**:

**Python/Flask example:**

python

```python
# app_with_apm.py
from flask import Flask, request, jsonify
from elasticapm.contrib.flask import ElasticAPM
import time
import random
import psycopg2
from redis import Redis

app = Flask(__name__)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è Elastic APM
app.config['ELASTIC_APM'] = {
    'SERVICE_NAME': 'my-flask-app',
    'SERVER_URL': 'http://apm-server:8200',
    'ENVIRONMENT': 'production',
    'CAPTURE_BODY': 'all',
    'TRANSACTION_SAMPLE_RATE': 1.0,  # 100% –≤ dev, 0.1 –≤ prod
}

apm = ElasticAPM(app)
redis_client = Redis(host='redis', port=6379, decode_responses=True)

@app.route('/')
def index():
    return jsonify({
        'status': 'ok',
        'service': 'my-flask-app'
    })

@app.route('/api/users')
def get_users():
    """Endpoint —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –æ–ø–µ—Ä–∞—Ü–∏—è–º–∏ –¥–ª—è APM"""
    
    # Custom span –¥–ª—è –∫–µ—à–∞
    with apm.capture_span('check_cache', span_type='cache'):
        cache_key = 'users:all'
        cached = redis_client.get(cache_key)
        
        if cached:
            apm.tag(cache='hit')
            return jsonify({'users': eval(cached), 'source': 'cache'})
        
        apm.tag(cache='miss')
    
    # Database query (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–µ—Ç—Å—è)
    users = fetch_users_from_db()
    
    # External API call
    with apm.capture_span('enrich_user_data', span_type='external.http'):
        enriched = enrich_users(users)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫–µ—à
    with apm.capture_span('save_to_cache', span_type='cache'):
        redis_client.setex(cache_key, 300, str(enriched))
    
    return jsonify({'users': enriched, 'source': 'database'})

@app.route('/api/slow')
def slow_endpoint():
    """–ú–µ–¥–ª–µ–Ω–Ω—ã–π endpoint –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    # –°–∏–º—É–ª—è—Ü–∏—è –º–µ–¥–ª–µ–Ω–Ω–æ–π –æ–ø–µ—Ä–∞—Ü–∏–∏
    time.sleep(random.uniform(2, 5))
    return jsonify({'message': 'This was slow'})

@app.route('/api/error')
def error_endpoint():
    """Endpoint —Å –æ—à–∏–±–∫–∞–º–∏"""
    if random.random() < 0.5:
        raise Exception("Random error occurred!")
    return jsonify({'message': 'Success'})

@app.route('/api/transactions')
def complex_transaction():
    """–°–ª–æ–∂–Ω–∞—è —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—è —Å –º–Ω–æ–∂–µ—Å—Ç–≤–æ–º –æ–ø–µ—Ä–∞—Ü–∏–π"""
    
    # –®–∞–≥ 1: –í–∞–ª–∏–¥–∞—Ü–∏—è
    with apm.capture_span('validate_request', span_type='app'):
        time.sleep(0.05)
        apm.label(validation='passed')
    
    # –®–∞–≥ 2: –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    with apm.capture_span('fetch_data', span_type='db.postgresql'):
        conn = get_db_connection()
        cursor = conn.cursor()
        cursor.execute("SELECT * FROM transactions LIMIT 10")
        data = cursor.fetchall()
        cursor.close()
        conn.close()
    
    # –®–∞–≥ 3: –û–±—Ä–∞–±–æ—Ç–∫–∞
    with apm.capture_span('process_data', span_type='app'):
        time.sleep(0.1)
        processed = [{'id': row[0], 'amount': row[1]} for row in data]
    
    # –®–∞–≥ 4: –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
    with apm.capture_span('save_result', span_type='cache'):
        redis_client.setex(f'tx:result:{random.randint(1,1000)}', 
                          600, 
                          str(processed))
    
    return jsonify({'transactions': processed, 'count': len(processed)})

def fetch_users_from_db():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∏–∑ –ë–î"""
    # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞—Ü–∏—è DB queries
    conn = get_db_connection()
    cursor = conn.cursor()
    
    # –ú–µ–¥–ª–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å
    time.sleep(random.uniform(0.1, 0.3))
    cursor.execute("SELECT id, name, email FROM users")
    
    users = [
        {'id': row[0], 'name': row[1], 'email': row[2]}
        for row in cursor.fetchall()
    ]
    
    cursor.close()
    conn.close()
    return users

def enrich_users(users):
    """–û–±–æ–≥–∞—â–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π"""
    import requests
    
    # APM –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –æ—Ç—Å–ª–µ–¥–∏—Ç HTTP requests
    for user in users:
        try:
            # –í–Ω–µ—à–Ω–∏–π API –≤—ã–∑–æ–≤
            response = requests.get(
                f'http://external-api:8000/user/{user["id"]}/details',
                timeout=1
            )
            if response.ok:
                user['details'] = response.json()
        except Exception as e:
            apm.capture_exception()
            user['details'] = None
    
    return users

def get_db_connection():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –ë–î"""
    return psycopg2.connect(
        host='postgres',
        database='mydb',
        user='user',
        password='password'
    )

# Custom –º–µ—Ç—Ä–∏–∫–∏
@app.before_request
def before_request():
    """–î–æ–±–∞–≤–ª—è–µ–º custom tags –∫ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏"""
    elasticapm.set_user_context(
        user_id=request.headers.get('X-User-ID'),
        username=request.headers.get('X-Username')
    )
    
    elasticapm.set_custom_context({
        'user_agent': request.headers.get('User-Agent'),
        'request_id': request.headers.get('X-Request-ID'),
        'api_version': 'v1'
    })

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000, debug=False)
```

**Node.js/Express example:**

javascript

```javascript
// app.js
const express = require('express');
const apm = require('elastic-apm-node').start({
  serviceName: 'my-node-app',
  serverUrl: 'http://apm-server:8200',
  environment: 'production',
  captureBody: 'all',
  transactionSampleRate: 1.0
});

const app = express();
const redis = require('redis');
const { Pool } = require('pg');

const redisClient = redis.createClient({
  host: 'redis',
  port: 6379
});

const pgPool = new Pool({
  host: 'postgres',
  database: 'mydb',
  user: 'user',
  password: 'password'
});

app.use(express.json());

app.get('/api/products', async (req, res) => {
  // Custom span
  const span = apm.startSpan('fetch_products', 'db');
  
  try {
    // Check cache
    const cached = await redisClient.get('products:all');
    if (cached) {
      apm.setLabel('cache', 'hit');
      if (span) span.end();
      return res.json(JSON.parse(cached));
    }
    
    apm.setLabel('cache', 'miss');
    
    // Fetch from DB
    const result = await pgPool.query('SELECT * FROM products LIMIT 100');
    const products = result.rows;
    
    // Save to cache
    await redisClient.setex('products:all', 300, JSON.stringify(products));
    
    if (span) span.end();
    res.json(products);
    
  } catch (error) {
    apm.captureError(error);
    if (span) span.end();
    res.status(500).json({ error: error.message });
  }
});

app.get('/api/checkout', async (req, res) => {
  const transaction = apm.currentTransaction;
  
  // Set custom context
  transaction.setCustomContext({
    cart_items: req.query.items,
    payment_method: req.query.payment
  });
  
  // Multiple spans
  const validateSpan = apm.startSpan('validate_cart', 'app');
  await simulateWork(100);
  if (validateSpan) validateSpan.end();
  
  const inventorySpan = apm.startSpan('check_inventory', 'db');
  await simulateWork(200);
  if (inventorySpan) inventorySpan.end();
  
  const paymentSpan = apm.startSpan('process_payment', 'external');
  await simulateWork(500);
  if (paymentSpan) paymentSpan.end();
  
  res.json({ order_id: Math.random().toString(36) });
});

function simulateWork(ms) {
  return new Promise(resolve => setTimeout(resolve, ms));
}

app.listen(3000, () => {
  console.log('Server running on port 3000');
});
```

3. **–ó–∞–ø—É—Å—Ç–∏ Elastic APM Stack**:

bash

````bash
# –ó–∞–ø—É—Å—Ç–∏ –≤—Å–µ —Å–µ—Ä–≤–∏—Å—ã
docker-compose up -d elasticsearch kibana apm-server

# –ü–æ–¥–æ–∂–¥–∏ –ø–æ–∫–∞ –ø–æ–¥–Ω–∏–º—É—Ç—Å—è (30-60 —Å–µ–∫)
docker logs -f kibana

# –ó–∞–ø—É—Å—Ç–∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ
docker-compose up -d app

# –°–≥–µ–Ω–µ—Ä–∏—Ä—É–π —Ç—Ä–∞—Ñ–∏–∫
for i in {1..100}; do
  curl http://localhost:5000/api/users
  curl http://localhost:5000/api/transactions
  curl http://localhost:5000/api/slow
  curl http://localhost:5000/api/error || true
  sleep 0.5
done
````

4. **–û—Ç–∫—Ä–æ–π Kibana –∏ –Ω–∞—Å—Ç—Ä–æ–π APM**:
````
1. –û—Ç–∫—Ä–æ–π: http://localhost:5601
2. –ü–µ—Ä–µ–π–¥–∏ –≤: Observability ‚Üí APM
3. –í—ã–±–µ—Ä–∏ —Å–µ—Ä–≤–∏—Å: my-flask-app
4. –ò–∑—É—á–∏:
   - Transactions: —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –æ—Ç–≤–µ—Ç–∞
   - Errors: –≤—Å–µ –æ—à–∏–±–∫–∏ —Å stacktrace
   - Metrics: throughput, latency, error rate
   - Service Map: –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –º–µ–∂–¥—É —Å–µ—Ä–≤–∏—Å–∞–º–∏
````

5. **–°–æ–∑–¥–∞–π –¥–∞—à–±–æ—Ä–¥ –¥–ª—è APM –º–µ—Ç—Ä–∏–∫ –≤ Grafana**:

bash

```bash
# APM –º–µ—Ç—Ä–∏–∫–∏ –¥–æ—Å—Ç—É–ø–Ω—ã —á–µ—Ä–µ–∑ Elasticsearch
# –ù–∞—Å—Ç—Ä–æ–π Elasticsearch data source –≤ Grafana
# URL: http://elasticsearch:9200
# Index: apm-*
```

Queries –¥–ª—è Grafana:

json

```json
// Average response time
{
  "query": {
    "bool": {
      "filter": [
        {"range": {"@timestamp": {"gte": "now-15m"}}},
        {"term": {"processor.event": "transaction"}}
      ]
    }
  },
  "aggs": {
    "response_time": {
      "date_histogram": {"field": "@timestamp", "interval": "1m"},
      "aggs": {
        "avg_duration": {"avg": {"field": "transaction.duration.us"}}
      }
    }
  }
}

// Error rate
{
  "query": {
    "bool": {
      "filter": [
        {"range": {"@timestamp": {"gte": "now-15m"}}},
        {"term": {"processor.event": "error"}}
      ]
    }
  },
  "aggs": {
    "errors_over_time": {
      "date_histogram": {"field": "@timestamp", "interval": "1m"},
      "aggs": {
        "error_count": {"value_count": {"field": "error.id"}}
      }
    }
  }
}
```

### üöÄ –ë–æ–Ω—É—Å (–Ω–æ–≤–æ–µ)

**1. –ù–∞—Å—Ç—Ä–æ–π Real User Monitoring (RUM)**:

**Frontend instrumentation:**

html

```html
<!-- index.html -->
<!DOCTYPE html>
<html>
<head>
    <title>My App with RUM</title>
    <script src="https://unpkg.com/@elastic/apm-rum@5.12.0/dist/bundles/elastic-apm-rum.umd.min.js"></script>
    <script>
        // –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è RUM
        var apm = window.elasticApm.init({
            serviceName: 'my-frontend',
            serverUrl: 'http://localhost:8200',
            serviceVersion: '1.0.0',
            environment: 'production'
        });
        
        // Custom transaction
        function searchProducts(query) {
            var transaction = apm.startTransaction('Search Products', 'custom');
            
            // Span –¥–ª—è API call
            var span = apm.startSpan('API Call', 'external.http');
            
            fetch('/api/search?q=' + query)
                .then(response => response.json())
                .then(data => {
                    if (span) span.end();
                    if (transaction) transaction.end();
                    displayResults(data);
                })
                .catch(error => {
                    apm.captureError(error);
                    if (span) span.end();
                    if (transaction) transaction.end();
                });
        }
        
        // Track user actions
        document.addEventListener('click', function(e) {
            if (e.target.matches('.product-item')) {
                apm.setCustomContext({
                    product_id: e.target.dataset.productId,
                    product_name: e.target.dataset.productName
                });
            }
        });
        
        // Web Vitals
        apm.observe('longtask', function(list) {
            list.getEntries().forEach(function(entry) {
                apm.captureError(new Error('Long Task detected: ' + entry.duration + 'ms'));
            });
        });
    </script>
</head>
<body>
    <h1>My Application</h1>
    <input type="text" id="search" placeholder="Search products...">
    <div id="results"></div>
    
    <script>
        document.getElementById('search').addEventListener('input', function(e) {
            searchProducts(e.target.value);
        });
    </script>
</body>
</html>
```

**2. –°–æ–∑–¥–∞–π custom metrics –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏**:

python

```python
from elasticapm import Client

apm_client = Client({
    'SERVICE_NAME': 'my-app',
    'SERVER_URL': 'http://apm-server:8200'
})

# Custom –º–µ—Ç—Ä–∏–∫–∏
class MetricsCollector:
    def __init__(self, apm_client):
        self.apm = apm_client
    
    def track_business_metric(self, metric_name, value, labels=None):
        """–û—Ç–ø—Ä–∞–≤–∫–∞ –±–∏–∑–Ω–µ—Å-–º–µ—Ç—Ä–∏–∫–∏"""
        self.apm.gauge(metric_name, value, labels=labels)
    
    def track_cart_value(self, user_id, cart_total):
        """–¢—Ä–µ–∫–∏–Ω–≥ —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –∫–æ—Ä–∑–∏–Ω—ã"""
        self.apm.gauge('cart.total', cart_total, labels={
            'user_id': user_id
        })
    
    def track_conversion(self, funnel_step, success):
        """–¢—Ä–µ–∫–∏–Ω–≥ –∫–æ–Ω–≤–µ—Ä—Å–∏–∏"""
        self.apm.counter('conversion.steps', labels={
            'step': funnel_step,
            'success': str(success)
        })

metrics = MetricsCollector(apm_client)

@app.route('/api/add-to-cart')
def add_to_cart():
    # ... –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ –∫–æ—Ä–∑–∏–Ω—É
    
    # –¢—Ä–µ–∫–∏–Ω–≥ –º–µ—Ç—Ä–∏–∫–∏
    metrics.track_cart_value(
        user_id=request.headers.get('X-User-ID'),
        cart_total=calculate_cart_total()
    )
    
    metrics.track_conversion('add_to_cart', True)
    
    return jsonify({'success': True})
```

**3. –ù–∞—Å—Ç—Ä–æ–π SLO –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥**:

python

```python
# slo_monitor.py
from elasticapm import Client
import time

class SLOMonitor:
    """–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ SLO/SLA"""
    
    def __init__(self, apm_client):
        self.apm = apm_client
        self.slo_target = 0.999  # 99.9%
        self.latency_target_ms = 500
    
    def check_availability_slo(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ SLO –ø–æ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏"""
        total_requests = self.get_total_requests()
        successful_requests = self.get_successful_requests()
        
        availability = successful_requests / total_requests if total_requests > 0 else 1.0
        error_budget_remaining = (self.slo_target - availability) * total_requests
        
        self.apm.gauge('slo.availability', availability)
        self.apm.gauge('slo.error_budget_remaining', error_budget_remaining)
        
        if availability < self.slo_target:
            self.apm.capture_message(
                f'SLO violation: Availability {availability:.4f} below target {self.slo_target}',
                level='warning'
            )
        
        return {
            'availability': availability,
            'target': self.slo_target,
            'error_budget': error_budget_remaining,
            'in_compliance': availability >= self.slo_target
        }
    
    def check_latency_slo(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ SLO –ø–æ –ª–∞—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏"""
        p95_latency = self.get_p95_latency()
        
        self.apm.gauge('slo.latency_p95', p95_latency)
        
        if p95_latency > self.latency_target_ms:
            self.apm.capture_message(
                f'SLO violation: p95 latency {p95_latency}ms above target {self.latency_target_ms}ms',
                level='warning'
            )
        
        return {
            'p95_latency': p95_latency,
            'target': self.latency_target_ms,
            'in_compliance': p95_latency <= self.latency_target_ms
        }

# –ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ SLO
def monitor_slo():
    monitor = SLOMonitor(apm_client)
    
    while True:
        availability_status = monitor.check_availability_slo()
        latency_status = monitor.check_latency_slo()
        
        print(f"Availability SLO: {availability_status}")
        print(f"Latency SLO: {latency_status}")
        
        time.sleep(60)  # –ö–∞–∂–¥—É—é –º–∏–Ω—É—Ç—É
```

**4. –ü—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏**:

python

````python
from elasticapm.contrib.flask import ElasticAPM
import cProfile
import pstats
from io import StringIO

@app.route('/api/profile')
def profile_endpoint():
    """Endpoint —Å –ø—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    
    # CPU profiling
    profiler = cProfile.Profile()
    profiler.enable()
    
    # –ö–æ–¥ –¥–ª—è –ø—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏—è
    result = expensive_operation()
    
    profiler.disable()
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ APM
    s = StringIO()
    stats = pstats.Stats(profiler, stream=s)
    stats.sort_stats('cumulative')
    stats.print_stats(20)
    
    apm.capture_message(
        'Profile results',
        custom={'profile': s.getvalue()}
    )
    
    return result

# Memory profiling
from memory_profiler import profile

@profile  # –î–µ–∫–æ—Ä–∞—Ç–æ—Ä –¥–ª—è memory profiling
def memory_intensive_operation():
    large_list = [i for i in range(1000000)]
    return sum(large_list)
````

---

## –ú–æ–¥—É–ª—å 8: –°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ Uptime (25 –º–∏–Ω—É—Ç)

### üéØ –ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞

**–°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ vs Real User Monitoring:**
````
Synthetic Monitoring:
‚úì –ü—Ä–æ–∞–∫—Ç–∏–≤–Ω—ã–π (–æ–±–Ω–∞—Ä—É–∂–∏–≤–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—ã –¥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π)
‚úì –ö–æ–Ω—Ç—Ä–æ–ª–∏—Ä—É–µ–º—ã–µ —É—Å–ª–æ–≤–∏—è
‚úì –†–µ–≥—É–ª—è—Ä–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ 24/7
‚úì –ì–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
‚úó –ù–µ –æ—Ç—Ä–∞–∂–∞–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–π user experience

Real User Monitoring (RUM):
‚úì –†–µ–∞–ª—å–Ω—ã–π user experience
‚úì –†–µ–∞–ª—å–Ω—ã–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –∏ —Å–µ—Ç–∏
‚úì –ë–∏–∑–Ω–µ—Å-–º–µ—Ç—Ä–∏–∫–∏
‚úó –†–µ–∞–∫—Ç–∏–≤–Ω—ã–π (–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏ —É–∂–µ –ø–æ—Å—Ç—Ä–∞–¥–∞–ª–∏)
````

**–¢–∏–ø—ã —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–≤–µ—Ä–æ–∫:**

yaml

````yaml
HTTP/HTTPS Check:
  - Status code
  - Response time
  - SSL certificate
  - Response body contains text

TCP Check:
  - Port availability
  - Connection time

DNS Check:
  - DNS resolution time
  - Correct IP returned

Browser Check (Headless):
  - Full page load
  - JavaScript execution
  - Form submission
  - Multi-step transactions
````

**–õ–æ–∫–∞—Ü–∏–∏ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–æ–∫:**
````
Multiple Geographic Locations:
- North America (US-East, US-West)
- Europe (London, Frankfurt)
- Asia (Tokyo, Singapore)
- South America (S√£o Paulo)

–¶–µ–ª—å: –û–±–Ω–∞—Ä—É–∂–∏—Ç—å —Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã
````

### üíª –ó–∞–¥–∞–Ω–∏–µ

–ù–∞—Å—Ç—Ä–æ–π —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Å Blackbox Exporter –∏ Uptime Kuma:

1. **–î–æ–±–∞–≤—å Blackbox Exporter –≤ docker-compose.yml**:

yaml

```yaml
  blackbox-exporter:
    image: prom/blackbox-exporter:latest
    container_name: blackbox-exporter
    ports:
      - "9115:9115"
    volumes:
      - ./blackbox.yml:/etc/blackbox_exporter/config.yml
    command:
      - '--config.file=/etc/blackbox_exporter/config.yml'
    restart: unless-stopped
```

2. **–°–æ–∑–¥–∞–π blackbox.yml**:

yaml

```yaml
modules:
  # HTTP 2xx check
  http_2xx:
    prober: http
    timeout: 5s
    http:
      valid_http_versions: ["HTTP/1.1", "HTTP/2.0"]
      valid_status_codes: []  # defaults to 2xx
      method: GET
      preferred_ip_protocol: "ip4"
      follow_redirects: true
      fail_if_ssl: false
      fail_if_not_ssl: false

  # HTTP check with POST
  http_post_2xx:
    prober: http
    http:
      method: POST
      headers:
        Content-Type: application/json
      body: '{"key": "value"}'

  # HTTP check —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
  http_content_check:
    prober: http
    http:
      fail_if_body_not_matches_regexp:
        - "Welcome"
        - "Status: OK"
      fail_if_body_matches_regexp:
        - "Error"
        - "Exception"

  # HTTPS —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π SSL
  https_ssl_check:
    prober: http
    timeout: 5s
    http:
      valid_status_codes: [200]
      fail_if_ssl: false
      fail_if_not_ssl: true
      tls_config:
        insecure_skip_verify: false

  # TCP check
  tcp_connect:
    prober: tcp
    timeout: 5s

  # ICMP (ping) check
  icmp:
    prober: icmp
    timeout: 5s
    icmp:
      preferred_ip_protocol: "ip4"

  # DNS check
  dns_check:
    prober: dns
    timeout: 5s
    dns:
      query_name: "example.com"
      query_type: "A"

  # SSH check
  ssh_banner:
    prober: tcp
    timeout: 5s
    tcp:
      query_response:
        - expect: "^SSH-2.0-"

  # PostgreSQL check
  postgres_check:
    prober: tcp
    timeout: 5s
    tcp:
      query_response:
        - send: "\x00\x00\x00\x08\x04\xd2\x16\x2f"
```

3. **–û–±–Ω–æ–≤–∏ prometheus.yml** –¥–ª—è Blackbox:

yaml

```yaml
scrape_configs:
  # ... —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ jobs

  # HTTP endpoints
  - job_name: 'blackbox-http'
    metrics_path: /probe
    params:
      module: [http_2xx]
    static_configs:
      - targets:
          - https://example.com
          - https://api.example.com
          - http://localhost:5000
          - http://localhost:3000
    relabel_configs:
      - source_labels: [__address__]
        target_label: __param_target
      - source_labels: [__param_target]
        target_label: instance
      - target_label: __address__
        replacement: blackbox-exporter:9115

  # TCP ports
  - job_name: 'blackbox-tcp'
    metrics_path: /probe
    params:
      module: [tcp_connect]
    static_configs:
      - targets:
          - localhost:5432  # PostgreSQL
          - localhost:6379  # Redis
          - localhost:9090  # Prometheus
    relabel_configs:
      - source_labels: [__address__]
        target_label: __param_target
      - source_labels: [__param_target]
        target_label: instance
      - target_label: __address__
        replacement: blackbox-exporter:9115

  # ICMP (ping)
  - job_name: 'blackbox-icmp'
    metrics_path: /probe
    params:
      module: [icmp]
    static_configs:
      - targets:
          - 8.8.8.8        # Google DNS
          - 1.1.1.1        # Cloudflare DNS
          - example.com
    relabel_configs:
      - source_labels: [__address__]
        target_label: __param_target
      - source_labels: [__param_target]
        target_label: instance
      - target_label: __address__
        replacement: blackbox-exporter:9115

```

4. **–°–æ–∑–¥–∞–π –∞–ª–µ—Ä—Ç—ã –¥–ª—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–≤–µ—Ä–æ–∫**:
```yaml
# synthetic_alerts.yml
groups:
  - name: blackbox_alerts
    rules:
      # Endpoint –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
      - alert: EndpointDown
        expr: probe_success == 0
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Endpoint {{ $labels.instance }} is down"
          description: "{{ $labels.instance }} has been down for more than 5 minutes"
          impact: "Service unavailable for users"

      # –ú–µ–¥–ª–µ–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
      - alert: SlowResponse
        expr: probe_duration_seconds > 3
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Slow response from {{ $labels.instance }}"
          description: "Response time is {{ $value }}s (threshold: 3s)"

      # SSL —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç –∏—Å—Ç–µ–∫–∞–µ—Ç
      - alert: SSLCertExpiringSoon
        expr: (probe_ssl_earliest_cert_expiry - time()) / 86400 < 30
        for: 1h
        labels:
          severity: warning
        annotations:
          summary: "SSL certificate expiring soon for {{ $labels.instance }}"
          description: "SSL certificate expires in {{ $value }} days"
          action: "Renew SSL certificate"

      # SSL —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç –∏—Å—Ç–µ–∫
      - alert: SSLCertExpired
        expr: probe_ssl_earliest_cert_expiry - time() <= 0
        labels:
          severity: critical
        annotations:
          summary: "SSL certificate expired for {{ $labels.instance }}"
          description: "SSL certificate has expired"
          impact: "HTTPS connections will fail"

      # HTTP status code –Ω–µ 2xx
      - alert: HTTPStatusCode5xx
        expr: probe_http_status_code >= 500
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "HTTP 5xx error on {{ $labels.instance }}"
          description: "Status code: {{ $value }}"

      - alert: HTTPStatusCode4xx
        expr: probe_http_status_code >= 400 and probe_http_status_code < 500
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "HTTP 4xx error on {{ $labels.instance }}"
          description: "Status code: {{ $value }}"
```

5. **–î–æ–±–∞–≤—å Uptime Kuma** (–∫—Ä–∞—Å–∏–≤—ã–π UI –¥–ª—è uptime –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞):
```yaml
  uptime-kuma:
    image: louislam/uptime-kuma:latest
    container_name: uptime-kuma
    ports:
      - "3001:3001"
    volumes:
      - uptime-kuma-data:/app/data
    restart: unless-stopped

volumes:
  uptime-kuma-data:
```

6. **–ó–∞–ø—É—Å—Ç–∏ –∏ –Ω–∞—Å—Ç—Ä–æ–π**:
```bash
# –ó–∞–ø—É—Å—Ç–∏ —Å–µ—Ä–≤–∏—Å—ã
docker-compose up -d blackbox-exporter uptime-kuma

# –ü—Ä–æ–≤–µ—Ä—å Blackbox
curl "http://localhost:9115/probe?module=http_2xx&target=https://example.com"

# –ù–∞—Å—Ç—Ä–æ–π Uptime Kuma
# –û—Ç–∫—Ä–æ–π: http://localhost:3001
# –°–æ–∑–¥–∞–π –∞–∫–∫–∞—É–Ω—Ç
# –î–æ–±–∞–≤—å –º–æ–Ω–∏—Ç–æ—Ä—ã:
#   - HTTP(s) –¥–ª—è –≤–µ–±-—Å–∞–π—Ç–æ–≤
#   - TCP –¥–ª—è –ø–æ—Ä—Ç–æ–≤
#   - Ping –¥–ª—è —Å–µ—Ä–≤–µ—Ä–æ–≤
```

7. **–°–æ–∑–¥–∞–π –¥–∞—à–±–æ—Ä–¥ –¥–ª—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –≤ Grafana**:

–ò–º–ø–æ—Ä—Ç–∏—Ä—É–π –≥–æ—Ç–æ–≤—ã–π –¥–∞—à–±–æ—Ä–¥: ID 7587 (Prometheus Blackbox Exporter)

–ò–ª–∏ —Å–æ–∑–¥–∞–π —Å–≤–æ–π —Å –ø–∞–Ω–µ–ª—è–º–∏:
```

Panel 1: Uptime % Query: avg_over_time(probe_success[24h]) * 100

Panel 2: Response Time Query: probe_duration_seconds

Panel 3: SSL Certificate Days Left Query: (probe_ssl_earliest_cert_expiry - time()) / 86400

Panel 4: HTTP Status Codes Query: probe_http_status_code

Panel 5: Availability Map Type: Status History Query: probe_success

```

### üöÄ –ë–æ–Ω—É—Å (–Ω–æ–≤–æ–µ)

**1. –°–æ–∑–¥–∞–π Multi-Step Browser Check —Å Playwright**:
```python
# synthetic_browser_check.py
from playwright.sync_api import sync_playwright
from prometheus_client import Gauge, Counter, start_http_server
import time

# –ú–µ—Ç—Ä–∏–∫–∏
check_duration = Gauge('synthetic_check_duration_seconds', 
                       'Duration of synthetic check', 
                       ['check_name', 'step'])
check_success = Gauge('synthetic_check_success', 
                     'Success status of synthetic check',
                     ['check_name'])
check_errors = Counter('synthetic_check_errors_total',
                      'Total errors in synthetic checks',
                      ['check_name', 'error_type'])

def run_login_flow_check():
    """Multi-step —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: –ª–æ–≥–∏–Ω –∏ –ø–æ–∫—É–ø–∫–∞"""
    
    check_name = 'ecommerce_purchase_flow'
    start_time = time.time()
    
    try:
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)
            page = browser.new_page()
            
            # Step 1: –ó–∞–≥—Ä—É–∑–∫–∞ –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            step_start = time.time()
            page.goto('https://example-shop.com')
            check_duration.labels(check_name=check_name, step='homepage').set(
                time.time() - step_start
            )
            
            # Step 2: –ü–æ–∏—Å–∫ —Ç–æ–≤–∞—Ä–∞
            step_start = time.time()
            page.fill('input[name="search"]', 'laptop')
            page.click('button[type="submit"]')
            page.wait_for_selector('.product-list')
            check_duration.labels(check_name=check_name, step='search').set(
                time.time() - step_start
            )
            
            # Step 3: –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤ –∫–æ—Ä–∑–∏–Ω—É
            step_start = time.time()
            page.click('.product-item:first-child .add-to-cart')
            page.wait_for_selector('.cart-notification')
            check_duration.labels(check_name=check_name, step='add_to_cart').set(
                time.time() - step_start
            )
            
            # Step 4: Checkout
            step_start = time.time()
            page.click('a[href="/cart"]')
            page.wait_for_selector('.checkout-button')
            page.click('.checkout-button')
            page.wait_for_url('**/checkout')
            check_duration.labels(check_name=check_name, step='checkout').set(
                time.time() - step_start
            )
            
            # Step 5: –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ —Ñ–æ—Ä–º—ã
            step_start = time.time()
            page.fill('input[name="email"]', 'test@example.com')
            page.fill('input[name="card_number"]', '4242424242424242')
            page.fill('input[name="exp_date"]', '12/25')
            page.fill('input[name="cvv"]', '123')
            check_duration.labels(check_name=check_name, step='fill_form').set(
                time.time() - step_start
            )
            
            # Step 6: Submit –∏ –ø—Ä–æ–≤–µ—Ä–∫–∞ —É—Å–ø–µ—Ö–∞
            step_start = time.time()
            page.click('button[type="submit"]')
            page.wait_for_selector('.order-success', timeout=10000)
            check_duration.labels(check_name=check_name, step='submit').set(
                time.time() - step_start
            )
            
            browser.close()
            
            # –£—Å–ø–µ—Ö
            check_success.labels(check_name=check_name).set(1)
            
            total_duration = time.time() - start_time
            print(f"‚úì Check {check_name} passed in {total_duration:.2f}s")
            
    except Exception as e:
        check_success.labels(check_name=check_name).set(0)
        check_errors.labels(
            check_name=check_name,
            error_type=type(e).__name__
        ).inc()
        print(f"‚úó Check {check_name} failed: {e}")

def run_api_workflow_check():
    """API workflow –ø—Ä–æ–≤–µ—Ä–∫–∞"""
    import requests
    
    check_name = 'api_workflow'
    
    try:
        # Step 1: Get auth token
        step_start = time.time()
        auth_response = requests.post('https://api.example.com/auth', json={
            'username': 'test',
            'password': 'test123'
        }, timeout=5)
        check_duration.labels(check_name=check_name, step='auth').set(
            time.time() - step_start
        )
        
        if auth_response.status_code != 200:
            raise Exception(f"Auth failed: {auth_response.status_code}")
        
        token = auth_response.json()['token']
        
        # Step 2: Create resource
        step_start = time.time()
        create_response = requests.post(
            'https://api.example.com/resources',
            headers={'Authorization': f'Bearer {token}'},
            json={'name': 'test-resource'},
            timeout=5
        )
        check_duration.labels(check_name=check_name, step='create').set(
            time.time() - step_start
        )
        
        resource_id = create_response.json()['id']
        
        # Step 3: Get resource
        step_start = time.time()
        get_response = requests.get(
            f'https://api.example.com/resources/{resource_id}',
            headers={'Authorization': f'Bearer {token}'},
            timeout=5
        )
        check_duration.labels(check_name=check_name, step='get').set(
            time.time() - step_start
        )
        
        # Step 4: Delete resource
        step_start = time.time()
        delete_response = requests.delete(
            f'https://api.example.com/resources/{resource_id}',
            headers={'Authorization': f'Bearer {token}'},
            timeout=5
        )
        check_duration.labels(check_name=check_name, step='delete').set(
            time.time() - step_start
        )
        
        check_success.labels(check_name=check_name).set(1)
        print(f"‚úì API workflow check passed")
        
    except Exception as e:
        check_success.labels(check_name=check_name).set(0)
        check_errors.labels(
            check_name=check_name,
            error_type=type(e).__name__
        ).inc()
        print(f"‚úó API workflow check failed: {e}")

if __name__ == '__main__':
    # –ó–∞–ø—É—Å—Ç–∏ Prometheus metrics server
    start_http_server(8000)
    print("Metrics available at http://localhost:8000")
    
    # –ó–∞–ø—É—Å–∫–∞–π –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–∞–∂–¥—ã–µ 5 –º–∏–Ω—É—Ç
    while True:
        print(f"\n{'='*50}")
        print(f"Running synthetic checks at {time.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"{'='*50}")
        
        run_login_flow_check()
        run_api_workflow_check()
        
        time.sleep(300)  # 5 –º–∏–Ω—É—Ç
```

**2. –°–æ–∑–¥–∞–π Geographic Distributed Monitoring**:
```yaml
# docker-compose –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ä–µ–≥–∏–æ–Ω–æ–≤
# us-east.docker-compose.yml
version: '3.8'

services:
  blackbox-us-east:
    image: prom/blackbox-exporter:latest
    container_name: blackbox-us-east
    ports:
      - "9116:9115"
    volumes:
      - ./blackbox.yml:/etc/blackbox_exporter/config.yml
    environment:
      - LOCATION=us-east
    restart: unless-stopped
```

Prometheus config –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ª–æ–∫–∞—Ü–∏–π:
```yaml
scrape_configs:
  - job_name: 'blackbox-us-east'
    metrics_path: /probe
    params:
      module: [http_2xx]
    static_configs:
      - targets:
          - https://example.com
    relabel_configs:
      - source_labels: [__address__]
        target_label: __param_target
      - target_label: instance
        replacement: example.com
      - target_label: region
        replacement: us-east
      - target_label: __address__
        replacement: blackbox-us-east:9115

  - job_name: 'blackbox-eu-west'
    metrics_path: /probe
    params:
      module: [http_2xx]
    static_configs:
      - targets:
          - https://example.com
    relabel_configs:
      - source_labels: [__address__]
        target_label: __param_target
      - target_label: instance
        replacement: example.com
      - target_label: region
        replacement: eu-west
      - target_label: __address__
        replacement: blackbox-eu-west:9115
```

**3. –ù–∞—Å—Ç—Ä–æ–π Status Page** —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Cachet:
```yaml
  cachet:
    image: cachethq/docker:latest
    container_name: cachet
    ports:
      - "8001:8000"
    environment:
      - DB_DRIVER=sqlite
      - APP_KEY=base64:yourapplicationkey
      - APP_URL=http://localhost:8001
    volumes:
      - cachet-data:/var/www/html/database
    restart: unless-stopped

volumes:
  cachet-data:
```

–ò–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π **Upptime** (GitHub-based):
```yaml
# .github/workflows/upptime.yml
name: Upptime CI
on:
  schedule:
    - cron: "*/5 * * * *"
  workflow_dispatch:

jobs:
  release:
    name: Check status
    runs-on: ubuntu-latest
    steps:
      - uses: upptime/upptime@v1.28.0
        with:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
```

`.upptimerc.yml`:
```yaml
owner: your-username
repo: upptime

sites:
  - name: Website
    url: https://example.com
    maxResponseTime: 5000
  - name: API
    url: https://api.example.com
    maxResponseTime: 3000
  - name: Blog
    url: https://blog.example.com

status-website:
  cname: status.example.com
  name: Status Page
  introTitle: "Service Status"
  introMessage: Real-time status and uptime monitoring
```

---
## –ú–æ–¥—É–ª—å 9: SLI/SLO/SLA –∏ Error Budget - Site Reliability Engineering (40 –º–∏–Ω—É—Ç)

### üéØ –ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞

**SRE –æ—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏:**

```
SLA (Service Level Agreement)
‚îú‚îÄ –Æ—Ä–∏–¥–∏—á–µ—Å–∫–æ–µ —Å–æ–≥–ª–∞—à–µ–Ω–∏–µ —Å –∫–ª–∏–µ–Ω—Ç–æ–º
‚îú‚îÄ –û–±—è–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ —É—Ä–æ–≤–Ω—è —Å–µ—Ä–≤–∏—Å–∞
‚îî‚îÄ –û–±—ã—á–Ω–æ: 99.9%, 99.95%, 99.99%

SLO (Service Level Objective)
‚îú‚îÄ –í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è —Ü–µ–ª—å –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏
‚îú‚îÄ –°—Ç—Ä–æ–∂–µ —á–µ–º SLA (–±—É—Ñ–µ—Ä –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏)
‚îî‚îÄ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π

SLI (Service Level Indicator)
‚îú‚îÄ –ú–µ—Ç—Ä–∏–∫–∞ –∏–∑–º–µ—Ä—è—é—â–∞—è –∫–∞—á–µ—Å—Ç–≤–æ —Å–µ—Ä–≤–∏—Å–∞
‚îú‚îÄ Availability, Latency, Error Rate
‚îî‚îÄ –û—Å–Ω–æ–≤–∞ –¥–ª—è SLO

Error Budget
‚îú‚îÄ –î–æ–ø—É—Å—Ç–∏–º–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—à–∏–±–æ–∫
‚îú‚îÄ 100% - SLO = Error Budget
‚îî‚îÄ –ë–∞–ª–∞–Ω—Å –º–µ–∂–¥—É –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç—å—é –∏ —Å–∫–æ—Ä–æ—Å—Ç—å—é
```

**–ü—Ä–∏–º–µ—Ä —Ä–∞—Å—á–µ—Ç–∞:**

```
SLA: 99.9% uptime –≤ –º–µ—Å—è—Ü
SLO: 99.95% uptime (–≤–Ω—É—Ç—Ä–µ–Ω–Ω—è—è —Ü–µ–ª—å, —Å—Ç—Ä–æ–∂–µ SLA)

Error Budget = 100% - 99.95% = 0.05%

–í –º–µ—Å—è—Ü (30 –¥–Ω–µ–π):
Total time: 30 * 24 * 60 = 43,200 –º–∏–Ω—É—Ç
Error Budget: 43,200 * 0.0005 = 21.6 –º–∏–Ω—É—Ç downtime –¥–æ–ø—É—Å—Ç–∏–º–æ

–ï—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ 10 –º–∏–Ω—É—Ç downtime:
Remaining Budget: 21.6 - 10 = 11.6 –º–∏–Ω—É—Ç
Budget consumed: 10/21.6 = 46.3%
```

**–¢–∏–ø—ã SLI:**

**1. Availability (–¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å):**

```
SLI = successful_requests / total_requests

Example:
- Successful: 999,500
- Failed: 500
- Total: 1,000,000
- Availability = 999,500 / 1,000,000 = 99.95%
```

**2. Latency (–∑–∞–¥–µ—Ä–∂–∫–∞):**

```
SLI = requests_under_threshold / total_requests

Example (threshold = 200ms):
- Under 200ms: 995,000
- Over 200ms: 5,000
- Total: 1,000,000
- Latency SLI = 995,000 / 1,000,000 = 99.5%
```

**3. Error Rate (—á–∞—Å—Ç–æ—Ç–∞ –æ—à–∏–±–æ–∫):**

```
SLI = (total_requests - error_requests) / total_requests

Example:
- Total: 1,000,000
- Errors (5xx): 300
- Error Rate SLI = (1,000,000 - 300) / 1,000,000 = 99.97%
```

**4. Throughput (–ø—Ä–æ–ø—É—Å–∫–Ω–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å):**

```
SLI = actual_throughput >= target_throughput

Example:
- Target: 1000 req/sec
- Actual: 1050 req/sec
- SLI = 100% (meets target)
```

**SLO Types (—Ç–∏–ø—ã —Ü–µ–ª–µ–π):**

**Request-based SLO:**

yaml

```yaml
# 99.9% requests –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —É—Å–ø–µ—à–Ω—ã–º–∏
slo:
  type: request_based
  target: 99.9
  sli:
    total_query: sum(rate(http_requests_total[5m]))
    good_query: sum(rate(http_requests_total{status!~"5.."}[5m]))
```

**Window-based SLO:**

yaml

````yaml
# 99.9% –≤—Ä–µ–º–µ–Ω–∏ —Å–µ—Ä–≤–∏—Å –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –¥–æ—Å—Ç—É–ø–µ–Ω
slo:
  type: window_based
  target: 99.9
  window: 30d
  sli:
    good_query: sum(up == 1)
    total_query: count(up)
````

**Multi-window SLO (Google SRE –ø–æ–¥—Ö–æ–¥):**
```
Short window (1 hour):
- Alert if burn rate > 14.4x (–∏—Å—á–µ—Ä–ø–∞–µ–º budget –∑–∞ 2 —á–∞—Å–∞)
- Severity: Critical

Medium window (6 hours):
- Alert if burn rate > 6x (–∏—Å—á–µ—Ä–ø–∞–µ–º budget –∑–∞ 5 –¥–Ω–µ–π)
- Severity: High

Long window (3 days):
- Alert if burn rate > 1x (–∏—Å—á–µ—Ä–ø–∞–µ–º budget –∑–∞ 30 –¥–Ω–µ–π)
- Severity: Warning
```

**Error Budget Policy:**
```
100% Error Budget –æ—Å—Ç–∞–µ—Ç—Å—è:
‚úÖ Ship –Ω–æ–≤—ã–µ features
‚úÖ –î–µ–ª–∞—Ç—å —Ä–∏—Å–∫–æ–≤–∞–Ω–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è
‚úÖ –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ on-call –¥–µ–∂—É—Ä—Å—Ç–≤–∞

< 50% Error Budget –æ—Å—Ç–∞–µ—Ç—Å—è:
‚ö†Ô∏è  –ó–∞–º–µ–¥–ª–∏—Ç—å releases
‚ö†Ô∏è  Code freeze –¥–ª—è non-critical features
‚ö†Ô∏è  –§–æ–∫—É—Å –Ω–∞ reliability

0% Error Budget –∏—Å—á–µ—Ä–ø–∞–Ω:
‚ùå –ü–æ–ª–Ω—ã–π code freeze
‚ùå –¢–æ–ª—å–∫–æ bug fixes –∏ reliability improvements
‚ùå Post-mortem –∞–Ω–∞–ª–∏–∑
‚ùå –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ on-call —Ä–µ—Å—É—Ä—Å—ã
````

**SLI/SLO –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤:**

**API Service:**

yaml

```yaml
slis:
  - name: availability
    description: "–ü—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—à–Ω—ã—Ö API –∑–∞–ø—Ä–æ—Å–æ–≤"
    query: |
      sum(rate(http_requests_total{status!~"5.."}[5m]))
      /
      sum(rate(http_requests_total[5m]))
    
  - name: latency
    description: "P95 latency –ø–æ–¥ 200ms"
    query: |
      histogram_quantile(0.95,
        rate(http_request_duration_seconds_bucket[5m])
      ) < 0.2

slos:
  - name: api_availability
    target: 99.9
    sli: availability
    window: 30d
    
  - name: api_latency
    target: 99.5
    sli: latency
    window: 30d
```

**Database:**

yaml

```yaml
slis:
  - name: query_success_rate
    query: |
      sum(rate(db_queries_total{status="success"}[5m]))
      /
      sum(rate(db_queries_total[5m]))
  
  - name: query_latency
    query: |
      histogram_quantile(0.99,
        rate(db_query_duration_seconds_bucket[5m])
      ) < 0.1

slos:
  - name: db_reliability
    target: 99.95
    sli: query_success_rate
    window: 30d
```

**Message Queue:**

yaml

```yaml
slis:
  - name: message_processing_success
    query: |
      sum(rate(queue_messages_processed_total{status="success"}[5m]))
      /
      sum(rate(queue_messages_processed_total[5m]))
  
  - name: queue_latency
    query: |
      queue_oldest_message_age_seconds < 300  # < 5 –º–∏–Ω—É—Ç

slos:
  - name: queue_reliability
    target: 99.9
    sli: message_processing_success
    window: 7d
```

**Monitoring SLO Compliance:**

promql

```promql
# –¢–µ–∫—É—â–∏–π SLO compliance
(
  sum(rate(http_requests_total{status!~"5.."}[30d]))
  /
  sum(rate(http_requests_total[30d]))
) * 100

# Error budget remaining (–≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö)
100 - (
  (1 - (sum(rate(http_requests_total{status!~"5.."}[30d])) / sum(rate(http_requests_total[30d]))))
  /
  (1 - 0.999)  # Target SLO
) * 100

# Error budget burn rate
(
  (1 - (sum(rate(http_requests_total{status!~"5.."}[1h])) / sum(rate(http_requests_total[1h]))))
  /
  (1 - 0.999)
)
```

**Alerting –Ω–∞ SLO –Ω–∞—Ä—É—à–µ–Ω–∏—è:**

yaml

````yaml
# Fast burn alert (2 —á–∞—Å–∞ –¥–æ –∏—Å—á–µ—Ä–ø–∞–Ω–∏—è)
- alert: ErrorBudgetBurnRateFast
  expr: |
    (
      (1 - (sum(rate(http_requests_total{status!~"5.."}[5m])) / sum(rate(http_requests_total[5m]))))
      /
      (1 - 0.999)  # SLO target
    ) > 14.4
  for: 2m
  labels:
    severity: critical
    slo: api_availability
  annotations:
    summary: "Fast error budget burn rate detected"
    description: "At current rate, error budget will be exhausted in 2 hours"

# Slow burn alert (5 –¥–Ω–µ–π –¥–æ –∏—Å—á–µ—Ä–ø–∞–Ω–∏—è)
- alert: ErrorBudgetBurnRateSlow
  expr: |
    (
      (1 - (sum(rate(http_requests_total{status!~"5.."}[1h])) / sum(rate(http_requests_total[1h]))))
      /
      (1 - 0.999)
    ) > 6
  for: 15m
  labels:
    severity: warning
    slo: api_availability
  annotations:
    summary: "Slow error budget burn rate detected"
    description: "At current rate, error budget will be exhausted in 5 days"

# SLO violation
- alert: SLOViolation
  expr: |
    (
      sum(rate(http_requests_total{status!~"5.."}[30d]))
      /
      sum(rate(http_requests_total[30d]))
    ) < 0.999
  for: 5m
  labels:
    severity: critical
    slo: api_availability
  annotations:
    summary: "SLO violation - 30 day window"
    description: "Current availability: {{ $value | humanizePercentage }}"
```

**SLO Dashboard –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:**
```
1. Current SLO Status
   - Gauge: —Ç–µ–∫—É—â–∏–π SLI
   - Target line: SLO
   - Color coding: green/yellow/red

2. Error Budget
   - Gauge: –æ—Å—Ç–∞–≤—à–∏–π—Å—è budget (%)
   - Graph: burn rate over time
   - Time to exhaustion

3. Burn Rate
   - Current burn rate (multiple windows)
   - Historical burn rate
   - Alerts status

4. Compliance History
   - 30-day rolling window
   - Daily compliance
   - Incidents impact

5. Budget Consumption
   - By incident
   - By service component
   - By time period
````

**User Journey SLO (—Å–∫–≤–æ–∑–Ω–æ–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥):**

yaml

```yaml
# –ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π SLO –¥–ª—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–≥–æ user journey
user_journey:
  name: "Checkout Flow"
  steps:
    - name: "Add to Cart"
      slo: 99.9
      latency_target: 200ms
      
    - name: "View Cart"
      slo: 99.9
      latency_target: 300ms
      
    - name: "Payment"
      slo: 99.95  # –°—Ç—Ä–æ–∂–µ –¥–ª—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–π –æ–ø–µ—Ä–∞—Ü–∏–∏
      latency_target: 500ms
      
    - name: "Order Confirmation"
      slo: 99.9
      latency_target: 1000ms
  
  overall_slo: 99.7  # –ö–æ–º–ø–æ–∑–∏—Ç–Ω—ã–π SLO (99.9 * 99.9 * 99.95 * 99.9)
```

**SLO Report Template:**

markdown

````markdown
# SLO Report: API Service

**Period:** 2025-01-01 to 2025-01-31

## Summary
- **SLO Target:** 99.9%
- **Actual Availability:** 99.87%
- **Status:** ‚ö†Ô∏è Below Target
- **Error Budget:** 100% consumed + 30% over

## SLI Breakdown

### Availability
- Target: 99.9%
- Actual: 99.87%
- Total Requests: 100,000,000
- Failed Requests: 130,000
- Success Rate: 99.87%

### Latency (P95)
- Target: < 200ms
- Actual: 185ms
- Status: ‚úÖ Met

## Incidents

### Incident #1: Database Outage
- Date: 2025-01-15
- Duration: 45 minutes
- Impact: 100% unavailability
- Budget Consumed: 90%
- Root Cause: Primary DB failure, replication lag
- Action Items: Improve failover automation

### Incident #2: High Latency
- Date: 2025-01-22
- Duration: 2 hours
- Impact: 20% of requests over threshold
- Budget Consumed: 40%
- Root Cause: Memory leak in application
- Action Items: Add memory profiling

## Action Items
1. [ ] Implement automated database failover
2. [ ] Add continuous memory profiling
3. [ ] Increase monitoring sensitivity
4. [ ] Schedule reliability sprint

## Next Month Forecast
- If current trend continues: ‚ùå SLO at risk
- Recommended: Code freeze for non-critical features
````

**Tools –¥–ª—è SLO –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞:**
```
1. Sloth (SLO generator)
   - Generates Prometheus rules
   - Multi-window alerts
   - Dashboard generation

2. Pyrra
   - SLO management UI
   - Error budget visualization
   - Alert configuration

3. Grafana SLO Plugin
   - Native SLO support
   - Dashboard templates
   - Integration with Prometheus

4. Google Cloud SLO Monitoring
   - Managed service
   - Built-in SLI library
   - Automated reporting

5. Datadog SLO
   - SLO tracking
   - Budget alerts
   - Integration with incidents
```

**Cost of Downtime:**
```
–†–∞—Å—á–µ—Ç business impact:

E-commerce site:
- Revenue: $10M/month
- Monthly minutes: 43,200
- Revenue per minute: $231.48
- 1 hour downtime = $13,888 loss

SaaS application:
- Customers: 10,000
- Churn rate –ø—Ä–∏ downtime: 2%
- Average LTV: $5,000
- 1 hour downtime = 200 churned customers = $1M loss

Developer productivity:
- Developers: 50
- Hourly cost: $100/hour
- Blocked time per outage: 2 hours
- Cost: 50 * 100 * 2 = $10,000
````

### üíª –ó–∞–¥–∞–Ω–∏–µ

–ù–∞—Å—Ç—Ä–æ–π –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π SLO –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥:

1. **–°–æ–∑–¥–∞–π SLO –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é —Å Sloth**:

`slos/api-service.yaml`:

yaml

```yaml
version: "prometheus/v1"
service: "api-service"
labels:
  owner: "backend-team"
  tier: "critical"
slos:
  # Availability SLO
  - name: "requests-availability"
    objective: 99.9
    description: "API requests –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —É—Å–ø–µ—à–Ω—ã–º–∏"
    sli:
      events:
        error_query: sum(rate(http_requests_total{job="api",status=~"(5..|429)"}[{{.window}}]))
        total_query: sum(rate(http_requests_total{job="api"}[{{.window}}]))
    alerting:
      name: ApiHighErrorRate
      labels:
        category: "availability"
      annotations:
        summary: "High error rate on API service"
      page_alert:
        labels:
          severity: critical
      ticket_alert:
        labels:
          severity: warning
  
  # Latency SLO
  - name: "requests-latency"
    objective: 99.5
    description: "95% requests –¥–æ–ª–∂–Ω—ã –≤—ã–ø–æ–ª–Ω—è—Ç—å—Å—è –∑–∞ < 200ms"
    sli:
      events:
        error_query: |
          (
            sum(rate(http_request_duration_seconds_count{job="api"}[{{.window}}]))
            -
            sum(rate(http_request_duration_seconds_bucket{job="api",le="0.2"}[{{.window}}]))
          )
        total_query: sum(rate(http_request_duration_seconds_count{job="api"}[{{.window}}]))
    alerting:
      name: ApiHighLatency
      labels:
        category: "latency"
      annotations:
        summary: "High latency on API service"
      page_alert:
        labels:
          severity: critical
      ticket_alert:
        labels:
          severity: warning
```

`slos/database.yaml`:

yaml

```yaml
version: "prometheus/v1"
service: "postgresql"
labels:
  owner: "platform-team"
  tier: "critical"
slos:
  # Database availability
  - name: "connection-availability"
    objective: 99.95
    description: "Database –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –¥–æ—Å—Ç—É–ø–Ω–∞ –¥–ª—è –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–π"
    sli:
      events:
        error_query: sum(rate(pg_up{job="postgres"}[{{.window}}]) == 0)
        total_query: count(pg_up{job="postgres"})
    alerting:
      name: DatabaseUnavailable
      labels:
        category: "availability"
      page_alert:
        labels:
          severity: critical
  
  # Query performance
  - name: "query-performance"
    objective: 99.9
    description: "Queries –¥–æ–ª–∂–Ω—ã –≤—ã–ø–æ–ª–Ω—è—Ç—å—Å—è –∑–∞ < 100ms"
    sli:
      events:
        error_query: |
          sum(rate(pg_stat_statements_mean_exec_time{job="postgres"}[{{.window}}])) > 100
        total_query: sum(rate(pg_stat_statements_calls{job="postgres"}[{{.window}}]))
    alerting:
      name: DatabaseSlowQueries
      labels:
        category: "performance"
      ticket_alert:
        labels:
          severity: warning
```

2. **–ì–µ–Ω–µ—Ä–∞—Ü–∏—è Prometheus rules —Å Sloth**:

bash

```bash
# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ Sloth
go install github.com/slok/sloth/cmd/sloth@latest

# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∞–≤–∏–ª
sloth generate -i slos/api-service.yaml -o prometheus/rules/api-slo.yaml
sloth generate -i slos/database.yaml -o prometheus/rules/database-slo.yaml

# –í–∞–ª–∏–¥–∞—Ü–∏—è
promtool check rules prometheus/rules/*.yaml
```

3. **–°–æ–∑–¥–∞–π SLO Dashboard –≤ Grafana**:

`dashboards/slo-overview.json`:

json

```json
{
  "dashboard": {
    "title": "SLO Overview",
    "tags": ["slo", "sre"],
    "timezone": "browser",
    "rows": [
      {
        "title": "SLO Status",
        "panels": [
          {
            "id": 1,
            "title": "API Availability SLO",
            "type": "gauge",
            "targets": [
              {
                "expr": "(\n  sum(rate(http_requests_total{job=\"api\",status!~\"(5..|429)\"}[30d]))\n  /\n  sum(rate(http_requests_total{job=\"api\"}[30d]))\n) * 100",
                "legendFormat": "Current"
              }
            ],
            "fieldConfig": {
              "defaults": {
                "unit": "percent",
                "min": 99,
                "max": 100,
                "thresholds": {
                  "steps": [
                    {"value": 99, "color": "red"},
                    {"value": 99.9, "color": "yellow"},
                    {"value": 99.95, "color": "green"}
                  ]
                }
              }
            }
          },
          {
            "id": 2,
            "title": "Error Budget Remaining",
            "type": "stat",
            "targets": [
              {
                "expr": "100 - (\n  (1 - (sum(rate(http_requests_total{job=\"api\",status!~\"(5..|429)\"}[30d])) / sum(rate(http_requests_total{job=\"api\"}[30d]))))\n  /\n  (1 - 0.999)\n) * 100",
                "legendFormat": "Budget Remaining"
              }
            ],
            "fieldConfig": {
              "defaults": {
                "unit": "percent",
                "thresholds": {
                  "steps": [
                    {"value": 0, "color": "red"},
                    {"value": 25, "color": "yellow"},
                    {"value": 50, "color": "green"}
                  ]
                }
              }
            }
          },
          {
            "id": 3,
            "title": "Burn Rate (1h window)",
            "type": "timeseries",
            "targets": [
              {
                "expr": "(\n  (1 - (sum(rate(http_requests_total{job=\"api\",status!~\"(5..|429)\"}[1h])) / sum(rate(http_requests_total{job=\"api\"}[1h]))))\n  /\n  (1 - 0.999)\n)",
                "legendFormat": "Burn Rate"
              },
              {
                "expr": "14.4",
                "legendFormat": "Critical Threshold (2h to exhaustion)"
              },
              {
                "expr": "6",
                "legendFormat": "Warning Threshold (5d to exhaustion)"
              }
            ]
          }
        ]
      },
      {
        "title": "SLO Compliance History",
        "panels": [
          {
            "id": 4,
            "title": "30-Day Rolling Availability",
            "type": "timeseries",
            "targets": [
              {
                "expr": "(\n  sum(rate(http_requests_total{job=\"api\",status!~\"(5..|429)\"}[30d]))\n  /\n  sum(rate(http_requests_total{job=\"api\"}[30d]))\n) * 100",
                "legendFormat": "Availability"
              },
              {
                "expr": "99.9",
                "legendFormat": "SLO Target"
              }
            ],
            "fieldConfig": {
              "defaults": {
                "unit": "percent",
                "min": 99,
                "max": 100
              }
            }
          },
          {
            "id": 5,
            "title": "Error Budget Consumption",
            "type": "bargauge",
            "targets": [
              {
                "expr": "(\n  (1 - (sum(rate(http_requests_total{job=\"api\",status!~\"(5..|429)\"}[30d])) / sum(rate(http_requests_total{job=\"api\"}[30d]))))\n  /\n  (1 - 0.999)\n) * 100",
                "legendFormat": "Budget Used"
              }
            ],
            "fieldConfig": {
              "defaults": {
                "unit": "percent",
                "max": 100,
                "thresholds": {
                  "steps": [
                    {"value": 0, "color": "green"},
                    {"value": 75, "color": "yellow"},
                    {"value": 100, "color": "red"}
                  ]
                }
              }
            }
          }
        ]
      }
    ]
  }
}
```

4. **–°–æ–∑–¥–∞–π Error Budget Policy –¥–æ–∫—É–º–µ–Ω—Ç**:

`docs/error-budget-policy.md`:

markdown

````markdown
# Error Budget Policy

## Overview
This document defines how we use error budgets to balance reliability and feature velocity.

## SLO Targets

| Service | SLO Target | Error Budget (30d) | Error Budget (minutes) |
|---------|------------|-------------------|----------------------|
| API Service | 99.9% | 0.1% | 43.2 minutes |
| Database | 99.95% | 0.05% | 21.6 minutes |
| Message Queue | 99.9% | 0.1% | 43.2 minutes |
| CDN | 99.99% | 0.01% | 4.3 minutes |

## Policy Levels

### üü¢ Level 1: Budget Healthy (> 50% remaining)

**Allowed Activities:**
- ‚úÖ Normal release cadence
- ‚úÖ Experimental features
- ‚úÖ Performance optimizations
- ‚úÖ Refactoring

**Requirements:**
- Standard change management process
- Automated testing
- Gradual rollouts

### üü° Level 2: Budget Warning (25-50% remaining)

**Allowed Activities:**
- ‚ö†Ô∏è Reduced release frequency
- ‚ö†Ô∏è Critical features only
- ‚ö†Ô∏è Additional testing required

**Requirements:**
- Senior engineer approval for changes
- Extended canary periods
- Increased monitoring
- Daily budget reviews

**Actions:**
- Conduct incident review
- Identify systemic issues
- Create reliability improvement tasks
- Schedule reliability sprint

### üî¥ Level 3: Budget Critical (< 25% remaining)

**Allowed Activities:**
- ‚ùå Feature freeze
- ‚úÖ Bug fixes only
- ‚úÖ Reliability improvements
- ‚úÖ Emergency security patches

**Requirements:**
- Director-level approval for any changes
- Mandatory post-mortems for all incidents
- 24/7 on-call rotation
- Hourly budget monitoring

**Actions:**
- Emergency reliability review
- All hands on stability
- External communication about delays
- Executive escalation

### ‚õî Level 4: Budget Exhausted (0% remaining)

**Allowed Activities:**
- ‚ùå Complete code freeze
- ‚úÖ Critical bug fixes only (with VP approval)
- ‚úÖ Incident response

**Requirements:**
- VP Engineering approval for ANY change
- Full post-mortem for budget exhaustion
- Recovery plan required before resuming features
- Daily executive updates

**Actions:**
- Immediate incident declared
- Full team mobilization
- Customer communication
- Systematic root cause analysis
- Multi-week recovery plan

## Escalation
```
Budget < 50% ‚Üí Team Lead notified
Budget < 25% ‚Üí Engineering Manager notified
Budget < 10% ‚Üí Director notified
Budget exhausted ‚Üí VP Engineering notified
```

## Review Process

- **Daily:** Automated budget reports
- **Weekly:** Team review of budget trends
- **Monthly:** SLO report to stakeholders
- **Quarterly:** Policy review and adjustment

## Exceptions

Exceptions to this policy require:
1. Written justification
2. Risk assessment
3. Approval from Director of Engineering
4. Documentation in incident log

## Contact

- **Policy Owner:** SRE Team
- **Questions:** #sre-team Slack channel
- **Escalations:** oncall-sre@company.com
````

5. **–°–æ–∑–¥–∞–π –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π SLO reporter**:

`scripts/slo-report.py`:

python

```python
#!/usr/bin/env python3
"""
Automated SLO Report Generator
"""
import requests
from datetime import datetime, timedelta
import json

class SLOReporter:
    def __init__(self, prometheus_url, period_days=30):
        self.prometheus_url = prometheus_url
        self.period_days = period_days
        self.slos = self.load_slo_config()
    
    def load_slo_config(self):
        """Load SLO configuration"""
        return {
            'api-availability': {
                'name': 'API Availability',
                'target': 99.9,
                'query': '''
                    (
                      sum(rate(http_requests_total{job="api",status!~"(5..|429)"}[30d]))
                      /
                      sum(rate(http_requests_total{job="api"}[30d]))
                    ) * 100
                '''
            },
            'api-latency': {
                'name': 'API Latency (P95 < 200ms)',
                'target': 99.5,
                'query': '''
                    (
                      sum(rate(http_request_duration_seconds_bucket{job="api",le="0.2"}[30d]))
                      /
                      sum(rate(http_request_duration_seconds_count{job="api"}[30d]))
                    ) * 100
                '''
            },
            'database-availability': {
                'name': 'Database Availability',
                'target': 99.95,
                'query': '''
                    (sum(up{job="postgres"}) / count(up{job="postgres"})) * 100
                '''
            }
        }
    
    def query_prometheus(self, query):
        """Execute Prometheus query"""
        response = requests.get(
            f"{self.prometheus_url}/api/v1/query",
            params={'query': query}
        )
        data = response.json()
        
        if data['status'] == 'success' and data['data']['result']:
            return float(data['data']['result'][0]['value'][1])
        return None
    
    def calculate_error_budget(self, actual, target):
        """Calculate error budget consumption"""
        total_budget = 100 - target
        consumed = target - actual
        
        if consumed < 0:
            return 0.0  # Over-performing
        
        budget_consumed_pct = (consumed / total_budget) * 100
        return min(budget_consumed_pct, 100.0)
    
    def get_downtime_minutes(self, availability_pct):
        """Calculate downtime in minutes"""
        total_minutes = self.period_days * 24 * 60
        uptime_minutes = total_minutes * (availability_pct / 100)
        downtime_minutes = total_minutes - uptime_minutes
        return downtime_minutes
    
    def determine_status(self, actual, target):
        """Determine SLO status"""
        if actual >= target:
            return "‚úÖ Met"
        elif actual >= target - 0.05:
            return "‚ö†Ô∏è At Risk"
        else:
            return "‚ùå Violated"
    
    def generate_report(self):
        """Generate complete SLO report"""
        report = {
            'period': f"{self.period_days} days",
            'generated_at': datetime.now().isoformat(),
            'slos': {}
        }
        
        for slo_id, slo_config in self.slos.items():
            actual = self.query_prometheus(slo_config['query'])
            
            if actual is None:
                continue
            
            target = slo_config['target']
            budget_consumed = self.calculate_error_budget(actual, target)
            status = self.determine_status(actual, target)
            downtime = self.get_downtime_minutes(actual)
            
            report['slos'][slo_id] = {
                'name': slo_config['name'],
                'target': target,
                'actual': round(actual, 4),
                'status': status,
                'error_budget_consumed': round(budget_consumed, 2),
                'error_budget_remaining': round(100 - budget_consumed, 2),
                'downtime_minutes': round(downtime, 2)
            }
        
        return report
    
    def format_markdown(self, report):
        """Format report as Markdown"""
        md = f"""# SLO Report

**Period:** {report['period']}  
**Generated:** {report['generated_at']}

## Summary

| SLO | Target | Actual | Status | Budget Used | Budget Remaining | Downtime |
|-----|--------|--------|--------|-------------|-----------------|----------|
"""
        
        for slo_id, slo_data in report['slos'].items():
            md += f"| {slo_data['name']} "
            md += f"| {slo_data['target']}% "
            m
```

–ü—Ä–æ–¥–æ–ª–∂–∏ —Å–æ–∑–¥–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞

14:06

+= f"| {slo_data['actual']}% " md += f"| {slo_data['status']} " md += f"| {slo_data['error_budget_consumed']}% " md += f"| {slo_data['error_budget_remaining']}% " md += f"| {slo_data['downtime_minutes']:.1f} min |\n"

```
    # Add recommendations
    md += "\n## Recommendations\n\n"
    
    for slo_id, slo_data in report['slos'].items():
        if slo_data['error_budget_remaining'] < 25:
            md += f"- ‚ö†Ô∏è **{slo_data['name']}**: Error budget critical. Implement code freeze.\n"
        elif slo_data['error_budget_remaining'] < 50:
            md += f"- ‚ö†Ô∏è **{slo_data['name']}**: Error budget warning. Reduce release frequency.\n"
    
    return md

def send_to_slack(self, report, webhook_url):
    """Send report to Slack"""
    # Implementation for Slack webhook
    pass
```

if **name** == "**main**": import argparse

```
parser = argparse.ArgumentParser()
parser.add_argument('--prometheus-url', default='http://localhost:9090')
parser.add_argument('--period-days', type=int, default=30)
parser.add_argument('--format', choices=['json', 'markdown'], default='markdown')
parser.add_argument('--output', help='Output file')

args = parser.parse_args()

reporter = SLOReporter(args.prometheus_url, args.period_days)
report = reporter.generate_report()

if args.format == 'json':
    output = json.dumps(report, indent=2)
else:
    output = reporter.format_markdown(report)

if args.output:
    with open(args.output, 'w') as f:
        f.write(output)
else:
    print(output)
```

````

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
```bash
# –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞
python scripts/slo-report.py --prometheus-url http://localhost:9090

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ —Ñ–∞–π–ª
python scripts/slo-report.py --output reports/slo-report-$(date +%Y%m%d).md

# JSON —Ñ–æ—Ä–º–∞—Ç
python scripts/slo-report.py --format json --output reports/slo-report.json
```

6. **–°–æ–∑–¥–∞–π –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—é –¥–ª—è Error Budget Policy**:

`scripts/error-budget-enforcer.py`:
```python
#!/usr/bin/env python3
"""
Error Budget Policy Enforcer
–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–∏–º–µ–Ω—è–µ—Ç –ø–æ–ª–∏—Ç–∏–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ—Å—Ç–∞—Ç–∫–∞ error budget
"""
import requests
import sys

class ErrorBudgetEnforcer:
    def __init__(self, prometheus_url, github_token):
        self.prometheus_url = prometheus_url
        self.github_token = github_token
        self.thresholds = {
            'healthy': 50,
            'warning': 25,
            'critical': 10,
            'exhausted': 0
        }
    
    def get_error_budget_remaining(self):
        """Get current error budget remaining percentage"""
        query = '''
            100 - (
              (1 - (sum(rate(http_requests_total{job="api",status!~"(5..|429)"}[30d])) / sum(rate(http_requests_total{job="api"}[30d]))))
              /
              (1 - 0.999)
            ) * 100
        '''
        
        response = requests.get(
            f"{self.prometheus_url}/api/v1/query",
            params={'query': query}
        )
        data = response.json()
        
        if data['status'] == 'success' and data['data']['result']:
            return float(data['data']['result'][0]['value'][1])
        return None
    
    def determine_level(self, budget_remaining):
        """Determine current policy level"""
        if budget_remaining > self.thresholds['healthy']:
            return 'healthy', 'üü¢'
        elif budget_remaining > self.thresholds['warning']:
            return 'warning', 'üü°'
        elif budget_remaining > self.thresholds['critical']:
            return 'critical', 'üî¥'
        else:
            return 'exhausted', '‚õî'
    
    def enable_github_protections(self, level):
        """Enable GitHub branch protections based on level"""
        # Implementation for GitHub API
        protections = {
            'healthy': {
                'required_approving_review_count': 1,
                'dismiss_stale_reviews': False
            },
            'warning': {
                'required_approving_review_count': 2,
                'dismiss_stale_reviews': True
            },
            'critical': {
                'required_approving_review_count': 3,
                'dismiss_stale_reviews': True,
                'require_code_owner_reviews': True
            },
            'exhausted': {
                'required_approving_review_count': 4,
                'dismiss_stale_reviews': True,
                'require_code_owner_reviews': True,
                'required_status_checks': ['director-approval']
            }
        }
        
        return protections.get(level, protections['healthy'])
    
    def send_notification(self, level, budget_remaining, icon):
        """Send notification to team"""
        messages = {
            'healthy': f"{icon} Error budget healthy: {budget_remaining:.1f}% remaining",
            'warning': f"{icon} Error budget warning: {budget_remaining:.1f}% remaining. Reduce release frequency.",
            'critical': f"{icon} Error budget critical: {budget_remaining:.1f}% remaining. Feature freeze recommended.",
            'exhausted': f"{icon} Error budget exhausted! Complete code freeze in effect."
        }
        
        print(messages[level])
        # Implementation for Slack/Email notifications
    
    def enforce(self):
        """Enforce error budget policy"""
        budget_remaining = self.get_error_budget_remaining()
        
        if budget_remaining is None:
            print("‚ùå Could not retrieve error budget data")
            sys.exit(1)
        
        level, icon = self.determine_level(budget_remaining)
        
        print(f"\n{icon} Current Error Budget: {budget_remaining:.2f}%")
        print(f"Policy Level: {level.upper()}")
        
        # Apply protections
        protections = self.enable_github_protections(level)
        print(f"\nGitHub Protections: {protections}")
        
        # Send notifications
        self.send_notification(level, budget_remaining, icon)
        
        return level, budget_remaining

if __name__ == "__main__":
    import os
    
    prometheus_url = os.getenv('PROMETHEUS_URL', 'http://localhost:9090')
    github_token = os.getenv('GITHUB_TOKEN')
    
    enforcer = ErrorBudgetEnforcer(prometheus_url, github_token)
    level, budget = enforcer.enforce()
    
    # Exit code based on level
    exit_codes = {
        'healthy': 0,
        'warning': 1,
        'critical': 2,
        'exhausted': 3
    }
    
    sys.exit(exit_codes.get(level, 0))
```

–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ cron:
```bash
# /etc/cron.d/error-budget-enforcer
*/15 * * * * /usr/local/bin/error-budget-enforcer.py >> /var/log/error-budget.log 2>&1
```

### üöÄ –ë–æ–Ω—É—Å (–Ω–æ–≤–æ–µ)

**1. Composite SLO (–∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π SLO –¥–ª—è user journey)**:
```python
# composite_slo.py
class CompositeSLO:
    """
    –í—ã—á–∏—Å–ª–µ–Ω–∏–µ composite SLO –¥–ª—è multi-step user journey
    """
    def __init__(self, steps):
        self.steps = steps
    
    def calculate_composite_slo(self):
        """
        Composite SLO = –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ SLO –≤—Å–µ—Ö —à–∞–≥–æ–≤
        """
        composite = 1.0
        for step in self.steps:
            composite *= (step['slo'] / 100)
        return composite * 100
    
    def calculate_step_targets(self, target_composite_slo):
        """
        –í—ã—á–∏—Å–ª–∏—Ç—å –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ SLO –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —à–∞–≥–∞
        —á—Ç–æ–±—ã –¥–æ—Å—Ç–∏—á—å —Ü–µ–ª–µ–≤–æ–≥–æ composite SLO
        """
        num_steps = len(self.steps)
        # –†–∞–≤–Ω–æ–º–µ—Ä–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
        step_slo = (target_composite_slo / 100) ** (1 / num_steps) * 100
        return step_slo

# Example
checkout_flow = CompositeSLO([
    {'name': 'Add to Cart', 'slo': 99.9},
    {'name': 'View Cart', 'slo': 99.9},
    {'name': 'Payment', 'slo': 99.95},
    {'name': 'Confirmation', 'slo': 99.9}
])

composite = checkout_flow.calculate_composite_slo()
print(f"Composite SLO: {composite:.2f}%")
# Output: 99.65%

# –ï—Å–ª–∏ —Ö–æ—Ç–∏–º 99.9% composite, –∫–∞–∫–æ–π SLO –Ω—É–∂–µ–Ω –Ω–∞ –∫–∞–∂–¥–æ–º —à–∞–≥–µ?
required_step_slo = checkout_flow.calculate_step_targets(99.9)
print(f"Required per-step SLO: {required_step_slo:.3f}%")
# Output: 99.975%
```

**2. SLO as Code —Å Terraform**:
```hcl
# terraform/slo/main.tf
resource "grafana_slo" "api_availability" {
  name        = "API Availability"
  description = "API requests –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —É—Å–ø–µ—à–Ω—ã–º–∏"
  
  query {
    type = "prometheus"
    
    # Success metric
    success {
      metric = "http_requests_total"
      filters = {
        job    = "api"
        status = "!~(5..|429)"
      }
    }
    
    # Total metric
    total {
      metric = "http_requests_total"
      filters = {
        job = "api"
      }
    }
  }
  
  objectives {
    value  = 99.9
    window = "30d"
  }
  
  alerting {
    fast_burn {
      enabled   = true
      threshold = 14.4
      window    = "1h"
    }
    
    slow_burn {
      enabled   = true
      threshold = 6
      window    = "6h"
    }
  }
  
  labels = {
    team     = "backend"
    service  = "api"
    tier     = "critical"
  }
}
```

**3. SLO Simulator –¥–ª—è testing**:
```python
# slo_simulator.py
import random
from datetime import datetime, timedelta

class SLOSimulator:
    """
    –°–∏–º—É–ª—è—Ç–æ—Ä –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è SLO policies
    """
    def __init__(self, target_slo, total_requests_per_day):
        self.target_slo = target_slo
        self.total_requests_per_day = total_requests_per_day
        self.error_budget = 100 - target_slo
    
    def simulate_month(self, incident_scenarios):
        """
        –°–∏–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –º–µ—Å—è—Ü —Ä–∞–±–æ—Ç—ã —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —Å—Ü–µ–Ω–∞—Ä–∏—è–º–∏ –∏–Ω—Ü–∏–¥–µ–Ω—Ç–æ–≤
        """
        days = 30
        total_requests = self.total_requests_per_day * days
        allowed_failures = total_requests * (self.error_budget / 100)
        
        print(f"Simulation Parameters:")
        print(f"  Target SLO: {self.target_slo}%")
        print(f"  Error Budget: {self.error_budget}%")
        print(f"  Total Requests (30d): {total_requests:,}")
        print(f"  Allowed Failures: {allowed_failures:,.0f}")
        print()
        
        actual_failures = 0
        
        for scenario in incident_scenarios:
            duration_minutes = scenario['duration_minutes']
            failure_rate = scenario['failure_rate']
            
            requests_during_incident = (duration_minutes / 1440) * self.total_requests_per_day
            failures = requests_during_incident * failure_rate
            
            actual_failures += failures
            
            print(f"Incident: {scenario['name']}")
            print(f"  Duration: {duration_minutes} minutes")
            print(f"  Failure Rate: {failure_rate * 100}%")
            print(f"  Requests Affected: {requests_during_incident:,.0f}")
            print(f"  Failures: {failures:,.0f}")
            print()
        
        actual_slo = ((total_requests - actual_failures) / total_requests) * 100
        budget_consumed = (actual_failures / allowed_failures) * 100
        
        print(f"Results:")
        print(f"  Actual SLO: {actual_slo:.4f}%")
        print(f"  Budget Consumed: {budget_consumed:.1f}%")
        print(f"  Status: {'‚úÖ Met' if actual_slo >= self.target_slo else '‚ùå Violated'}")
        
        return actual_slo, budget_consumed

# Example usage
simulator = SLOSimulator(target_slo=99.9, total_requests_per_day=10_000_000)

incidents = [
    {
        'name': 'Database Outage',
        'duration_minutes': 30,
        'failure_rate': 1.0  # 100% failure
    },
    {
        'name': 'High Latency Event',
        'duration_minutes': 120,
        'failure_rate': 0.2  # 20% failure
    },
    {
        'name': 'Partial Service Degradation',
        'duration_minutes': 60,
        'failure_rate': 0.5  # 50% failure
    }
]

simulator.simulate_month(incidents)
```

---

## –ò—Ç–æ–≥–∏ –º–æ–¥—É–ª—è 8

–ü–æ—Å–ª–µ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏—è —ç—Ç–æ–≥–æ –º–æ–¥—É–ª—è —Ç—ã –¥–æ–ª–∂–µ–Ω —É–º–µ—Ç—å:

‚úÖ –û–ø—Ä–µ–¥–µ–ª—è—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ SLI –¥–ª—è —Å–µ—Ä–≤–∏—Å–æ–≤
‚úÖ –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã–µ SLO targets
‚úÖ –í—ã—á–∏—Å–ª—è—Ç—å –∏ –º–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å Error Budget
‚úÖ –ù–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å multi-window alerting
‚úÖ –°–æ–∑–¥–∞–≤–∞—Ç—å SLO dashboards
‚úÖ –ì–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ SLO reports
‚úÖ –ü—Ä–∏–º–µ–Ω—è—Ç—å Error Budget Policy
‚úÖ –í—ã—á–∏—Å–ª—è—Ç—å composite SLO
‚úÖ –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞—Ç—å reliability –∏ velocity
‚úÖ –ü—Ä–∏–Ω–∏–º–∞—Ç—å data-driven —Ä–µ—à–µ–Ω–∏—è –æ releases

**–ö–ª—é—á–µ–≤—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã SRE:**
1. SLO –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç target reliability
2. Error Budget –ø–æ–∑–≤–æ–ª—è–µ—Ç –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞—Ç—å —Ä–∏—Å–∫
3. –ò–∑–º–µ—Ä—è–π —Ç–æ, —á—Ç–æ –≤–∞–∂–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º
4. –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä—É–π enforcement policies
5. –ò—Å–ø–æ–ª—å–∑—É–π –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π
6. –†–µ–≥—É–ª—è—Ä–Ω–æ –ø–µ—Ä–µ—Å–º–∞—Ç—Ä–∏–≤–∞–π SLO targets
7. –î–æ–∫—É–º–µ–Ω—Ç–∏—Ä—É–π –∏ –∫–æ–º–º—É–Ω–∏—Ü–∏—Ä—É–π —Å—Ç–∞—Ç—É—Å
````
---
## –ú–æ–¥—É–ª—å 10: Infrastructure as Code –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ (35 –º–∏–Ω—É—Ç)

### üéØ –ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞

**IaC –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ - –∑–∞—á–µ–º:**

```
–ü—Ä–æ–±–ª–µ–º–∞: –†—É—á–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
‚ùå –î–æ–ª–≥–æ (—á–∞—Å—ã –Ω–∞ setup)
‚ùå –û—à–∏–±–∫–∏ (—á–µ–ª–æ–≤–µ—á–µ—Å–∫–∏–π —Ñ–∞–∫—Ç–æ—Ä)
‚ùå –ù–µ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ
‚ùå –°–ª–æ–∂–Ω–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞—Ç—å
‚ùå –ù–µ—Ç version control

–†–µ—à–µ–Ω–∏–µ: Infrastructure as Code
‚úÖ –ë—ã—Å—Ç—Ä–æ (–º–∏–Ω—É—Ç—ã –Ω–∞ deploy)
‚úÖ –ù–∞–¥–µ–∂–Ω–æ (–∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è)
‚úÖ –í–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ (–∏–¥–µ–Ω—Ç–∏—á–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è)
‚úÖ –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ (–ª–µ–≥–∫–æ –¥–æ–±–∞–≤–∏—Ç—å –Ω–æ–≤—ã–µ —Å–µ—Ä–≤–∏—Å—ã)
‚úÖ Version control (Git history)
```

**–û—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—ã:**

```
1. Configuration Management:
   - Ansible
   - Chef
   - Puppet
   - SaltStack

2. Container Orchestration:
   - Docker Compose
   - Kubernetes (Helm)
   - Docker Swarm

3. Infrastructure Provisioning:
   - Terraform
   - Pulumi
   - CloudFormation (AWS)

4. GitOps:
   - ArgoCD
   - Flux
   - Jenkins X
```

**Terraform –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞:**

```
Provider Support:
- Prometheus (rules, alertmanager config)
- Grafana (dashboards, data sources, folders)
- PagerDuty (services, escalation policies)
- Datadog (monitors, dashboards)
- AWS CloudWatch (alarms, dashboards)

–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:
- –î–µ–∫–ª–∞—Ä–∞—Ç–∏–≤–Ω—ã–π —Å–∏–Ω—Ç–∞–∫—Å–∏—Å
- State management
- Plan/Apply workflow
- Module reusability
```

**Ansible –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞:**

```
–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
- –£—Å—Ç–∞–Ω–æ–≤–∫–∞ monitoring agents
- –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è exporters
- Deployment monitoring stack
- –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ dashboards

–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:
- Agentless (SSH)
- –ü—Ä–æ—Å—Ç–æ–π YAML —Å–∏–Ω—Ç–∞–∫—Å–∏—Å
- –ë–æ–ª—å—à–∞—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ modules
- Idempotent operations
```

**Helm –¥–ª—è Kubernetes:**

```
–û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ charts:
- prometheus-community/kube-prometheus-stack
- grafana/grafana
- grafana/loki-stack
- jaegertracing/jaeger
- elastic/elasticsearch

–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:
- Templating
- Values override
- Release management
- Dependency management
```

**GitOps workflow:**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Git    ‚îÇ (Source of Truth)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ
     ‚îÇ Push
     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  CI/CD   ‚îÇ (Validation, Testing)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ
     ‚îÇ Deploy
     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Cluster  ‚îÇ (Auto-sync)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Principles:
1. Declarative configuration
2. Version controlled
3. Automated deployment
4. Self-healing
```

**Prometheus Configuration Management:**

yaml

```yaml
# prometheus.yml –∫–∞–∫ –∫–æ–¥
global:
  scrape_interval: 15s
  evaluation_interval: 15s
  external_labels:
    cluster: {{ cluster_name }}
    environment: {{ environment }}

# Template –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
alerting:
  alertmanagers:
  - static_configs:
    - targets: 
      {{ range .AlertmanagerTargets }}
      - {{ . }}
      {{ end }}

scrape_configs:
  {{ range .Jobs }}
  - job_name: {{ .Name }}
    static_configs:
      - targets: {{ .Targets }}
        labels: {{ .Labels }}
  {{ end }}
```

**Grafana Dashboard as Code:**

json

```json
{
  "dashboard": {
    "title": "{{ .Title }}",
    "tags": {{ .Tags | toJson }},
    "timezone": "browser",
    "panels": [
      {{ range .Panels }}
      {
        "id": {{ .ID }},
        "title": "{{ .Title }}",
        "type": "{{ .Type }}",
        "targets": [
          {
            "expr": "{{ .Query }}",
            "legendFormat": "{{ .Legend }}"
          }
        ]
      }{{ if not (last $.Panels .) }},{{ end }}
      {{ end }}
    ]
  }
}
```

**Alert Rules as Code:**

yaml

````yaml
# alerts.yml template
groups:
{{ range .AlertGroups }}
  - name: {{ .Name }}
    interval: {{ .Interval }}
    rules:
    {{ range .Rules }}
    - alert: {{ .Name }}
      expr: |
        {{ .Expression }}
      for: {{ .For }}
      labels:
        severity: {{ .Severity }}
        team: {{ .Team }}
      annotations:
        summary: {{ .Summary }}
        description: {{ .Description }}
        runbook: {{ .Runbook }}
    {{ end }}
{{ end }}
````

**Monitoring Stack Components:**
```
Full Stack:
‚îú‚îÄ‚îÄ Metrics Collection
‚îÇ   ‚îú‚îÄ‚îÄ Prometheus
‚îÇ   ‚îú‚îÄ‚îÄ Node Exporter
‚îÇ   ‚îú‚îÄ‚îÄ Blackbox Exporter
‚îÇ   ‚îî‚îÄ‚îÄ Custom Exporters
‚îú‚îÄ‚îÄ Logs Collection
‚îÇ   ‚îú‚îÄ‚îÄ Loki
‚îÇ   ‚îú‚îÄ‚îÄ Promtail
‚îÇ   ‚îî‚îÄ‚îÄ Fluentd/Fluent Bit
‚îú‚îÄ‚îÄ Tracing
‚îÇ   ‚îú‚îÄ‚îÄ Jaeger/Tempo
‚îÇ   ‚îî‚îÄ‚îÄ OpenTelemetry Collector
‚îú‚îÄ‚îÄ Visualization
‚îÇ   ‚îî‚îÄ‚îÄ Grafana
‚îú‚îÄ‚îÄ Alerting
‚îÇ   ‚îî‚îÄ‚îÄ Alertmanager
‚îî‚îÄ‚îÄ Notification
    ‚îú‚îÄ‚îÄ Slack
    ‚îú‚îÄ‚îÄ PagerDuty
    ‚îî‚îÄ‚îÄ Email
```

**Directory Structure (best practices):**
````
monitoring-infrastructure/
‚îú‚îÄ‚îÄ terraform/
‚îÇ   ‚îú‚îÄ‚îÄ modules/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prometheus/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ grafana/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ alertmanager/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ loki/
‚îÇ   ‚îú‚îÄ‚îÄ environments/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dev/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ staging/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ prod/
‚îÇ   ‚îî‚îÄ‚îÄ main.tf
‚îú‚îÄ‚îÄ ansible/
‚îÇ   ‚îú‚îÄ‚îÄ playbooks/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ install-prometheus.yml
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ configure-exporters.yml
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ deploy-dashboards.yml
‚îÇ   ‚îú‚îÄ‚îÄ roles/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prometheus/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ grafana/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ node-exporter/
‚îÇ   ‚îî‚îÄ‚îÄ inventory/
‚îú‚îÄ‚îÄ kubernetes/
‚îÇ   ‚îú‚îÄ‚îÄ helm/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ values-dev.yaml
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ values-staging.yaml
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ values-prod.yaml
‚îÇ   ‚îî‚îÄ‚îÄ manifests/
‚îú‚îÄ‚îÄ docker/
‚îÇ   ‚îú‚îÄ‚îÄ docker-compose.yml
‚îÇ   ‚îî‚îÄ‚îÄ docker-compose.override.yml
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ prometheus/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prometheus.yml
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ alerts/
‚îÇ   ‚îú‚îÄ‚îÄ grafana/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ dashboards/
‚îÇ   ‚îî‚îÄ‚îÄ alertmanager/
‚îÇ       ‚îî‚îÄ‚îÄ alertmanager.yml
‚îî‚îÄ‚îÄ scripts/
    ‚îú‚îÄ‚îÄ deploy.sh
    ‚îú‚îÄ‚îÄ backup.sh
    ‚îî‚îÄ‚îÄ validate.sh
````

**Validation & Testing:**

bash

````bash
# Prometheus config validation
promtool check config prometheus.yml
promtool check rules alerts.yml

# Alert testing
promtool test rules test-alerts.yml

# Grafana dashboard validation
grafana-cli admin validate-dashboard dashboard.json

# Terraform validation
terraform validate
terraform plan

# Ansible syntax check
ansible-playbook --syntax-check playbook.yml
ansible-lint playbook.yml
````

**Backup & Disaster Recovery:**
````
–ß—Ç–æ –±—ç–∫–∞–ø–∏—Ç—å:
1. ‚úÖ Prometheus TSDB (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –¥–∞–Ω–Ω—ã–µ ephemeral)
2. ‚úÖ Grafana database (dashboards, users, settings)
3. ‚úÖ Alertmanager data (silences, notification log)
4. ‚úÖ Configuration files (prometheus.yml, alerts, etc)
5. ‚úÖ Custom exporters config

–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã:
- Prometheus: snapshots API
- Grafana: grafana-backup tool, API export
- Velero: Kubernetes backup
- Restic: filesystem backup
````

**Environment Management:**

yaml

````yaml
# –†–∞–∑–Ω—ã–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è –æ–∫—Ä—É–∂–µ–Ω–∏–π
Dev:
  retention: 7d
  replicas: 1
  resources: small
  scrape_interval: 30s

Staging:
  retention: 14d
  replicas: 2
  resources: medium
  scrape_interval: 15s

Production:
  retention: 30d
  replicas: 3
  resources: large
  scrape_interval: 15s
  high_availability: true
````

**Secrets Management:**
````
Options:
1. HashiCorp Vault
   - Centralized secrets
   - Dynamic credentials
   - Audit logging

2. Kubernetes Secrets
   - Native K8s
   - External Secrets Operator

3. AWS Secrets Manager
   - Managed service
   - Rotation support

4. Sealed Secrets
   - GitOps friendly
   - Encrypted in Git

Best Practice: Never commit secrets to Git!
````

**CI/CD Pipeline –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞:**

yaml

```yaml
# .github/workflows/monitoring.yml
name: Deploy Monitoring Stack

on:
  push:
    branches: [main]
    paths:
      - 'monitoring/**'

jobs:
  validate:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      
      - name: Validate Prometheus Config
        run: |
          docker run --rm -v $PWD:/prometheus prom/prometheus:latest \
            promtool check config /prometheus/config/prometheus.yml
      
      - name: Test Alert Rules
        run: |
          docker run --rm -v $PWD:/prometheus prom/prometheus:latest \
            promtool test rules /prometheus/tests/alerts-test.yml
      
      - name: Validate Grafana Dashboards
        run: |
          for file in grafana/dashboards/*.json; do
            jq empty "$file" || exit 1
          done

  deploy-dev:
    needs: validate
    runs-on: ubuntu-latest
    environment: dev
    steps:
      - name: Deploy to Dev
        run: |
          helm upgrade --install monitoring ./helm \
            --values values-dev.yaml \
            --namespace monitoring \
            --create-namespace

  deploy-prod:
    needs: deploy-dev
    runs-on: ubuntu-latest
    environment: production
    steps:
      - name: Deploy to Production
        run: |
          helm upgrade --install monitoring ./helm \
            --values values-prod.yaml \
            --namespace monitoring
```

### üíª –ó–∞–¥–∞–Ω–∏–µ

–°–æ–∑–¥–∞–π –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—É—é IaC –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞:

1. **–°–æ–∑–¥–∞–π Terraform –ø—Ä–æ–µ–∫—Ç –¥–ª—è Grafana**:

`terraform/main.tf`:

hcl

```hcl
terraform {
  required_version = ">= 1.0"
  
  required_providers {
    grafana = {
      source  = "grafana/grafana"
      version = "~> 2.9.0"
    }
  }
}

provider "grafana" {
  url  = var.grafana_url
  auth = var.grafana_auth
}

# Data source –¥–ª—è Prometheus
resource "grafana_data_source" "prometheus" {
  type = "prometheus"
  name = "Prometheus"
  url  = var.prometheus_url
  
  is_default = true
  
  json_data_encoded = jsonencode({
    httpMethod    = "POST"
    timeInterval  = "30s"
  })
}

# Data source –¥–ª—è Loki
resource "grafana_data_source" "loki" {
  type = "loki"
  name = "Loki"
  url  = var.loki_url
  
  json_data_encoded = jsonencode({
    maxLines = 1000
  })
}

# Folder –¥–ª—è dashboards
resource "grafana_folder" "monitoring" {
  title = "Monitoring"
}

resource "grafana_folder" "applications" {
  title = "Applications"
}

# Dashboard - Node Exporter
resource "grafana_dashboard" "node_exporter" {
  folder      = grafana_folder.monitoring.id
  config_json = file("${path.module}/dashboards/node-exporter.json")
}

# Dashboard - Application Metrics
resource "grafana_dashboard" "application" {
  folder      = grafana_folder.applications.id
  config_json = templatefile("${path.module}/dashboards/application.json.tpl", {
    datasource = grafana_data_source.prometheus.name
    environment = var.environment
  })
}

# Alert notification channel - Slack
resource "grafana_contact_point" "slack" {
  name = "Slack Alerts"
  
  slack {
    url  = var.slack_webhook_url
    text = templatefile("${path.module}/templates/slack-message.tpl", {})
  }
}

# Alert notification channel - PagerDuty
resource "grafana_contact_point" "pagerduty" {
  name = "PagerDuty"
  
  pagerduty {
    integration_key = var.pagerduty_key
    severity        = "critical"
  }
}

# Notification policy
resource "grafana_notification_policy" "main" {
  group_by      = ["alertname", "grafana_folder"]
  group_wait    = "10s"
  group_interval = "5m"
  repeat_interval = "4h"
  
  policy {
    matcher {
      label = "severity"
      match = "="
      value = "critical"
    }
    contact_point = grafana_contact_point.pagerduty.name
    continue      = true
  }
  
  policy {
    matcher {
      label = "severity"
      match = "="
      value = "warning"
    }
    contact_point = grafana_contact_point.slack.name
  }
}

# Alert rule - High CPU
resource "grafana_rule_group" "infrastructure" {
  name             = "Infrastructure Alerts"
  folder_uid       = grafana_folder.monitoring.uid
  interval_seconds = 60
  
  rule {
    name      = "HighCPUUsage"
    condition = "C"
    
    data {
      ref_id = "A"
      
      relative_time_range {
        from = 600
        to   = 0
      }
      
      datasource_uid = grafana_data_source.prometheus.uid
      model = jsonencode({
        expr         = "100 - (avg(irate(node_cpu_seconds_total{mode=\"idle\"}[5m])) * 100)"
        refId        = "A"
        intervalMs   = 1000
        maxDataPoints = 43200
      })
    }
    
    data {
      ref_id = "B"
      
      relative_time_range {
        from = 0
        to   = 0
      }
      
      datasource_uid = "__expr__"
      model = jsonencode({
        expression = "A"
        reducer    = "last"
        refId      = "B"
        type       = "reduce"
      })
    }
    
    data {
      ref_id = "C"
      
      relative_time_range {
        from = 0
        to   = 0
      }
      
      datasource_uid = "__expr__"
      model = jsonencode({
        expression = "B > 80"
        refId      = "C"
        type       = "threshold"
      })
    }
    
    no_data_state  = "NoData"
    exec_err_state = "Error"
    for            = "5m"
    
    annotations = {
      summary     = "High CPU usage detected"
      description = "CPU usage is above 80%"
      runbook_url = "https://runbooks.example.com/high-cpu"
    }
    
    labels = {
      severity = "warning"
      team     = "infrastructure"
    }
  }
}

# Organization settings
resource "grafana_organization_preferences" "main" {
  theme            = "dark"
  home_dashboard_uid = grafana_dashboard.node_exporter.uid
  timezone         = "UTC"
}

# Team
resource "grafana_team" "infrastructure" {
  name  = "Infrastructure Team"
  email = "infra@example.com"
}

# Service Account –¥–ª—è API access
resource "grafana_service_account" "automation" {
  name = "automation"
  role = "Admin"
}

resource "grafana_service_account_token" "automation" {
  name               = "automation-token"
  service_account_id = grafana_service_account.automation.id
}
```

`terraform/variables.tf`:

hcl

```hcl
variable "grafana_url" {
  description = "Grafana URL"
  type        = string
  default     = "http://localhost:3000"
}

variable "grafana_auth" {
  description = "Grafana auth (admin:password)"
  type        = string
  sensitive   = true
}

variable "prometheus_url" {
  description = "Prometheus URL"
  type        = string
  default     = "http://prometheus:9090"
}

variable "loki_url" {
  description = "Loki URL"
  type        = string
  default     = "http://loki:3100"
}

variable "environment" {
  description = "Environment name"
  type        = string
  default     = "development"
}

variable "slack_webhook_url" {
  description = "Slack webhook URL"
  type        = string
  sensitive   = true
}

variable "pagerduty_key" {
  description = "PagerDuty integration key"
  type        = string
  sensitive   = true
}
```

`terraform/outputs.tf`:

hcl

```hcl
output "prometheus_datasource_uid" {
  description = "Prometheus data source UID"
  value       = grafana_data_source.prometheus.uid
}

output "loki_datasource_uid" {
  description = "Loki data source UID"
  value       = grafana_data_source.loki.uid
}

output "automation_token" {
  description = "Automation service account token"
  value       = grafana_service_account_token.automation.key
  sensitive   = true
}

output "dashboard_urls" {
  description = "URLs of created dashboards"
  value = {
    node_exporter = "${var.grafana_url}/d/${grafana_dashboard.node_exporter.uid}"
    application   = "${var.grafana_url}/d/${grafana_dashboard.application.uid}"
  }
}
```

`terraform/dashboards/application.json.tpl`:

json

```json
{
  "dashboard": {
    "title": "Application Metrics - ${environment}",
    "tags": ["application", "${environment}"],
    "timezone": "browser",
    "panels": [
      {
        "id": 1,
        "title": "Request Rate",
        "type": "timeseries",
        "gridPos": {
          "h": 8,
          "w": 12,
          "x": 0,
          "y": 0
        },
        "targets": [
          {
            "datasource": "${datasource}",
            "expr": "sum(rate(http_requests_total{environment=\"${environment}\"}[5m])) by (service)",
            "legendFormat": "{{service}}"
          }
        ]
      },
      {
        "id": 2,
        "title": "Error Rate",
        "type": "stat",
        "gridPos": {
          "h": 8,
          "w": 12,
          "x": 12,
          "y": 0
        },
        "targets": [
          {
            "datasource": "${datasource}",
            "expr": "sum(rate(http_requests_total{environment=\"${environment}\",status=~\"5..\"}[5m])) / sum(rate(http_requests_total{environment=\"${environment}\"}[5m]))",
            "legendFormat": "Error Rate"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "percentunit",
            "thresholds": {
              "steps": [
                {"value": 0, "color": "green"},
                {"value": 0.01, "color": "yellow"},
                {"value": 0.05, "color": "red"}
              ]
            }
          }
        }
      }
    ]
  }
}
```

2. **–°–æ–∑–¥–∞–π Ansible playbook –¥–ª—è deployment**:

`ansible/inventory/hosts.ini`:

ini

```ini
[monitoring_servers]
monitoring-01 ansible_host=192.168.1.10 ansible_user=ubuntu
monitoring-02 ansible_host=192.168.1.11 ansible_user=ubuntu

[app_servers]
app-01 ansible_host=192.168.1.20 ansible_user=ubuntu
app-02 ansible_host=192.168.1.21 ansible_user=ubuntu
app-03 ansible_host=192.168.1.22 ansible_user=ubuntu

[all:vars]
ansible_python_interpreter=/usr/bin/python3
environment=production
```

`ansible/playbooks/deploy-monitoring-stack.yml`:

yaml

````yaml
---
- name: Deploy Monitoring Stack
  hosts: monitoring_servers
  become: yes
  vars:
    prometheus_version: "2.48.0"
    grafana_version: "10.2.3"
    alertmanager_version: "0.26.0"
    node_exporter_version: "1.7.0"
    
  tasks:
    - name: Update apt cache
      apt:
        update_cache: yes
        cache_valid_time: 3600
    
    - name: Install prerequisites
      apt:
        name:
          - apt-transport-https
          - software-properties-common
          - wget
          - curl
          - tar
        state: present
    
    - name: Create monitoring user
      user:
        name: monitoring
        system: yes
        shell: /bin/false
        create_home: no
    
    - name: Deploy Prometheus
      include_role:
        name: prometheus
      vars:
        prometheus_config_template: "{{ playbook_dir }}/../config/prometheus/prometheus.yml.j2"
        prometheus_alerts_dir: "{{ playbook_dir }}/../config/prometheus/alerts"
    
    - name: Deploy Alertmanager
      include_role:
        name: alertmanager
      vars:
        alertmanager_config_template: "{{ playbook_dir }}/../config/alertmanager/alertmanager.yml.j2"
    
    - name: Deploy Grafana
      include_role:
        name: grafana
      vars:
        grafana_provisioning_dir: "{{ playbook_dir }}/../config/grafana/provisioning"

- name: Install Node Exporter on all servers
  hosts: all
  become: yes
  tasks:
    - name: Deploy Node Exporter
      include_role:
        name: node_exporter

- name: Configure Application Monitoring
  hosts: app_servers
  become: yes
  tasks:
    - name: Install application exporters
      include_role:
        name: app_exporter
````

`ansible/roles/prometheus/tasks/main.yml`:

yaml

````yaml
---
- name: Create Prometheus directories
  file:
    path: "{{ item }}"
    state: directory
    owner: monitoring
    group: monitoring
    mode: '0755'
  loop:
    - /etc/prometheus
    - /etc/prometheus/rules
    - /var/lib/prometheus

- name: Download Prometheus
  get_url:
    url: "https://github.com/prometheus/prometheus/releases/download/v{{ prometheus_version }}/prometheus-{{ prometheus_version }}.linux-amd64.tar.gz"
    dest: "/tmp/prometheus-{{ prometheus_version }}.tar.gz"

- name: Extract Prometheus
  unarchive:
    src: "/tmp/prometheus-{{ prometheus_version }}.tar.gz"
    dest: /tmp
    remote_src: yes

- name: Install Prometheus binaries
  copy:
    src: "/tmp/prometheus-{{ prometheus_version }}.linux-amd64/{{ item }}"
    dest: "/usr/local/bin/{{ item }}"
    owner: monitoring
    group: monitoring
    mode: '0755'
    remote_src: yes
  loop:
    - prometheus
    - promtool

- name: Copy Prometheus configuration
  template:
    src: "{{ prometheus_config_template }}"
    dest: /etc/prometheus/prometheus.yml
    owner: monitoring
    group: monitoring
    mode: '0644'
  notify: reload prometheus

- name: Copy alert rules
  copy:
    src: "{{ prometheus_alerts_dir }}/"
    dest: /etc/prometheus/rules/
    owner: monitoring
    group: monitoring
    mode: '0644'
  notify: reload prometheus

- name: Create Prometheus systemd service
  template:
    src: prometheus.service.j2
    dest: /etc/systemd/system/prometheus.service
    owner: root
    group: root
    mode: '0644'
  notify: restart prometheus

- name: Enable and start Prometheus
  systemd:
    name: prometheus
    enabled: yes
    state: started
    daemon_reload: yes
````

`ansible/roles/prometheus/templates/prometheus.service.j2`:

ini

```ini
[Unit]
Description=Prometheus
Wants=network-online.target
After=network-online.target

[Service]
User=monitoring
Group=monitoring
Type=simple
ExecStart=/usr/local/bin/prometheus \
  --config.file=/etc/prometheus/prometheus.yml \
  --storage.tsdb.path=/var/lib/prometheus \
  --web.console.templates=/etc/prometheus/consoles \
  --web.console.libraries=/etc/prometheus/console_libraries \
  --storage.tsdb.retention.time={{ prometheus_retention | default('30d') }} \
  --web.enable-lifecycle \
  --web.enable-admin-api

Restart=always
RestartSec=5

[Install]
WantedBy=multi-user.target
```

`ansible/roles/prometheus/handlers/main.yml`:

yaml

````yaml
---
- name: reload prometheus
  uri:
    url: "http://localhost:9090/-/reload"
    method: POST
  listen: reload prometheus

- name: restart prometheus
  systemd:
    name: prometheus
    state: restarted
  listen: restart prometheus
````

3. **–°–æ–∑–¥–∞–π Helm chart –¥–ª—è Kubernetes**:

`helm/monitoring/Chart.yaml`:

yaml

```yaml
apiVersion: v2
name: monitoring-stack
description: Complete monitoring stack for Kubernetes
type: application
version: 1.0.0
appVersion: "1.0"

dependencies:
  - name: kube-prometheus-stack
    version: "55.0.0"
    repository: "https://prometheus-community.github.io/helm-charts"
    condition: prometheus.enabled
    
  - name: loki-stack
    version: "2.9.11"
    repository: "https://grafana.github.io/helm-charts"
    condition: loki.enabled
    
  - name: jaeger
    version: "0.71.11"
    repository: "https://jaegertracing.github.io/helm-charts"
    condition: jaeger.enabled
```

`helm/monitoring/values.yaml`:

yaml

```yaml
# Global settings
global:
  environment: production
  clusterName: main-cluster

# Prometheus configuration
prometheus:
  enabled: true
  
kube-prometheus-stack:
  prometheus:
    prometheusSpec:
      retention: 30d
      retentionSize: "50GB"
      replicas: 2
      storageSpec:
        volumeClaimTemplate:
          spec:
            accessModes: ["ReadWriteOnce"]
            resources:
              requests:
                storage: 100Gi
      
      # Additional scrape configs
      additionalScrapeConfigs:
        - job_name: 'custom-app'
          kubernetes_sd_configs:
            - role: pod
          relabel_configs:
            - source_labels: [__meta_kubernetes_pod_label_app]
              regex: my-app
              action: keep
      
      # Remote write (–¥–ª—è long-term storage)
      remoteWrite:
        - url: http://thanos-receiver:19291/api/v1/receive
          queueConfig:
            capacity: 10000
            maxShards: 50
  
  # Alert rules
  additionalPrometheusRulesMap:
    custom-alerts:
      groups:
        - name: custom_application_alerts
          interval: 30s
          rules:
            - alert: ApplicationDown
              expr: up{job="my-app"} == 0
              for: 2m
              labels:
                severity: critical
                team: backend
              annotations:
                summary: "Application {{ $labels.instance }} is down"
                description: "Application has been down for more than 2 minutes"
  
  # Alertmanager
  alertmanager:
    config:
      global:
        resolve_timeout: 5m
        slack_api_url: {{ .Values.slack.webhookUrl }}
      
      route:
        group_by: ['alertname', 'cluster', 'service']
        group_wait: 10s
        group_interval: 5m
        repeat_interval: 4h
        receiver: 'default'
        
        routes:
          - match:
              severity: critical
            receiver: pagerduty
            continue: true
          
          - match:
              severity: warning
            receiver: slack
      
      receivers:
        - name: 'default'
          webhook_configs:
            - url: 'http://webhook-receiver:8080/webhook'
        
        - name: 'slack'
          slack_configs:
            - channel: '#alerts'
              title: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
              text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
        
        - name: 'pagerduty'
          pagerduty_configs:
            - service_key: {{ .Values.pagerduty.serviceKey }}
  
  # Grafana
  grafana:
    enabled: true
    adminPassword: {{ .Values.grafana.adminPassword }}
    
    persistence:
      enabled: true
      size: 10Gi
    
    dashboardProviders:
      dashboardproviders.yaml:
        apiVersion: 1
        providers:
          - name: 'default'
            orgId: 1
            folder: ''
            type: file
            disableDeletion: false
            editable: true
            options:
              path: /var/lib/grafana/dashboards/default
    
    dashboards:
      default:
        node-exporter:
          gnetId: 1860
          revision: 31
          datasource: Prometheus
        
        kubernetes-cluster:
          gnetId: 7249
          revision: 1
          datasource: Prometheus

# Loki configuration
loki:
  enabled: true

loki-stack:
  loki:
    persistence:
      enabled: true
      size: 50Gi
    
    config:
      limits_config:
        retention_period: 168h  # 7 days
      
      compactor:
        retention_enabled: true
  
  promtail:
    config:
      clients:
        - url: http://loki:3100/loki/api/v1/push

# Jaeger configuration
jaeger:
  enabled: true

jaeger:
  storage:
    type: elasticsearch
  
  elasticsearch:
    replicas: 3
    minimumMasterNodes: 2
```

`helm/monitoring/values-dev.yaml`:
```yaml
global:
  environment: development
  clusterName: dev-cluster

kube-prometheus-stack:
  prometheus:
    prometheusSpec:
      retention: 7d
      replicas: 1
      storageSpec:
        volumeClaimTemplate:
          spec:
            resources:
              requests:
                storage: 20Gi

loki-stack:
  loki:
    config:
      limits_config:
        retention_period: 48h

jaeger:
  enabled: false  # Disable in dev
```

4. **–°–æ–∑–¥–∞–π GitOps configuration –¥–ª—è ArgoCD**:

`argocd/monitoring-app.yaml`:
```yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: monitoring-stack
  namespace: argocd
spec:
  project: default
  
  source:
    repoURL: https://github.com/your-org/monitoring-infrastructure.git
    targetRevision: main
    path: helm/monitoring
    helm:
      valueFiles:
        - values.yaml
        - values-{{ .Values.environment }}.yaml
  
  destination:
    server: https://kubernetes.default.svc
    namespace: monitoring
  
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
      allowEmpty: false
    
    syncOptions:
      - CreateNamespace=true
      - PrunePropagationPolicy=foreground
      - PruneLast=true
    
    retry:
      limit: 5
      backoff:
        duration: 5s
        factor: 2
        maxDuration: 3m
  
  ignoreDifferences:
    - group: apps
      kind: Deployment
      jsonPointers:
        - /spec/replicas  # Ignore HPA changes
```

5. **–°–æ–∑–¥–∞–π CI/CD pipeline**:

`.github/workflows/deploy-monitoring.yml`:
```yaml
name: Deploy Monitoring Stack

on:
  push:
    branches: [main, develop]
    paths:
      - 'helm/**'
      - 'config/**'
      - 'terraform/**'
  pull_request:
    branches: [main]

env:
  HELM_VERSION: v3.13.0
  TERRAFORM_VERSION: 1.6.0

jobs:
  validate:
    name: Validate Configurations
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Setup Helm
        uses: azure/setup-helm@v3
        with:
          version: ${{ env.HELM_VERSION }}
      
      - name: Lint Helm Charts
        run: |
          helm lint helm/monitoring
          helm lint helm/monitoring --values helm/monitoring/values-dev.yaml
          helm lint helm/monitoring --values helm/monitoring/values-prod.yaml
      
      - name: Validate Prometheus Config
        run: |
          docker run --rm -v $PWD/config/prometheus:/prometheus \
            prom/prometheus:latest \
            promtool check config /prometheus/prometheus.yml
      
      - name: Test Alert Rules
        run: |
          docker run --rm -v $PWD:/workspace \
            prom/prometheus:latest \
            promtool test rules /workspace/tests/alerts-test.yml
      
      - name: Validate Grafana Dashboards
        run: |
          for file in config/grafana/dashboards/*.json; do
            echo "Validating $file"
            jq empty "$file" || exit 1
          done
      
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}
      
      - name: Terraform Format Check
        run: terraform fmt -check -recursive terraform/
      
      - name: Terraform Validate
        run: |
          cd terraform
          terraform init -backend=false
          terraform validate

  deploy-dev:
    name: Deploy to Development
    needs: validate
    if: github.ref == 'refs/heads/develop'
    runs-on: ubuntu-latest
    environment: development
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Setup kubectl
        uses: azure/setup-kubectl@v3
      
      - name: Setup Helm
        uses: azure/setup-helm@v3
        with:
          version: ${{ env.HELM_VERSION }}
      
      - name: Configure kubeconfig
        run: |
          echo "${{ secrets.KUBECONFIG_DEV }}" | base64 -d > ~/.kube/config
      
      - name: Deploy Helm Chart
        run: |
          helm upgrade --install monitoring ./helm/monitoring \
            --namespace monitoring \
            --create-namespace \
            --values helm/monitoring/values-dev.yaml \
            --wait \
            --timeout 10m
      
      - name: Run Smoke Tests
        run: |
          kubectl wait --for=condition=ready pod \
            -l app.kubernetes.io/name=prometheus \
            -n monitoring \
            --timeout=300s
          
          kubectl wait --for=condition=ready pod \
            -l app.kubernetes.io/name=grafana \
            -n monitoring \
            --timeout=300s

  deploy-prod:
    name: Deploy to Production
    needs: validate
    if: github.ref == 'refs/heads/main'
    runs-on: ubuntu-latest
    environment: production
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      
      - name: Setup Helm
        uses: azure/setup-helm@v3
        with:
          version: ${{ env.HELM_VERSION }}
      
      - name: Configure kubeconfig
        run: |
          echo "${{ secrets.KUBECONFIG_PROD }}" | base64 -d > ~/.kube/config
      
      - name: Backup current state
        run: |
          helm get values monitoring -n monitoring > backup-values.yaml
          kubectl get configmap -n monitoring -o yaml > backup-configmaps.yaml
      
      - name: Deploy Helm Chart
        run: |
          helm upgrade --install monitoring ./helm/monitoring \
            --namespace monitoring \
            --create-namespace \
            --values helm/monitoring/values-prod.yaml \
            --wait \
            --timeout 15m
      
      - name: Verify Deployment
        run: |
          kubectl rollout status deployment/monitoring-grafana -n monitoring
          kubectl rollout status statefulset/prometheus-monitoring-kube-prometheus-prometheus -n monitoring
      
      - name: Notify Slack
        if: always()
        uses: 8398a7/action-slack@v3
        with:
          status: ${{ job.status }}
          text: 'Monitoring stack deployment to production: ${{ job.status }}'
          webhook_url: ${{ secrets.SLACK_WEBHOOK }}
```

6. **–°–æ–∑–¥–∞–π backup script**:

`scripts/backup-monitoring.sh`:
```bash
#!/bin/bash

set -e

# Configuration
BACKUP_DIR="/backup/monitoring"
RETENTION_DAYS=30
DATE=$(date +%Y%m%d-%H%M%S)
NAMESPACE="monitoring"

echo "Starting monitoring backup at $(date)"

# Create backup directory
mkdir -p "$BACKUP_DIR/$DATE"

# Backup Grafana
echo "Backing up Grafana..."
kubectl exec -n $NAMESPACE deployment/grafana -- \
  grafana-cli admin export-dashboard > "$BACKUP_DIR/$DATE/grafana-dashboards.json"

# Backup Grafana database
kubectl exec -n $NAMESPACE deployment/grafana -- \
  sqlite3 /var/lib/grafana/grafana.db .dump > "$BACKUP_DIR/$DATE/grafana-db.sql"

# Backup Prometheus config
echo "Backing up Prometheus configuration..."
kubectl get configmap -n $NAMESPACE prometheus-config -o yaml > \
  "$BACKUP_DIR/$DATE/prometheus-config.yaml"

# Backup Alert rules
kubectl get prometheusrule -n $NAMESPACE -o yaml > \
  "$BACKUP_DIR/$DATE/prometheus-rules.yaml"

# Backup Alertmanager config
kubectl get secret -n $NAMESPACE alertmanager-config -o yaml > \
  "$BACKUP_DIR/$DATE/alertmanager-config.yaml"

# Backup PVCs
echo "Backing up PVCs..."
kubectl get pvc -n $NAMESPACE -o yaml > "$BACKUP_DIR/$DATE/pvcs.yaml"

# Create tar archive
echo "Creating archive..."
tar -czf "$BACKUP_DIR/monitoring-backup-$DATE.tar.gz" -C "$BACKUP_DIR" "$DATE"

# Remove temporary directory
rm -rf "$BACKUP_DIR/$DATE"

# Cleanup old backups
echo "Cleaning up old backups..."
find "$BACKUP_DIR" -name "monitoring-backup-*.tar.gz" -mtime +$RETENTION_DAYS -delete

# Upload to S3 (optional)
if [ -n "$AWS_S3_BUCKET" ]; then
  echo "Uploading to S3..."
  aws s3 cp "$BACKUP_DIR/monitoring-backup-$DATE.tar.gz" \
    "s3://$AWS_S3_BUCKET/monitoring-backups/"
fi

echo "Backup completed successfully at $(date)"
echo "Backup location: $BACKUP_DIR/monitoring-backup-$DATE.tar.gz"
```

7. **–°–æ–∑–¥–∞–π validation tests**:

`tests/alerts-test.yml`:
```yaml
# Unit tests –¥–ª—è alert rules
rule_files:
  - ../config/prometheus/alerts/*.yml

evaluation_interval: 1m

tests:
  # Test HighCPUUsage alert
  - interval: 1m
    input_series:
      - series: 'node_cpu_seconds_total{mode="idle",instance="localhost:9100"}'
        values: '100+0x10'  # Idle CPU = 100 (–ø–æ—Å—Ç–æ—è–Ω–Ω–æ)
      
      - series: 'node_cpu_seconds_total{mode="system",instance="localhost:9100"}'
        values: '0+10x10'   # System CPU —Ä–∞—Å—Ç—ë—Ç
    
    alert_rule_test:
      - eval_time: 5m
        alertname: HighCPUUsage
        exp_alerts:
          - exp_labels:
              severity: warning
              instance: localhost:9100
            exp_annotations:
              summary: "High CPU usage on localhost:9100"
  
  # Test DiskSpaceCritical alert
  - interval: 1m
    input_series:
      - series: 'node_filesystem_size_bytes{mountpoint="/",instance="localhost:9100"}'
        values: '100000000000+0x10'  # 100GB total
      
      - series: 'node_filesystem_avail_bytes{mountpoint="/",instance="localhost:9100"}'
        values: '5000000000+0x10'    # 5GB available (95% used)
    
    alert_rule_test:
      - eval_time: 5m
        alertname: DiskSpaceCritical
        exp_alerts:
          - exp_labels:
              severity: critical
              instance: localhost:9100
              mountpoint: "/"
            exp_annotations:
              summary: "Critical disk space on localhost:9100"
  
  # Test no alert when metrics are normal
  - interval: 1m
    input_series:
      - series: 'node_cpu_seconds_total{mode="idle",instance="localhost:9100"}'
        values: '100+10x10'  # Normal idle CPU
    
    alert_rule_test:
      - eval_time: 10m
        alertname: HighCPUUsage
        exp_alerts: []  # No alerts expected
```

8. **–ó–∞–ø—É—Å–∫ –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ**:
```bash
# Terraform
cd terraform
terraform init
terraform plan -var="grafana_auth=admin:admin"
terraform apply -var="grafana_auth=admin:admin"

# Ansible
cd ansible
ansible-playbook -i inventory/hosts.ini playbooks/deploy-monitoring-stack.yml

# Helm (local test)
helm install monitoring ./helm/monitoring \
  --namespace monitoring \
  --create-namespace \
  --values helm/monitoring/values-dev.yaml \
  --dry-run --debug

# Real deployment
helm install monitoring ./helm/monitoring \
  --namespace monitoring \
  --create-namespace \
  --values helm/monitoring/values-prod.yaml

# Verify
kubectl get pods -n monitoring
helm list -n monitoring

# Run tests
promtool test rules tests/alerts-test.yml

# Backup
./scripts/backup-monitoring.sh
```

### üöÄ –ë–æ–Ω—É—Å (–Ω–æ–≤–æ–µ)

**1. Monitoring as Code with Jsonnet**:

`jsonnet/dashboards/application.jsonnet`:
```jsonnet
local grafana = import 'grafonnet/grafana.libsonnet';
local dashboard = grafana.dashboard;
local row = grafana.row;
local prometheus = grafana.prometheus;
local graphPanel = grafana.graphPanel;
local statPanel = grafana.statPanel;

dashboard.new(
  'Application Metrics',
  tags=['application', 'monitoring'],
  editable=true,
)
.addRow(
  row.new(title='Request Metrics')
  .addPanel(
    graphPanel.new(
      'Request Rate',
      datasource='Prometheus',
      format='reqps',
    )
    .addTarget(
      prometheus.target(
        'sum(rate(http_requests_total[5m])) by (service)',
        legendFormat='{{service}}',
      )
    )
  )
  .addPanel(
    statPanel.new(
      'Error Rate',
      datasource='Prometheus',
      unit='percentunit',
    )
    .addTarget(
      prometheus.target(
        'sum(rate(http_requests_total{status=~"5.."}[5m])) / sum(rate(http_requests_total[5m]))',
      )
    )
    .addThresholds([
      { value: 0, color: 'green' },
      { value: 0.01, color: 'yellow' },
      { value: 0.05, color: 'red' },
    ])
  )
)
```

–ö–æ–º–ø–∏–ª—è—Ü–∏—è:
```bash
jsonnet -J vendor dashboards/application.jsonnet > dashboards/application.json
```

**2. Monitoring Configuration Testing Framework**:

`tests/integration_test.py`:
```python
#!/usr/bin/env python3
"""
Integration tests –¥–ª—è monitoring stack
"""
import requests
import time
import pytest

PROMETHEUS_URL = "http://localhost:9090"
GRAFANA_URL = "http://localhost:3000"
ALERTMANAGER_URL = "http://localhost:9093"

class TestPrometheus:
    def test_prometheus_healthy(self):
        """Test Prometheus health"""
        response = requests.get(f"{PROMETHEUS_URL}/-/healthy")
        assert response.status_code == 200
    
    def test_prometheus_targets(self):
        """Test all targets are up"""
        response = requests.get(f"{PROMETHEUS_URL}/api/v1/targets")
        data = response.json()
        
        active_targets = data['data']['activeTargets']
        down_targets = [t for t in active_targets if t['health'] != 'up']
        
        assert len(down_targets) == 0, f"Down targets: {down_targets}"
    
    def test_prometheus_rules_loaded(self):
        """Test alert rules are loaded"""
        response = requests.get(f"{PROMETHEUS_URL}/api/v1/rules")
        data = response.json()
        
        groups = data['data']['groups']
        assert len(groups) > 0, "No alert rule groups found"
    
    def test_query_works(self):
        """Test PromQL queries work"""
        query = "up"
        response = requests.get(
            f"{PROMETHEUS_URL}/api/v1/query",
            params={'query': query}
        )
        data = response.json()
        
        assert data['status'] == 'success'
        assert len(data['data']['result']) > 0

class TestGrafana:
    def test_grafana_healthy(self):
        """Test Grafana health"""
        response = requests.get(f"{GRAFANA_URL}/api/health")
        assert response.status_code == 200
    
    def test_datasources_configured(self):
        """Test datasources are configured"""
        response = requests.get(
            f"{GRAFANA_URL}/api/datasources",
            auth=('admin', 'admin')
        )
        datasources = response.json()
        
        assert len(datasources) > 0, "No datasources configured"
        
        # Check Prometheus datasource
        prometheus_ds = [ds for ds in datasources if ds['type'] == 'prometheus']
        assert len(prometheus_ds) > 0, "Prometheus datasource not found"
    
    def test_dashboards_exist(self):
        """Test dashboards are provisioned"""
        response = requests.get(
            f"{GRAFANA_URL}/api/search",
            auth=('admin', 'admin')
        )
        dashboards = response.json()
        
        assert len(dashboards) > 0, "No dashboards found"

class TestAlertmanager:
    def test_alertmanager_healthy(self):
        """Test Alertmanager health"""
        response = requests.get(f"{ALERTMANAGER_URL}/-/healthy")
        assert response.status_code == 200
    
    def test_alertmanager_config(self):
        """Test Alertmanager configuration is valid"""
        response = requests.get(f"{ALERTMANAGER_URL}/api/v2/status")
        data = response.json()
        
        assert 'config' in data
        assert 'original' in data['config']

class TestEndToEnd:
    def test_alert_flow(self):
        """Test complete alert flow"""
        # 1. Trigger alert by sending bad metrics
        # 2. Wait for alert to fire
        # 3. Check alert in Alertmanager
        # 4. Verify notification sent
        
        # Wait for alert to evaluate
        time.sleep(60)
        
        # Check for firing alerts
        response = requests.get(f"{ALERTMANAGER_URL}/api/v2/alerts")
        alerts = response.json()
        
        # Should have some alerts
        assert isinstance(alerts, list)

if __name__ == "__main__":
    pytest.main([__file__, "-v"])
```

**3. Automated Dashboard Sync**:

`scripts/sync-dashboards.py`:
```python
#!/usr/bin/env python3
"""
Sync Grafana dashboards between environments
"""
import requests
import json
import os
from pathlib import Path

class GrafanaDashboardSync:
    def __init__(self, source_url, target_url, api_key):
        self.source_url = source_url
        self.target_url = target_url
        self.headers = {'Authorization': f'Bearer {api_key}'}
    
    def export_dashboards(self, output_dir):
        """Export all dashboards from source"""
        # Search all dashboards
        response = requests.get(
            f"{self.source_url}/api/search?type=dash-db",
            headers=self.headers
        )
        dashboards = response.json()
        
        Path(output_dir).mkdir(parents=True, exist_ok=True)
        
        for dash in dashboards:
            uid = dash['uid']
            title = dash['title']
            
            # Get dashboard JSON
            response = requests.get(
                f"{self.source_url}/api/dashboards/uid/{uid}",
                headers=self.headers
            )
            dashboard_json = response.json()
            
            # Save to file
            filename = f"{output_dir}/{uid}_{title.replace(' ', '_')}.json"
            with open(filename, 'w') as f:
                json.dump(dashboard_json, f, indent=2)
            
            print(f"Exported: {title}")
    
    def import_dashboards(self, input_dir):
        """Import dashboards to target"""
        for file_path in Path(input_dir).glob('*.json'):
            with open(file_path, 'r') as f:
                dashboard_data = json.load(f)
            
            # Prepare import payload
            payload = {
                'dashboard': dashboard_data['dashboard'],
                'folderId': 0,
                'overwrite': True
            }
            
            # Import dashboard
            response = requests.post(
                f"{self.target_url}/api/dashboards/db",
                headers=self.headers,
                json=payload
            )
            
            if response.status_code == 200:
                print(f"Imported: {file_path.name}")
            else:
                print(f"Failed to import {file_path.name}: {response.text}")

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser()
    parser.add_argument('--source', required=True)
    parser.add_argument('--target', required=True)
    parser.add_argument('--api-key', required=True)
    parser.add_argument('--export-dir', default='./dashboards')
    parser.add_argument('--action', choices=['export', 'import', 'sync'], required=True)
    
    args = parser.parse_args()
    
    sync = GrafanaDashboardSync(args.source, args.target, args.api_key)
    
    if args.action in ['export', 'sync']:
        sync.export_dashboards(args.export_dir)
    
    if args.action in ['import', 'sync']:
        sync.import_dashboards(args.export_dir)
```

**4. Cost Optimization –¥–ª—è Cloud Monitoring**:

`terraform/modules/cost-optimization/main.tf`:
```hcl
# Intelligent tiering –¥–ª—è Prometheus storage
resource "aws_s3_bucket" "prometheus_long_term" {
  bucket = "prometheus-long-term-storage"
  
  lifecycle_rule {
    enabled = true
    
    transition {
      days          = 30
      storage_class = "STANDARD_IA"
    }
    
    transition {
      days          = 90
      storage_class = "GLACIER"
    }
    
    expiration {
      days = 365
    }
  }
}

# Spot instances –¥–ª—è non-critical monitoring
resource "aws_autoscaling_group" "monitoring_workers" {
  name = "monitoring-workers"
  
  mixed_instances_policy {
    launch_template {
      launch_template_specification {
        launch_template_id = aws_launch_template.monitoring.id
      }
      
      override {
        instance_type = "t3.medium"
      }
      
      override {
        instance_type = "t3a.medium"
      }
    }
    
    instances_distribution {
      on_demand_base_capacity                  = 1
      on_demand_percentage_above_base_capacity = 0
      spot_allocation_strategy                 = "capacity-optimized"
    }
  }
}
```

---

## –ò—Ç–æ–≥–∏ –º–æ–¥—É–ª—è 8

–ü–æ—Å–ª–µ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏—è —ç—Ç–æ–≥–æ –º–æ–¥—É–ª—è —Ç—ã –¥–æ–ª–∂–µ–Ω —É–º–µ—Ç—å:

‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Terraform –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è Grafana
‚úÖ –°–æ–∑–¥–∞–≤–∞—Ç—å Ansible playbooks –¥–ª—è deployment monitoring
‚úÖ –†–∞–±–æ—Ç–∞—Ç—å —Å Helm charts –¥–ª—è Kubernetes
‚úÖ –ù–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å GitOps —Å ArgoCD
‚úÖ –ü–∏—Å–∞—Ç—å CI/CD pipelines –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
‚úÖ –°–æ–∑–¥–∞–≤–∞—Ç—å automated backups
‚úÖ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å monitoring –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
‚úÖ –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å dashboards –º–µ–∂–¥—É –æ–∫—Ä—É–∂–µ–Ω–∏—è–º–∏
‚úÖ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∑–∞—Ç—Ä–∞—Ç—ã –Ω–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥
‚úÖ Version control –≤—Å–µ–π monitoring –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã

**–ö–ª—é—á–µ–≤—ã–µ –ø—Ä–∏–Ω—Ü–∏–ø—ã:**
1. –í—Å—è –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –≤ Git
2. –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è deployment
3. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–µ—Ä–µ–¥ production
4. –†–µ–≥—É–ª—è—Ä–Ω—ã–µ backups
5. Environment parity (dev/staging/prod –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ)
6. –î–æ–∫—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏–π
7. Cost optimization



## –ú–æ–¥—É–ª—å 11: Kubernetes Monitoring - –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤ –∏ –æ—Ä–∫–µ—Å—Ç—Ä–∞—Ü–∏–∏ (45 –º–∏–Ω—É—Ç)

### üéØ –ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞

**Kubernetes –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ           Control Plane                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îÇ
‚îÇ  ‚îÇ   API    ‚îÇ  ‚îÇ  etcd    ‚îÇ            ‚îÇ
‚îÇ  ‚îÇ  Server  ‚îÇ  ‚îÇ          ‚îÇ            ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îÇ
‚îÇ  ‚îÇScheduler ‚îÇ  ‚îÇController‚îÇ            ‚îÇ
‚îÇ  ‚îÇ          ‚îÇ  ‚îÇ Manager  ‚îÇ            ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
              ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ                   ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Node 1 ‚îÇ         ‚îÇ Node 2 ‚îÇ
‚îÇ        ‚îÇ         ‚îÇ        ‚îÇ
‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ         ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ ‚îÇPod ‚îÇ ‚îÇ         ‚îÇ ‚îÇPod ‚îÇ ‚îÇ
‚îÇ ‚îÇ    ‚îÇ ‚îÇ         ‚îÇ ‚îÇ    ‚îÇ ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ         ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ         ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ ‚îÇPod ‚îÇ ‚îÇ         ‚îÇ ‚îÇPod ‚îÇ ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ         ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ        ‚îÇ         ‚îÇ        ‚îÇ
‚îÇkubelet ‚îÇ         ‚îÇkubelet ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**–£—Ä–æ–≤–Ω–∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ K8s:**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Application Level                   ‚îÇ  - –ë–∏–∑–Ω–µ—Å –º–µ—Ç—Ä–∏–∫–∏
‚îÇ  (–≤–∞—à–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ)                   ‚îÇ  - Custom metrics
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Container Level                     ‚îÇ  - CPU, Memory, Network
‚îÇ  (Docker/containerd)                 ‚îÇ  - Restart count
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Pod Level                           ‚îÇ  - Pod status
‚îÇ  (K8s workload)                      ‚îÇ  - Resource limits
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Node Level                          ‚îÇ  - Node resources
‚îÇ  (Worker nodes)                      ‚îÇ  - Disk, Network
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Cluster Level                       ‚îÇ  - API server health
‚îÇ  (Control plane)                     ‚îÇ  - etcd, scheduler
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**–ö–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ K8s:**

**Cluster metrics:**

```
- –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ nodes
- Nodes ready/not ready
- Total CPU/Memory capacity
- Total CPU/Memory usage
- API server request rate
- API server latency
- etcd latency
- Scheduler latency
```

**Node metrics:**

```
- CPU usage/limits
- Memory usage/limits
- Disk usage/IOPS
- Network traffic
- Pod count per node
- Node conditions (Ready, DiskPressure, MemoryPressure)
```

**Pod metrics:**

```
- CPU usage/requests/limits
- Memory usage/requests/limits
- Restart count
- Pod phase (Pending, Running, Failed, Succeeded)
- Container state
- Network I/O
```

**Container metrics:**

```
- CPU usage
- Memory usage (RSS, cache, swap)
- Disk I/O
- Network I/O
- OOM kills
```

**–í–∞–∂–Ω—ã–µ K8s —Å–æ—Å—Ç–æ—è–Ω–∏—è:**

```
Pod Phases:
- Pending     - –ñ–¥–µ—Ç scheduling
- Running     - –ó–∞–ø—É—â–µ–Ω –Ω–∞ node
- Succeeded   - –í—Å–µ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–∏–ª–∏—Å—å
- Failed      - –•–æ—Ç—è –±—ã –æ–¥–∏–Ω –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä failed
- Unknown     - –°–æ—Å—Ç–æ—è–Ω–∏–µ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ

Container States:
- Waiting     - –ñ–¥–µ—Ç –∑–∞–ø—É—Å–∫–∞
- Running     - –í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è
- Terminated  - –ó–∞–≤–µ—Ä—à–µ–Ω

Node Conditions:
- Ready              - Node –≥–æ—Ç–æ–≤ –ø—Ä–∏–Ω–∏–º–∞—Ç—å pods
- MemoryPressure     - –ú–∞–ª–æ –ø–∞–º—è—Ç–∏
- DiskPressure       - –ú–∞–ª–æ –º–µ—Å—Ç–∞ –Ω–∞ –¥–∏—Å–∫–µ
- PIDPressure        - –ú–Ω–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤
- NetworkUnavailable - –ü—Ä–æ–±–ª–µ–º—ã —Å —Å–µ—Ç—å—é
```

**Prometheus –≤ Kubernetes:**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Prometheus Operator             ‚îÇ
‚îÇ                                         ‚îÇ
‚îÇ  –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä—É–µ—Ç:                        ‚îÇ
‚îÇ  - Deployment Prometheus                ‚îÇ
‚îÇ  - Service Discovery                    ‚îÇ
‚îÇ  - Scrape configuration                 ‚îÇ
‚îÇ  - Alert rules                          ‚îÇ
‚îÇ  - ServiceMonitor CRDs                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ      Prometheus Server(s)               ‚îÇ
‚îÇ                                         ‚îÇ
‚îÇ  –°–æ–±–∏—Ä–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ —Å:                    ‚îÇ
‚îÇ  - kubelet (cAdvisor)                   ‚îÇ
‚îÇ  - API server                           ‚îÇ
‚îÇ  - Node exporters                       ‚îÇ
‚îÇ  - Application pods                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Kube-state-metrics vs Metrics Server:**

```
Metrics Server:
- –ë–∞–∑–æ–≤—ã–µ CPU/Memory –º–µ—Ç—Ä–∏–∫–∏
- –î–ª—è Horizontal Pod Autoscaler (HPA)
- –î–ª—è kubectl top
- Real-time –¥–∞–Ω–Ω—ã–µ
- –ù–µ —Ö—Ä–∞–Ω–∏—Ç –∏—Å—Ç–æ—Ä–∏—é

Kube-state-metrics:
- –ú–µ—Ç—Ä–∏–∫–∏ –æ K8s –æ–±—ä–µ–∫—Ç–∞—Ö (Deployments, Pods, etc)
- –°–æ—Å—Ç–æ—è–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–∞
- –î–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∏ alerting
- –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç –≤ Prometheus —Ñ–æ—Ä–º–∞—Ç–µ
- –î–æ–ø–æ–ª–Ω—è–µ—Ç Metrics Server
```

**ServiceMonitor –∏ PodMonitor:**

yaml

```yaml
# ServiceMonitor - –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π scraping —á–µ—Ä–µ–∑ Service
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: my-app
  labels:
    team: backend
spec:
  selector:
    matchLabels:
      app: my-app
  endpoints:
  - port: metrics
    interval: 30s
    path: /metrics

# PodMonitor - –ø—Ä—è–º–æ–π scraping pods
apiVersion: monitoring.coreos.com/v1
kind: PodMonitor
metadata:
  name: my-app-pods
spec:
  selector:
    matchLabels:
      app: my-app
  podMetricsEndpoints:
  - port: metrics
    interval: 30s
```

**Resource Requests –∏ Limits:**

yaml

```yaml
resources:
  requests:
    cpu: 100m        # –ì–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ –ø–æ–ª—É—á–∏—Ç
    memory: 128Mi
  limits:
    cpu: 500m        # –ú–∞–∫—Å–∏–º—É–º –º–æ–∂–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å
    memory: 512Mi    # OOM kill –µ—Å–ª–∏ –ø—Ä–µ–≤—ã—Å–∏—Ç

QoS Classes:
1. Guaranteed  - requests == limits (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç highest)
2. Burstable   - requests < limits
3. BestEffort  - –Ω–µ—Ç requests/limits (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç lowest)
```

**Horizontal Pod Autoscaler (HPA):**

yaml

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: my-app-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: my-app
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70  # Target 70% CPU
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
    scaleDown:
      stabilizationWindowSeconds: 300
```

**Vertical Pod Autoscaler (VPA):**

yaml

```yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: my-app-vpa
spec:
  targetRef:
    apiVersion: "apps/v1"
    kind: Deployment
    name: my-app
  updatePolicy:
    updateMode: "Auto"  # Auto, Recreate, Initial, Off
  resourcePolicy:
    containerPolicies:
    - containerName: app
      minAllowed:
        cpu: 100m
        memory: 128Mi
      maxAllowed:
        cpu: 2
        memory: 2Gi
```

**–í–∞–∂–Ω—ã–µ PromQL –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è K8s:**

promql

````promql
# CPU
## Node CPU usage
100 - (avg by (instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)

## Pod CPU usage
sum(rate(container_cpu_usage_seconds_total{pod!=""}[5m])) by (pod, namespace)

## CPU throttling
rate(container_cpu_cfs_throttled_seconds_total[5m]) > 0

# Memory
## Pod memory usage
sum(container_memory_working_set_bytes{pod!=""}) by (pod, namespace)

## Memory usage vs limit
sum(container_memory_working_set_bytes{pod!=""}) by (pod)
/
sum(container_spec_memory_limit_bytes{pod!=""}) by (pod) * 100

## OOM kills
rate(container_oom_events_total[5m]) > 0

# Disk
## Disk usage per node
(1 - (node_filesystem_avail_bytes{mountpoint="/"} 
/ node_filesystem_size_bytes{mountpoint="/"})) * 100

## Pod disk I/O
rate(container_fs_reads_bytes_total[5m])
rate(container_fs_writes_bytes_total[5m])

# Network
## Pod network traffic
rate(container_network_receive_bytes_total[5m])
rate(container_network_transmit_bytes_total[5m])

## Network errors
rate(container_network_receive_errors_total[5m])
rate(container_network_transmit_errors_total[5m])

# Kubernetes objects
## Pods not ready
kube_pod_status_phase{phase!~"Running|Succeeded"} > 0

## Deployment replicas mismatch
kube_deployment_spec_replicas != kube_deployment_status_replicas_available

## Pod restarts
rate(kube_pod_container_status_restarts_total[15m]) > 0

## Failed pods
kube_pod_status_phase{phase="Failed"} > 0

## Pending pods (–¥–æ–ª–≥–æ)
kube_pod_status_phase{phase="Pending"} > 0

# Resources
## CPU requests vs limits
sum(kube_pod_container_resource_requests{resource="cpu"})
/
sum(kube_pod_container_resource_limits{resource="cpu"})

## Memory requests vs limits
sum(kube_pod_container_resource_requests{resource="memory"})
/
sum(kube_pod_container_resource_limits{resource="memory"})

## Node capacity vs allocatable
sum(kube_node_status_capacity{resource="cpu"})
sum(kube_node_status_allocatable{resource="cpu"})

# API Server
## Request rate
rate(apiserver_request_total[5m])

## Request latency
histogram_quantile(0.99, 
  rate(apiserver_request_duration_seconds_bucket[5m])
)

## Request errors
rate(apiserver_request_total{code=~"5.."}[5m])

# etcd
## Leader changes
rate(etcd_server_leader_changes_seen_total[5m])

## Proposal failures
rate(etcd_server_proposals_failed_total[5m])

## DB size
etcd_mvcc_db_total_size_in_bytes

# Scheduler
## Scheduling latency
histogram_quantile(0.99,
  rate(scheduler_scheduling_duration_seconds_bucket[5m])
)

## Pending pods in queue
scheduler_pending_pods

# HPA
## Current replicas vs desired
kube_horizontalpodautoscaler_status_current_replicas
vs
kube_horizontalpodautoscaler_status_desired_replicas

## HPA metric value
kube_horizontalpodautoscaler_status_current_metrics_value
````

**–õ—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏ K8s –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞:**
````
1. ‚úÖ –í—Å–µ–≥–¥–∞ —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–π resource requests/limits
2. ‚úÖ –ú–æ–Ω–∏—Ç–æ—Ä—å –≤—Å–µ —É—Ä–æ–≤–Ω–∏: cluster ‚Üí node ‚Üí pod ‚Üí container
3. ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–π ServiceMonitor –¥–ª—è auto-discovery
4. ‚úÖ –ù–∞—Å—Ç—Ä–æ–π alerting –Ω–∞ Pod restarts –∏ OOM kills
5. ‚úÖ –û—Ç—Å–ª–µ–∂–∏–≤–∞–π CPU throttling
6. ‚úÖ –ú–æ–Ω–∏—Ç–æ—Ä—å kube-state-metrics –¥–ª—è –æ–±—ä–µ–∫—Ç–æ–≤ K8s
7. ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–π HPA –¥–ª—è auto-scaling
8. ‚úÖ –ù–∞—Å—Ç—Ä–æ–π PodDisruptionBudget –¥–ª—è availability
9. ‚úÖ –ú–æ–Ω–∏—Ç–æ—Ä—å control plane –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
10. ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–π namespace –¥–ª—è –∏–∑–æ–ª—è—Ü–∏–∏ –∏ multi-tenancy
11. ‚úÖ –≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–π –º–µ—Ç—Ä–∏–∫–∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ /metrics endpoint
12. ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–π labels –¥–ª—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ –∏ filtering
````

**Namespace isolation:**

yaml

```yaml
# ResourceQuota - –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –Ω–∞ namespace
apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-quota
  namespace: production
spec:
  hard:
    requests.cpu: "10"
    requests.memory: 20Gi
    limits.cpu: "20"
    limits.memory: 40Gi
    persistentvolumeclaims: "10"
    pods: "50"

# LimitRange - default limits –¥–ª—è pods
apiVersion: v1
kind: LimitRange
metadata:
  name: default-limits
  namespace: production
spec:
  limits:
  - default:
      cpu: 500m
      memory: 512Mi
    defaultRequest:
      cpu: 100m
      memory: 128Mi
    type: Container
```

**PodDisruptionBudget (–¥–ª—è HA):**

yaml

```yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: my-app-pdb
spec:
  minAvailable: 2  # –∏–ª–∏ maxUnavailable: 1
  selector:
    matchLabels:
      app: my-app
```

**Liveness –∏ Readiness –ø—Ä–æbes:**

yaml

````yaml
livenessProbe:
  httpGet:
    path: /healthz
    port: 8080
  initialDelaySeconds: 30
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 3

readinessProbe:
  httpGet:
    path: /ready
    port: 8080
  initialDelaySeconds: 5
  periodSeconds: 5
  timeoutSeconds: 3
  failureThreshold: 2

# Startup probe (–¥–ª—è –º–µ–¥–ª–µ–Ω–Ω—ã—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π)
startupProbe:
  httpGet:
    path: /healthz
    port: 8080
  initialDelaySeconds: 0
  periodSeconds: 10
  failureThreshold: 30  # 300s total
````

**–¢–∏–ø–∏—á–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –∏ –∏—Ö –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞:**
```
–ü—Ä–æ–±–ª–µ–º–∞: Pod –ø–æ—Å—Ç–æ—è–Ω–Ω–æ restarts
–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞:
- kubectl describe pod <pod-name>
- kubectl logs <pod-name> --previous
- –ü—Ä–æ–≤–µ—Ä—å liveness probe
- –ü—Ä–æ–≤–µ—Ä—å OOM kills: kube_pod_container_status_terminated_reason{reason="OOMKilled"}

–ü—Ä–æ–±–ª–µ–º–∞: High CPU throttling
–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞:
- rate(container_cpu_cfs_throttled_seconds_total[5m])
- –£–≤–µ–ª–∏—á—å CPU limits –∏–ª–∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä—É–π –∫–æ–¥

–ü—Ä–æ–±–ª–µ–º–∞: Pod Pending –¥–æ–ª–≥–æ
–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞:
- kubectl describe pod <pod-name>
- –ü—Ä–æ–≤–µ—Ä—å Events
- –ü—Ä–∏—á–∏–Ω—ã: insufficient resources, node selector mismatch, PVC issues

–ü—Ä–æ–±–ª–µ–º–∞: High memory usage
–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞:
- container_memory_working_set_bytes
- –ü—Ä–æ–≤–µ—Ä—å memory leaks
- –ù–∞—Å—Ç—Ä–æ–π VPA –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏

–ü—Ä–æ–±–ª–µ–º–∞: Slow API requests
–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞:
- apiserver_request_duration_seconds
- –ü—Ä–æ–≤–µ—Ä—å etcd latency
- –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–π API server replicas
```

**Grafana dashboards –¥–ª—è K8s:**
````
–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ community dashboards:

1. Kubernetes Cluster Monitoring (315)
   - –û–±—â–∏–π –æ–±–∑–æ—Ä –∫–ª–∞—Å—Ç–µ—Ä–∞
   - Nodes, Pods, CPU, Memory

2. Kubernetes / Compute Resources / Cluster (7249)
   - Resource usage –ø–æ namespace
   - Requests vs Limits

3. Kubernetes / Compute Resources / Namespace (Pods) (7630)
   - –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø–æ pods

4. Node Exporter Full (1860)
   - –î–µ—Ç–∞–ª–∏ –ø–æ nodes

5. Kubernetes apiserver (12006)
   - API server metrics

–ò–º–ø–æ—Ä—Ç: Grafana ‚Üí Dashboards ‚Üí Import ‚Üí ID
````

### üíª –ó–∞–¥–∞–Ω–∏–µ

–ù–∞—Å—Ç—Ä–æ–π –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ Kubernetes –∫–ª–∞—Å—Ç–µ—Ä–∞:

1. **–°–æ–∑–¥–∞–π –ª–æ–∫–∞–ª—å–Ω—ã–π K8s –∫–ª–∞—Å—Ç–µ—Ä —Å kind**:

`kind-config.yaml`:

yaml

```yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
name: monitoring-cluster
nodes:
  - role: control-plane
    image: kindest/node:v1.29.0
    extraPortMappings:
      - containerPort: 30000
        hostPort: 9090
        protocol: TCP
      - containerPort: 30001
        hostPort: 3000
        protocol: TCP
      - containerPort: 30002
        hostPort: 16686
        protocol: TCP
  - role: worker
    image: kindest/node:v1.29.0
  - role: worker
    image: kindest/node:v1.29.0
```

–°–æ–∑–¥–∞–π –∫–ª–∞—Å—Ç–µ—Ä:

bash

```bash
# –£—Å—Ç–∞–Ω–æ–≤–∏ kind –µ—Å–ª–∏ –Ω–µ—Ç
# Mac: brew install kind
# Linux: curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.20.0/kind-linux-amd64

# –°–æ–∑–¥–∞–π –∫–ª–∞—Å—Ç–µ—Ä
kind create cluster --config kind-config.yaml

# –ü—Ä–æ–≤–µ—Ä—å
kubectl cluster-info
kubectl get nodes
```

2. **–£—Å—Ç–∞–Ω–æ–≤–∏ kube-prometheus-stack (Prometheus Operator)**:

bash

```bash
# –î–æ–±–∞–≤—å Helm repo
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update

# –°–æ–∑–¥–∞–π namespace
kubectl create namespace monitoring

# –£—Å—Ç–∞–Ω–æ–≤–∏ kube-prometheus-stack
helm install prometheus prometheus-community/kube-prometheus-stack \
  --namespace monitoring \
  --set prometheus.prometheusSpec.serviceMonitorSelectorNilUsesHelmValues=false \
  --set prometheus.service.type=NodePort \
  --set prometheus.service.nodePort=30000 \
  --set grafana.service.type=NodePort \
  --set grafana.service.nodePort=30001 \
  --set grafana.adminPassword=admin

# –ü—Ä–æ–≤–µ—Ä—å —É—Å—Ç–∞–Ω–æ–≤–∫—É
kubectl get pods -n monitoring
kubectl get svc -n monitoring
```

3. **–°–æ–∑–¥–∞–π demo –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏**:

`k8s-manifests/demo-app-deployment.yaml`:

yaml

````yaml
apiVersion: v1
kind: Namespace
metadata:
  name: demo-app

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: demo-app
  namespace: demo-app
  labels:
    app: demo-app
spec:
  replicas: 3
  selector:
    matchLabels:
      app: demo-app
  template:
    metadata:
      labels:
        app: demo-app
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8080"
        prometheus.io/path: "/metrics"
    spec:
      containers:
      - name: app
        image: quay.io/brancz/prometheus-example-app:v0.5.0
        ports:
        - containerPort: 8080
          name: metrics
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 500m
            memory: 512Mi
        livenessProbe:
          httpGet:
            path: /
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 5
        readinessProbe:
          httpGet:
            path: /
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 3

---
apiVersion: v1
kind: Service
metadata:
  name: demo-app
  namespace: demo-app
  labels:
    app: demo-app
spec:
  selector:
    app: demo-app
  ports:
  - port: 8080
    targetPort: 8080
    name: metrics
  type: ClusterIP

---
# ServiceMonitor –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ scraping
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: demo-app
  namespace: demo-app
  labels:
    app: demo-app
spec:
  selector:
    matchLabels:
      app: demo-app
  endpoints:
  - port: metrics
    interval: 30s
    path: /metrics

---
# HPA –¥–ª—è –∞–≤—Ç–æ–º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: demo-app-hpa
  namespace: demo-app
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: demo-app
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60

---
# PodDisruptionBudget –¥–ª—è HA
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: demo-app-pdb
  namespace: demo-app
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: demo-app

---
# ResourceQuota –¥–ª—è namespace
apiVersion: v1
kind: ResourceQuota
metadata:
  name: demo-app-quota
  namespace: demo-app
spec:
  hard:
    requests.cpu: "4"
    requests.memory: 8Gi
    limits.cpu: "8"
    limits.memory: 16Gi
    pods: "20"
````

–ü—Ä–∏–º–µ–Ω–∏:

bash

```bash
kubectl apply -f k8s-manifests/demo-app-deployment.yaml

# –ü—Ä–æ–≤–µ—Ä—å
kubectl get all -n demo-app
kubectl get servicemonitor -n demo-app
kubectl get hpa -n demo-app
```

4. **–°–æ–∑–¥–∞–π PrometheusRule –¥–ª—è alerting**:

`k8s-manifests/prometheus-rules.yaml`:

yaml

````yaml
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: kubernetes-alerts
  namespace: monitoring
  labels:
    prometheus: kube-prometheus
spec:
  groups:
  - name: kubernetes.rules
    interval: 30s
    rules:
    # Pod alerts
    - alert: PodCrashLooping
      expr: rate(kube_pod_container_status_restarts_total[15m]) > 0
      for: 5m
      labels:
        severity: critical
        component: pod
      annotations:
        summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is crash looping"
        description: "Pod has restarted {{ $value }} times in the last 15 minutes"
        dashboard: "http://localhost:3000/d/kubernetes-pods"

    - alert: PodNotReady
      expr: |
        sum by (namespace, pod) (
          kube_pod_status_phase{phase!~"Running|Succeeded"}
        ) > 0
      for: 10m
      labels:
        severity: warning
        component: pod
      annotations:
        summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} not ready"
        description: "Pod has been in {{ $labels.phase }} state for more than 10 minutes"

    - alert: PodOOMKilled
      expr: |
        sum by (namespace, pod) (
          rate(kube_pod_container_status_terminated_reason{reason="OOMKilled"}[5m])
        ) > 0
      for: 1m
      labels:
        severity: critical
        component: pod
      annotations:
        summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} OOMKilled"
        description: "Pod was killed due to out of memory"
        runbook: "Increase memory limits or fix memory leak"

    # Container alerts
    - alert: ContainerCPUThrottling
      expr: |
        rate(container_cpu_cfs_throttled_seconds_total[5m]) > 0.5
      for: 10m
      labels:
        severity: warning
        component: container
      annotations:
        summary: "Container {{ $labels.namespace }}/{{ $labels.pod }}/{{ $labels.container }} CPU throttling"
        description: "Container is being throttled {{ $value | humanizePercentage }}"
        runbook: "Increase CPU limits"

    - alert: ContainerHighMemoryUsage
      expr: |
        (
          sum by (namespace, pod, container) (container_memory_working_set_bytes)
          /
          sum by (namespace, pod, container) (container_spec_memory_limit_bytes)
        ) > 0.9
      for: 5m
      labels:
        severity: warning
        component: container
      annotations:
        summary: "Container {{ $labels.namespace }}/{{ $labels.pod }}/{{ $labels.container }} high memory"
        description: "Memory usage is {{ $value | humanizePercentage }}"

    # Deployment alerts
    - alert: DeploymentReplicasMismatch
      expr: |
        kube_deployment_spec_replicas != kube_deployment_status_replicas_available
      for: 10m
      labels:
        severity: warning
        component: deployment
      annotations:
        summary: "Deployment {{ $labels.namespace }}/{{ $labels.deployment }} replicas mismatch"
        description: "Desired: {{ $value }}, Available: {{ $labels.replicas_available }}"

    # Node alerts
    - alert: NodeNotReady
      expr: kube_node_status_condition{condition="Ready",status="true"} == 0
      for: 5m
      labels:
        severity: critical
        component: node
      annotations:
        summary: "Node {{ $labels.node }} not ready"
        description: "Node has been unready for more than 5 minutes"

    - alert: NodeHighCPUUsage
      expr: |
        100 - (avg by (instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
      for: 10m
      labels:
        severity: warning
        component: node
      annotations:
        summary: "Node {{ $labels.instance }} high CPU"
        description: "CPU usage is {{ $value | humanize }}%"

    - alert: NodeHighMemoryUsage
      expr: |
        (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 90
      for: 5m
      labels:
        severity: warning
        component: node
      annotations:
        summary: "Node {{ $labels.instance }} high memory"
        description: "Memory usage is {{ $value | humanize }}%"

    - alert: NodeDiskPressure
      expr: kube_node_status_condition{condition="DiskPressure",status="true"} == 1
      for: 5m
      labels:
        severity: critical
        component: node
      annotations:
        summary: "Node {{ $labels.node }} disk pressure"
        description: "Node is experiencing disk pressure"

    # HPA alerts
    - alert: HPAMaxedOut
      expr: |
        kube_horizontalpodautoscaler_status_current_replicas
        ==
        kube_horizontalpodautoscaler_spec_max_replicas
      for: 15m
      labels:
        severity: warning
        component: hpa
      annotations:
        summary: "HPA {{ $labels.namespace }}/{{ $labels.horizontalpodautoscaler }} maxed out"
        description: "HPA has been at max replicas ({{ $value }}) for 15 minutes"
        runbook: "Consider increasing max replicas"

    - alert: HPAScalingDisabled
      expr: |
        kube_horizontalpodautoscaler_status_condition{condition="ScalingActive",status="false"} == 1
      for: 5m
      labels:
        severity: warning
        component: hpa
      annotations:
        summary: "HPA {{ $labels.namespace }}/{{ $labels.horizontalpodautoscaler }} scaling disabled"
        description: "HPA is unable to compute metrics"

    # Control plane alerts
    - alert: APIServerHighLatency
      expr: |
        histogram_quantile(0.99,
          sum by (le) (rate(apiserver_request_duration_seconds_bucket[5m]))
        ) > 1
      for: 5m
      labels:
        severity: warning
        component: apiserver
      annotations:
        summary: "API Server high latency"
        description: "P99 latency is {{ $value }}s"

    - alert: APIServerErrorRate
      expr: |
        sum(rate(apiserver_request_total{code=~"5.."}[5m]))
        /
        sum(rate(apiserver_request_total[5m])) > 0.05
      for: 5m
      labels:
        severity: critical
        component: apiserver
      annotations:
        summary: "API Server high error rate"
        description: "Error rate is {{ $value | humanizePercentage }}"

    - alert: EtcdHighLatency
      expr: |
        histogram_quantile(0.99,
          rate(etcd_disk_wal_fsync_duration_seconds_bucket[5m])
        ) > 0.5
      for: 5m
      labels:
        severity: warning
        component: etcd
      annotations:
        summary: "etcd high latency"
        description: "P99 fsync latency is {{ $value }}s"

    # PersistentVolume alerts
    - alert: PersistentVolumeFillingUp
      expr: |
        (
          kubelet_volume_stats_available_bytes
          /
          kubelet_volume_stats_capacity_bytes
        ) < 0.1
      for: 5m
      labels:
        severity: warning
        component: pv
      annotations:
        summary: "PV {{ $labels.persistentvolumeclaim }} filling up"
        description: "Only {{ $value | humanizePercentage }} available"

````

–ü—Ä–∏–º–µ–Ω–∏:
```bash
kubectl apply -f k8s-manifests/prometheus-rules.yaml

# –ü—Ä–æ–≤–µ—Ä—å rules –≤ Prometheus
kubectl port-forward -n monitoring svc/prometheus-kube-prometheus-prometheus 9090:9090
# –û—Ç–∫—Ä–æ–π http://localhost:9090/rules
```

5. **–°–æ–∑–¥–∞–π load generator –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è**:

`k8s-manifests/load-generator.yaml`:
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: load-generator
  namespace: demo-app
spec:
  replicas: 1
  selector:
    matchLabels:
      app: load-generator
  template:
    metadata:
      labels:
        app: load-generator
    spec:
      containers:
      - name: load-generator
        image: busybox:latest
        command:
        - /bin/sh
        - -c
        - |
          while true; do
            # –ù–æ—Ä–º–∞–ª—å–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
            for i in $(seq 1 10); do
              wget -q -O- http://demo-app.demo-app.svc.cluster.local:8080/ > /dev/null 2>&1
              sleep 0.1
            done
            
            # –°–ª—É—á–∞–π–Ω—ã–µ –º–µ–¥–ª–µ–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã
            if [ $((RANDOM % 10)) -eq 0 ]; then
              echo "Generating slow request..."
              wget -q -O- http://demo-app.demo-app.svc.cluster.local:8080/?sleep=3 > /dev/null 2>&1
            fi
            
            sleep 1
          done
        resources:
          requests:
            cpu: 50m
            memory: 64Mi
          limits:
            cpu: 100m
            memory: 128Mi

---
# Job –¥–ª—è —Å—Ç—Ä–µ—Å—Å-—Ç–µ—Å—Ç–∞
apiVersion: batch/v1
kind: Job
metadata:
  name: stress-test
  namespace: demo-app
spec:
  parallelism: 5
  completions: 5
  template:
    spec:
      containers:
      - name: stress
        image: busybox:latest
        command:
        - /bin/sh
        - -c
        - |
          echo "Starting stress test..."
          for i in $(seq 1 100); do
            wget -q -O- http://demo-app.demo-app.svc.cluster.local:8080/ > /dev/null 2>&1 &
          done
          wait
          echo "Stress test complete"
      restartPolicy: Never
  backoffLimit: 4
```

–ü—Ä–∏–º–µ–Ω–∏:
```bash
kubectl apply -f k8s-manifests/load-generator.yaml

# –ó–∞–ø—É—Å—Ç–∏ —Å—Ç—Ä–µ—Å—Å-—Ç–µ—Å—Ç
kubectl apply -f k8s-manifests/load-generator.yaml

# –ù–∞–±–ª—é–¥–∞–π –∑–∞ HPA
watch kubectl get hpa -n demo-app

# –ü—Ä–æ–≤–µ—Ä—å pods
watch kubectl get pods -n demo-app
```

6. **–î–æ—Å—Ç—É–ø –∫ UI**:
```bash
# Prometheus
kubectl port-forward -n monitoring svc/prometheus-kube-prometheus-prometheus 9090:9090
# http://localhost:9090

# Grafana (admin/admin)
kubectl port-forward -n monitoring svc/prometheus-grafana 3000:80
# http://localhost:3000

# –ò–ª–∏ —á–µ—Ä–µ–∑ NodePort (–µ—Å–ª–∏ kind —Å portMapping)
# Prometheus: http://localhost:9090
# Grafana: http://localhost:3000
```

7. **–°–æ–∑–¥–∞–π custom Grafana dashboard**:

–°–æ—Ö—Ä–∞–Ω–∏ –∫–∞–∫ `k8s-dashboard.json` –∏ –∏–º–ø–æ—Ä—Ç–∏—Ä—É–π –≤ Grafana:
```json
{
  "dashboard": {
    "title": "Kubernetes Cluster Overview",
    "tags": ["kubernetes", "cluster"],
    "timezone": "browser",
    "panels": [
      {
        "id": 1,
        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 0},
        "type": "stat",
        "title": "Cluster Status",
        "targets": [
          {
            "expr": "sum(kube_node_status_condition{condition=\"Ready\",status=\"true\"})",
            "legendFormat": "Ready Nodes"
          },
          {
            "expr": "sum(kube_pod_status_phase{phase=\"Running\"})",
            "legendFormat": "Running Pods"
          }
        ]
      },
      {
        "id": 2,
        "gridPos": {"h": 8, "w": 12, "x": 12, "y": 0},
        "type": "timeseries",
        "title": "Cluster CPU Usage",
        "targets": [
          {
            "expr": "sum(rate(container_cpu_usage_seconds_total[5m])) by (namespace)",
            "legendFormat": "{{ namespace }}"
          }
        ]
      },
      {
        "id": 3,
        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 8},
        "type": "timeseries",
        "title": "Cluster Memory Usage",
        "targets": [
          {
            "expr": "sum(container_memory_working_set_bytes) by (namespace)",
            "legendFormat": "{{ namespace }}"
          }
        ]
      },
      {
        "id": 4,
        "gridPos": {"h": 8, "w": 12, "x": 12, "y": 8},
        "type": "table",
        "title": "Top Pods by CPU",
        "targets": [
          {
            "expr": "topk(10, sum(rate(container_cpu_usage_seconds_total[5m])) by (namespace, pod))",
            "format": "table",
            "instant": true
          }
        ]
      }
    ]
  }
}
```

8. **–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è**:
```bash
# –ü—Ä–æ–≤–µ—Ä—å –≤—Å–µ –º–µ—Ç—Ä–∏–∫–∏ —Å–æ–±–∏—Ä–∞—é—Ç—Å—è
kubectl exec -n monitoring prometheus-kube-prometheus-prometheus-0 -- \
  promtool query instant http://localhost:9090 'up'

# –ü—Ä–æ–≤–µ—Ä—å ServiceMonitor –æ–±–Ω–∞—Ä—É–∂–µ–Ω
kubectl get servicemonitor -A

# –ü—Ä–æ–≤–µ—Ä—å targets –≤ Prometheus
# http://localhost:9090/targets

# –ü—Ä–æ–≤–µ—Ä—å alerts
# http://localhost:9090/alerts

# –°–∏–º—É–ª–∏—Ä—É–π –ø—Ä–æ–±–ª–µ–º—ã
# OOMKill
kubectl run oom-test --image=polinux/stress --restart=Never -- \
  stress --vm 1 --vm-bytes 1G --timeout 10s

# CPU stress –¥–ª—è HPA
kubectl run cpu-stress --image=polinux/stress --restart=Never -- \
  stress --cpu 4 --timeout 60s

# –ù–∞–±–ª—é–¥–∞–π –∑–∞ scaling
watch kubectl get hpa -n demo-app
watch kubectl get pods -n demo-app
```

### üöÄ –ë–æ–Ω—É—Å (–Ω–æ–≤–æ–µ)

**1. –£—Å—Ç–∞–Ω–æ–≤–∏ Metrics Server –¥–ª—è kubectl top**:
```bash
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

# –î–ª—è kind –Ω—É–∂–µ–Ω –ø–∞—Ç—á (insecure TLS)
kubectl patch deployment metrics-server -n kube-system --type='json' \
  -p='[{"op": "add", "path": "/spec/template/spec/containers/0/args/-", "value": "--kubelet-insecure-tls"}]'

# –ü—Ä–æ–≤–µ—Ä—å
kubectl top nodes
kubectl top pods -A
```

**2. Vertical Pod Autoscaler**:
```bash
# –£—Å—Ç–∞–Ω–æ–≤–∏ VPA
git clone https://github.com/kubernetes/autoscaler.git
cd autoscaler/vertical-pod-autoscaler
./hack/vpa-up.sh

# –°–æ–∑–¥–∞–π VPA –¥–ª—è demo-app
cat <<EOF | kubectl apply -f -
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: demo-app-vpa
  namespace: demo-app
spec:
  targetRef:
    apiVersion: "apps/v1"
    kind: Deployment
    name: demo-app
  updatePolicy:
    updateMode: "Off"  # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –±–µ–∑ –∞–≤—Ç–æ–ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è
  resourcePolicy:
    containerPolicies:
    - containerName: app
      minAllowed:
        cpu: 50m
        memory: 64Mi
      maxAllowed:
        cpu: 1
        memory: 1Gi
EOF

# –ü—Ä–æ–≤–µ—Ä—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
kubectl describe vpa demo-app-vpa -n demo-app
```

**3. Kube-state-metrics custom metrics**:

–°–æ–∑–¥–∞–π ConfigMap —Å custom resource state metrics:
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: kube-state-metrics-customresourcestate-config
  namespace: monitoring
data:
  config.yaml: |
    kind: CustomResourceStateMetrics
    spec:
      resources:
        - groupVersionKind:
            group: "apps"
            version: "v1"
            kind: "Deployment"
          metricNamePrefix: "kube_deployment"
          metrics:
            - name: "replicas_custom"
              help: "Custom deployment replicas metric"
              each:
                type: Gauge
                gauge:
                  path: [spec, replicas]
```

**4. Cost monitoring —Å OpenCost**:
```bash
# –£—Å—Ç–∞–Ω–æ–≤–∏ OpenCost
helm install opencost opencost/opencost \
  --namespace opencost --create-namespace \
  --set prometheus.internal.enabled=false \
  --set prometheus.external.url=http://prometheus-kube-prometheus-prometheus.monitoring:9090

# Port-forward
kubectl port-forward -n opencost svc/opencost 9090:9090

# –û—Ç–∫—Ä–æ–π UI
# http://localhost:9090
```

**5. Cluster autoscaler (–¥–ª—è cloud)**:
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cluster-autoscaler
  namespace: kube-system
spec:
  replicas: 1
  selector:
    matchLabels:
      app: cluster-autoscaler
  template:
    metadata:
      labels:
        app: cluster-autoscaler
    spec:
      serviceAccountName: cluster-autoscaler
      containers:
      - image: k8s.gcr.io/autoscaling/cluster-autoscaler:v1.29.0
        name: cluster-autoscaler
        command:
        - ./cluster-autoscaler
        - --cloud-provider=aws  # –∏–ª–∏ gce, azure
        - --nodes=2:10:worker-nodes
        - --skip-nodes-with-local-storage=false
        - --expander=least-waste
        resources:
          limits:
            cpu: 100m
            memory: 300Mi
          requests:
            cpu: 100m
            memory: 300Mi
```

**6. Network Policy monitoring**:
```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: demo-app-netpol
  namespace: demo-app
spec:
  podSelector:
    matchLabels:
      app: demo-app
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: demo-app
    ports:
    - protocol: TCP
      port: 8080
  egress:
  - to:
    - namespaceSelector: {}
    ports:
    - protocol: TCP
      port: 53  # DNS
  - to:
    - namespaceSelector: {}
    ports:
    - protocol: TCP
      port: 443  # HTTPS
```

**7. –°–æ–∑–¥–∞–π script –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–æ–±–ª–µ–º**:

`k8s-troubleshoot.sh`:
```bash
#!/bin/bash

echo "=== Kubernetes Cluster Health Check ==="
echo ""

# Nodes
echo "üì¶ Nodes Status:"
kubectl get nodes -o wide
echo ""

echo "‚ö†Ô∏è  Not Ready Nodes:"
kubectl get nodes --field-selector spec.unschedulable=false | grep -v "Ready" || echo "All nodes ready"
echo ""

# Pods
echo "üî¥ Failed/Pending Pods:"
kubectl get pods -A --field-selector status.phase!=Running,status.phase!=Succeeded
echo ""

echo "üîÑ Restarting Pods (last hour):"
kubectl get pods -A -o json | jq -r '.items[] | select(.status.containerStatuses[]?.restartCount > 0) | "\(.metadata.namespace)/\(.metadata.name): \(.status.containerStatuses[0].restartCount) restarts"'
echo ""

# Resources
echo "üìä Top Resource Consumers:"
echo "CPU:"
kubectl top pods -A --sort-by=cpu | head -10
echo ""
echo "Memory:"
kubectl top pods -A --sort-by=memory | head -10
echo ""

# Events
echo "‚ö° Recent Events (errors):"
kubectl get events -A --sort-by='.lastTimestamp' | grep -i "error\|fail\|warning" | tail -20
echo ""

# HPA Status
echo "üìà HPA Status:"
kubectl get hpa -A
echo ""

# PVC Status
echo "üíæ PVC Status:"
kubectl get pvc -A
echo ""

echo "=== Health Check Complete ==="
```

**8. Monitoring Helm chart values –¥–ª—è production**:

`prometheus-values-prod.yaml`:
```yaml
prometheus:
  prometheusSpec:
    retention: 30d
    retentionSize: "50GB"
    storageSpec:
      volumeClaimTemplate:
        spec:
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 100Gi
    resources:
      requests:
        cpu: 1
        memory: 2Gi
      limits:
        cpu: 2
        memory: 4Gi
    
    # High availability
    replicas: 2
    
    # Remote write –¥–ª—è long-term storage
    remoteWrite:
    - url: "http://thanos-receive:19291/api/v1/receive"
    
    # Service monitors
    serviceMonitorSelectorNilUsesHelmValues: false
    podMonitorSelectorNilUsesHelmValues: false

alertmanager:
  alertmanagerSpec:
    replicas: 3
    storage:
      volumeClaimTemplate:
        spec:
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 10Gi

grafana:
  replicas: 2
  persistence:
    enabled: true
    size: 10Gi
  
  # SSO integration
  grafana.ini:
    auth.generic_oauth:
      enabled: true
      name: OAuth
      allow_sign_up: true
      client_id: your-client-id
      client_secret: your-client-secret
      scopes: openid profile email
      auth_url: https://auth.example.com/authorize
      token_url: https://auth.example.com/token
      api_url: https://auth.example.com/userinfo

# Node exporter –Ω–∞ –≤—Å–µ—Ö nodes
prometheus-node-exporter:
  tolerations:
  - effect: NoSchedule
    operator: Exists

# Kube-state-metrics
kube-state-metrics:
  replicas: 2
  resources:
    requests:
      cpu: 100m
      memory: 256Mi
    limits:
      cpu: 200m
      memory: 512Mi
```

---

## –ò—Ç–æ–≥–∏ –º–æ–¥—É–ª—è 9

–ü–æ—Å–ª–µ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏—è —ç—Ç–æ–≥–æ –º–æ–¥—É–ª—è —Ç—ã –¥–æ–ª–∂–µ–Ω —É–º–µ—Ç—å:

‚úÖ –ü–æ–Ω–∏–º–∞—Ç—å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É Kubernetes –∏ —É—Ä–æ–≤–Ω–∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
‚úÖ –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å –∏ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å kube-prometheus-stack
‚úÖ –°–æ–∑–¥–∞–≤–∞—Ç—å ServiceMonitor –¥–ª—è auto-discovery
‚úÖ –ü–∏—Å–∞—Ç—å PrometheusRule –¥–ª—è K8s alerting
‚úÖ –ù–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å HPA –∏ VPA –¥–ª—è autoscaling
‚úÖ –ú–æ–Ω–∏—Ç–æ—Ä–∏—Ç—å control plane –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
‚úÖ –ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å pod restarts, OOM kills, CPU throttling
‚úÖ –°–æ–∑–¥–∞–≤–∞—Ç—å Grafana dashboards –¥–ª—è K8s
‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å kubectl top –∏ Metrics Server
‚úÖ –ù–∞—Å—Ç—Ä–∞–∏–≤–∞—Ç—å ResourceQuota –∏ LimitRange
‚úÖ Troubleshooting –ø—Ä–æ–±–ª–µ–º –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ
‚úÖ –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å cost monitoring

**–ö–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ K8s:**

**Cluster:** Nodes ready, API latency, etcd health 
**Nodes:** CPU/Memory usage, disk pressure 
**Pods:** Restarts, OOM kills, phase 
**Workload:** Replicas mismatch, HPA status 
**Network:** Traffic, errors, latency


**Production checklist:**
- ‚úÖ –ù–∞—Å—Ç—Ä–æ–µ–Ω—ã resource requests/limits –¥–ª—è –≤—Å–µ—Ö pods
- ‚úÖ HPA –¥–ª—è –∫—Ä–∏—Ç–∏—á–Ω—ã—Ö —Å–µ—Ä–≤–∏—Å–æ–≤
- ‚úÖ PodDisruptionBudget –¥–ª—è HA
- ‚úÖ Liveness/Readiness probes
- ‚úÖ Monitoring –≤—Å–µ—Ö —É—Ä–æ–≤–Ω–µ–π (cluster ‚Üí node ‚Üí pod ‚Üí container)
- ‚úÖ Alerting –Ω–∞ –∫—Ä–∏—Ç–∏—á–Ω—ã–µ —Å–æ–±—ã—Ç–∏—è
- ‚úÖ Grafana dashboards –¥–ª—è –≤—Å–µ–π –∫–æ–º–∞–Ω–¥—ã
- ‚úÖ ServiceMonitor –¥–ª—è –≤—Å–µ—Ö –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π
- ‚úÖ ResourceQuota –¥–ª—è namespaces
- ‚úÖ Network policies –¥–ª—è security
- ‚úÖ Regular backup etcd
- ‚úÖ Cost tracking –∏ optimization


## –ú–æ–¥—É–ª—å 12: –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã –∏ –æ–±–ª–∞—á–Ω—ã—Ö —Å–µ—Ä–≤–∏—Å–æ–≤ (30 –º–∏–Ω—É—Ç)

### üéØ –ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞

**–£—Ä–æ–≤–Ω–∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã:**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Cloud Services (AWS/GCP/Azure)      ‚îÇ
‚îÇ - EC2, S3, RDS, Lambda              ‚îÇ
‚îÇ - Billing, Quotas, API limits       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Kubernetes / Orchestration          ‚îÇ
‚îÇ - Cluster health                    ‚îÇ
‚îÇ - Pod/Node metrics                  ‚îÇ
‚îÇ - Resource quotas                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Containers (Docker)                 ‚îÇ
‚îÇ - Container metrics                 ‚îÇ
‚îÇ - Image vulnerabilities             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Infrastructure (VMs, Bare Metal)    ‚îÇ
‚îÇ - CPU, Memory, Disk, Network        ‚îÇ
‚îÇ - Hardware health                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Network                             ‚îÇ
‚îÇ - Switches, Routers                 ‚îÇ
‚îÇ - Bandwidth, Latency                ‚îÇ
‚îÇ - SNMP, NetFlow                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Kubernetes –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ - –∫–ª—é—á–µ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:**

yaml

```yaml
kube-state-metrics:
  # –ú–µ—Ç—Ä–∏–∫–∏ –æ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ K8s –æ–±—ä–µ–∫—Ç–æ–≤
  - Deployments: replicas, available, unavailable
  - Pods: phase, restarts, conditions
  - Nodes: capacity, allocatable, conditions
  - PersistentVolumes: phase, capacity
  - Jobs: succeeded, failed, active

metrics-server:
  # –†–µ–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Ä–µ—Å—É—Ä—Å–æ–≤
  - CPU usage (–ø–æ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞–º/–ø–æ–¥–∞–º/–Ω–æ–¥–∞–º)
  - Memory usage
  - –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è HPA

cAdvisor:
  # Container-level –º–µ—Ç—Ä–∏–∫–∏
  - CPU/Memory usage
  - Network I/O
  - Filesystem I/O
  - –í—Å—Ç—Ä–æ–µ–Ω –≤ kubelet
```

**Cloud Provider –º–µ—Ç—Ä–∏–∫–∏:**

yaml

````yaml
AWS CloudWatch:
  - EC2: CPU, Network, Disk I/O
  - RDS: Connections, IOPS, Storage
  - S3: Requests, Bandwidth, Storage
  - Lambda: Invocations, Duration, Errors
  - ELB: Request Count, Latency, HTTP codes
  - Billing: Estimated charges

GCP Monitoring:
  - Compute Engine: CPU, Disk, Network
  - Cloud SQL: Queries, Connections
  - Cloud Storage: Operations, Bandwidth
  - Cloud Functions: Executions, Memory
  - Load Balancer: Request rate, Latency

Azure Monitor:
  - Virtual Machines: CPU, Memory, Disk
  - SQL Database: DTU, Storage, Connections
  - Storage: Transactions, Ingress/Egress
  - Functions: Execution count, Duration
  - Application Insights: APM –º–µ—Ç—Ä–∏–∫–∏
````

**SNMP –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ (Network devices):**
````
SNMP OIDs (Object Identifiers):
- .1.3.6.1.2.1.1.1.0        # System description
- .1.3.6.1.2.1.1.3.0        # Uptime
- .1.3.6.1.2.1.2.2.1.10.*   # Interface inbound octets
- .1.3.6.1.2.1.2.2.1.16.*   # Interface outbound octets
- .1.3.6.1.2.1.25.1.1.0     # Host CPU load

SNMP Versions:
v1: Basic, no encryption
v2c: Community strings, better performance
v3: Authentication + Encryption (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)
````

**Cost Monitoring:**

yaml

```yaml
Cloud Cost Metrics:
  - Daily/Monthly spend by service
  - Cost per application/team
  - Unutilized resources
  - Reserved vs On-Demand usage
  - Savings opportunities

Infrastructure Cost:
  - Compute: Instance types, utilization
  - Storage: Type, size, IOPS
  - Network: Data transfer, NAT gateways
  - Managed Services: RDS, Lambda, etc.
```

### üíª –ó–∞–¥–∞–Ω–∏–µ

–ù–∞—Å—Ç—Ä–æ–π –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ Kubernetes –∫–ª–∞—Å—Ç–µ—Ä–∞:

1. **–£—Å—Ç–∞–Ω–æ–≤–∏ kube-state-metrics**:

yaml

```yaml
# kube-state-metrics-deployment.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: kube-state-metrics
  namespace: monitoring
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: kube-state-metrics
rules:
  - apiGroups: [""]
    resources:
      - configmaps
      - secrets
      - nodes
      - pods
      - services
      - resourcequotas
      - replicationcontrollers
      - limitranges
      - persistentvolumeclaims
      - persistentvolumes
      - namespaces
      - endpoints
    verbs: ["list", "watch"]
  - apiGroups: ["apps"]
    resources:
      - statefulsets
      - daemonsets
      - deployments
      - replicasets
    verbs: ["list", "watch"]
  - apiGroups: ["batch"]
    resources:
      - cronjobs
      - jobs
    verbs: ["list", "watch"]
  - apiGroups: ["autoscaling"]
    resources:
      - horizontalpodautoscalers
    verbs: ["list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: kube-state-metrics
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: kube-state-metrics
subjects:
  - kind: ServiceAccount
    name: kube-state-metrics
    namespace: monitoring
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kube-state-metrics
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kube-state-metrics
  template:
    metadata:
      labels:
        app: kube-state-metrics
    spec:
      serviceAccountName: kube-state-metrics
      containers:
        - name: kube-state-metrics
          image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.10.1
          ports:
            - name: http-metrics
              containerPort: 8080
            - name: telemetry
              containerPort: 8081
          livenessProbe:
            httpGet:
              path: /healthz
              port: 8080
            initialDelaySeconds: 5
            timeoutSeconds: 5
          readinessProbe:
            httpGet:
              path: /
              port: 8081
            initialDelaySeconds: 5
            timeoutSeconds: 5
          resources:
            requests:
              cpu: 100m
              memory: 128Mi
            limits:
              cpu: 200m
              memory: 256Mi
---
apiVersion: v1
kind: Service
metadata:
  name: kube-state-metrics
  namespace: monitoring
  labels:
    app: kube-state-metrics
spec:
  ports:
    - name: http-metrics
      port: 8080
      targetPort: http-metrics
    - name: telemetry
      port: 8081
      targetPort: telemetry
  selector:
    app: kube-state-metrics
```

–ü—Ä–∏–º–µ–Ω–∏:

bash

```bash
kubectl create namespace monitoring
kubectl apply -f kube-state-metrics-deployment.yaml
```

2. **–ù–∞—Å—Ç—Ä–æ–π Prometheus –¥–ª—è Kubernetes –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞**:

–û–±–Ω–æ–≤–∏ `prometheus.yml`:

yaml

```yaml
scrape_configs:
  # ... —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ jobs

  # Kubernetes API server
  - job_name: 'kubernetes-apiservers'
    kubernetes_sd_configs:
      - role: endpoints
    scheme: https
    tls_config:
      ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
    bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
    relabel_configs:
      - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
        action: keep
        regex: default;kubernetes;https

  # Kubernetes nodes
  - job_name: 'kubernetes-nodes'
    kubernetes_sd_configs:
      - role: node
    scheme: https
    tls_config:
      ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
    bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
    relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)

  # Kubernetes nodes (Kubelet)
  - job_name: 'kubernetes-nodes-kubelet'
    kubernetes_sd_configs:
      - role: node
    scheme: https
    tls_config:
      ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
    bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
    relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
      - target_label: __address__
        replacement: kubernetes.default.svc:443
      - source_labels: [__meta_kubernetes_node_name]
        regex: (.+)
        target_label: __metrics_path__
        replacement: /api/v1/nodes/${1}/proxy/metrics

  # Kubernetes pods
  - job_name: 'kubernetes-pods'
    kubernetes_sd_configs:
      - role: pod
    relabel_configs:
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        action: keep
        regex: true
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
        action: replace
        target_label: __metrics_path__
        regex: (.+)
      - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
        action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
        target_label: __address__
      - action: labelmap
        regex: __meta_kubernetes_pod_label_(.+)
      - source_labels: [__meta_kubernetes_namespace]
        action: replace
        target_label: kubernetes_namespace
      - source_labels: [__meta_kubernetes_pod_name]
        action: replace
        target_label: kubernetes_pod_name

  # kube-state-metrics
  - job_name: 'kube-state-metrics'
    static_configs:
      - targets: ['kube-state-metrics.monitoring.svc.cluster.local:8080']

  # cAdvisor (–≤—Å—Ç—Ä–æ–µ–Ω –≤ kubelet)
  - job_name: 'kubernetes-cadvisor'
    kubernetes_sd_configs:
      - role: node
    scheme: https
    tls_config:
      ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
    bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
    relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
      - target_label: __address__
        replacement: kubernetes.default.svc:443
      - source_labels: [__meta_kubernetes_node_name]
        regex: (.+)
        target_label: __metrics_path__
        replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor
```

3. **–°–æ–∑–¥–∞–π Kubernetes-specific –∞–ª–µ—Ä—Ç—ã**:

yaml

```yaml
# kubernetes_alerts.yml
groups:
  - name: kubernetes_cluster
    rules:
      # –ù–æ–¥–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞
      - alert: KubernetesNodeNotReady
        expr: kube_node_status_condition{condition="Ready",status="true"} == 0
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Kubernetes node not ready"
          description: "Node {{ $labels.node }} has been unready for more than 5 minutes"

      # –í—ã—Å–æ–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ CPU –Ω–∞ –Ω–æ–¥–µ
      - alert: KubernetesNodeHighCPU
        expr: |
          (1 - avg(rate(node_cpu_seconds_total{mode="idle"}[5m])) by (instance)) * 100 > 80
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: "High CPU on node {{ $labels.instance }}"
          description: "CPU usage is {{ $value }}%"

      # –í—ã—Å–æ–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –Ω–∞ –Ω–æ–¥–µ
      - alert: KubernetesNodeHighMemory
        expr: |
          (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 85
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High memory on node {{ $labels.instance }}"
          description: "Memory usage is {{ $value }}%"

      # Pod –≤ CrashLoopBackOff
      - alert: KubernetesPodCrashLooping
        expr: |
          rate(kube_pod_container_status_restarts_total[15m]) > 0
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} crash looping"
          description: "Pod is restarting frequently"

      # Pod –Ω–µ –º–æ–∂–µ—Ç –∑–∞–ø—É—Å—Ç–∏—Ç—å—Å—è
      - alert: KubernetesPodNotReady
        expr: |
          sum by (namespace, pod) (kube_pod_status_phase{phase=~"Pending|Unknown"}) > 0
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} not ready"
          description: "Pod has been in {{ $labels.phase }} state for more than 10 minutes"

      # Deployment replicas mismatch
      - alert: KubernetesDeploymentReplicasMismatch
        expr: |
          kube_deployment_spec_replicas != kube_deployment_status_replicas_available
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Deployment {{ $labels.namespace }}/{{ $labels.deployment }} replicas mismatch"
          description: "Desired: {{ $value }}, Available: {{ $labels.replicas_available }}"

      # StatefulSet replicas mismatch
      - alert: KubernetesStatefulSetReplicasMismatch
        expr: |
          kube_statefulset_status_replicas_ready != kube_statefulset_status_replicas
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "StatefulSet {{ $labels.namespace }}/{{ $labels.statefulset }} replicas mismatch"

      # DaemonSet pods –Ω–µ –Ω–∞ –≤—Å–µ—Ö –Ω–æ–¥–∞—Ö
      - alert: KubernetesDaemonSetRolloutStuck
        expr: |
          kube_daemonset_status_number_ready / kube_daemonset_status_desired_number_scheduled * 100 < 100
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: "DaemonSet {{ $labels.namespace }}/{{ $labels.daemonset }} rollout stuck"

      # Job failed
      - alert: KubernetesJobFailed
        expr: |
          kube_job_status_failed > 0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Job {{ $labels.namespace }}/{{ $labels.job_name }} failed"

      # PVC pending
      - alert: KubernetesPersistentVolumeClaimPending
        expr: |
          kube_persistentvolumeclaim_status_phase{phase="Pending"} == 1
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "PVC {{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} pending"

      # Container OOMKilled
      - alert: KubernetesContainerOOMKilled
        expr: |
          (kube_pod_container_status_restarts_total - kube_pod_container_status_restarts_total offset 10m >= 1)
          and ignoring (reason) min_over_time(kube_pod_container_status_last_terminated_reason{reason="OOMKilled"}[10m]) == 1
        labels:
          severity: warning
        annotations:
          summary: "Container OOMKilled in {{ $labels.namespace }}/{{ $labels.pod }}"
          description: "Container {{ $labels.container }} was OOMKilled"

  - name: kubernetes_resources
    rules:
      # –í—ã—Å–æ–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ CPU –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–º
      - alert: KubernetesContainerHighCPU
        expr: |
          sum(rate(container_cpu_usage_seconds_total{container!=""}[5m])) by (namespace, pod, container)
          / 
          sum(kube_pod_container_resource_limits{resource="cpu"}) by (namespace, pod, container)
          * 100 > 80
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High CPU usage in container"
          description: "Container {{ $labels.namespace }}/{{ $labels.pod }}/{{ $labels.container }} using {{ $value }}% of limit"

      # –í—ã—Å–æ–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–º
      - alert: KubernetesContainerHighMemory
        expr: |
          sum(container_memory_working_set_bytes{container!=""}) by (namespace, pod, container)
          /
          sum(kube_pod_container_resource_limits{resource="memory"}) by (namespace, pod, container)
          * 100 > 80
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage in container"
          description: "Container {{ $labels.namespace }}/{{ $labels.pod }}/{{ $labels.container }} using {{ $value }}% of limit"

      # –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ä–µ—Å—É—Ä—Å–æ–≤ –Ω–∞ –Ω–æ–¥–µ
      - alert: KubernetesNodeResourcePressure
        expr: |
          kube_node_status_condition{condition=~"MemoryPressure|DiskPressure|PIDPressure",status="true"} == 1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Node {{ $labels.node }} under {{ $labels.condition }}"
```

4. **–°–æ–∑–¥–∞–π –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã–π Kubernetes –¥–∞—à–±–æ—Ä–¥ –≤ Grafana**:

–ò–º–ø–æ—Ä—Ç–∏—Ä—É–π –≥–æ—Ç–æ–≤—ã–µ –¥–∞—à–±–æ—Ä–¥—ã:

- **Kubernetes Cluster Monitoring**: ID 7249
- **Kubernetes Pod Resources**: ID 6417
- **Node Exporter Full**: ID 1860

–ò–ª–∏ —Å–æ–∑–¥–∞–π —Å–≤–æ–π —Å –ø–∞–Ω–µ–ª—è–º–∏:

**Panel 1: Cluster Overview**

promql

```promql
# –í—Å–µ–≥–æ –Ω–æ–¥
count(kube_node_info)

# –ù–æ–¥ Ready
sum(kube_node_status_condition{condition="Ready",status="true"})

# –í—Å–µ–≥–æ Pods
count(kube_pod_info)

# Running Pods
count(kube_pod_status_phase{phase="Running"})
```

**Panel 2: Resource Usage**

promql

```promql
# CPU Requests vs Allocatable
sum(kube_pod_container_resource_requests{resource="cpu"}) 
/ 
sum(kube_node_status_allocatable{resource="cpu"}) * 100

# Memory Requests vs Allocatable
sum(kube_pod_container_resource_requests{resource="memory"}) 
/ 
sum(kube_node_status_allocatable{resource="memory"}) * 100
```

**Panel 3: Pod Status by Phase**

promql

```promql
sum(kube_pod_status_phase{phase="Running"})
sum(kube_pod_status_phase{phase="Pending"})
sum(kube_pod_status_phase{phase="Failed"})
```

**Panel 4: Top Pods by CPU**

promql

```promql
topk(10, 
  sum(rate(container_cpu_usage_seconds_total{container!=""}[5m])) by (namespace, pod)
)
```

**Panel 5: Top Pods by Memory**

promql

```promql
topk(10,
  sum(container_memory_working_set_bytes{container!=""}) by (namespace, pod)
)
```

**Panel 6: Network Traffic**

promql

```promql
# Inbound
sum(rate(container_network_receive_bytes_total[5m])) by (namespace, pod)

# Outbound
sum(rate(container_network_transmit_bytes_total[5m])) by (namespace, pod)
```

5. **–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞**:

bash

```bash
# –°–æ–∑–¥–∞–π —Ç–µ—Å—Ç–æ–≤–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ
kubectl create deployment test-app --image=nginx --replicas=3

# –ü–æ—Å–º–æ—Ç—Ä–∏ –º–µ—Ç—Ä–∏–∫–∏
kubectl port-forward -n monitoring svc/prometheus 9090:9090

# –û—Ç–∫—Ä–æ–π Prometheus
http://localhost:9090

# –ü–æ–ø—Ä–æ–±—É–π –∑–∞–ø—Ä–æ—Å—ã:
# - kube_deployment_status_replicas{deployment="test-app"}
# - kube_pod_info{created_by_name="test-app"}
# - sum(rate(container_cpu_usage_seconds_total{pod=~"test-app.*"}[5m]))

# –°–∏–º—É–ª–∏—Ä—É–π –ø—Ä–æ–±–ª–µ–º—É
kubectl scale deployment test-app --replicas=10
kubectl delete pod -l app=test-app --force --grace-period=0

# –°–º–æ—Ç—Ä–∏ –∞–ª–µ—Ä—Ç—ã –≤ Alertmanager
http://localhost:9093
```

### üöÄ –ë–æ–Ω—É—Å (–Ω–æ–≤–æ–µ)

**1. –ù–∞—Å—Ç—Ä–æ–π AWS CloudWatch Exporter** –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ AWS —Ä–µ—Å—É—Ä—Å–æ–≤:

yaml

```yaml
  cloudwatch-exporter:
    image: prom/cloudwatch-exporter:latest
    container_name: cloudwatch-exporter
    ports:
      - "9106:9106"
    volumes:
      - ./cloudwatch-exporter.yml:/config/config.yml
      - ~/.aws:/root/.aws:ro
    command:
      - '/bin/cloudwatch_exporter'
      - '/config/config.yml'
    restart: unless-stopped
```

`cloudwatch-exporter.yml`:

yaml

```yaml
region: us-east-1
metrics:
  # EC2 Instances
  - aws_namespace: AWS/EC2
    aws_metric_name: CPUUtilization
    aws_dimensions:
      - InstanceId
    aws_statistics:
      - Average
    period_seconds: 300
    range_seconds: 600

  - aws_namespace: AWS/EC2
    aws_metric_name: NetworkIn
    aws_dimensions:
      - InstanceId
    aws_statistics:
      - Sum
    period_seconds: 300

  # RDS
  - aws_namespace: AWS/RDS
    aws_metric_name: DatabaseConnections
    aws_dimensions:
      - DBInstanceIdentifier
    aws_statistics:
      - Average
    period_seconds: 300

  - aws_namespace: AWS/RDS
    aws_metric_name: ReadLatency
    aws_dimensions:
      - DBInstanceIdentifier
    aws_statistics:
      - Average
    period_seconds: 300

  # ELB
  - aws_namespace: AWS/ELB
    aws_metric_name: RequestCount
    aws_dimensions:
      - LoadBalancerName
    aws_statistics:
      - Sum
    period_seconds: 300

  - aws_namespace: AWS/ELB
    aws_metric_name: Latency
    aws_dimensions:
      - LoadBalancerName
    aws_statistics:
      - Average
    period_seconds: 300

  # Lambda
  - aws_namespace: AWS/Lambda
    aws_metric_name: Invocations
    aws_dimensions:
      - FunctionName
    aws_statistics:
      - Sum
    period_seconds: 300

  - aws_namespace: AWS/Lambda
    aws_metric_name: Duration
    aws_dimensions:
      - FunctionName
    aws_statistics:
      - Average
    period_seconds: 300

  - aws_namespace: AWS/Lambda
    aws_metric_name: Errors
    aws_dimensions:
      - FunctionName
    aws_statistics:
      - Sum
    period_seconds: 300

  # S3
  - aws_namespace: AWS/S3
    aws_metric_name: NumberOfObjects
    aws_dimensions:
      - BucketName
      - StorageType
    aws_statistics:
      - Average
    period_seconds: 86400  # Once per day

  # Billing
  - aws_namespace: AWS/Billing
    aws_metric_name: EstimatedCharges
    aws_dimensions:
      - Currency
    aws_statistics:
      - Maximum
    period_seconds: 86400
```

–î–æ–±–∞–≤—å –≤ `prometheus.yml`:

yaml

```yaml
scrape_configs:
  - job_name: 'cloudwatch'
    static_configs:
      - targets: ['cloudwatch-exporter:9106']
```

**2. –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ Network —Å SNMP Exporter**:

yaml

```yaml
  snmp-exporter:
    image: prom/snmp-exporter:latest
    container_name: snmp-exporter
    ports:
      - "9116:9116"
    volumes:
      - ./snmp.yml:/etc/snmp_exporter/snmp.yml
    command:
      - '--config.file=/etc/snmp_exporter/snmp.yml'
    restart: unless-stopped
```

`snmp.yml` (–ø—Ä–∏–º–µ—Ä –¥–ª—è Cisco):

yaml

```yaml
auths:
  public_v2:
    community: public
    security_level: noAuthNoPriv
    auth_protocol: MD5
    priv_protocol: DES
    version: 2

modules:
  if_mib:
    walk:
      - 1.3.6.1.2.1.2.2.1.2   # ifDescr
      - 1.3.6.1.2.1.2.2.1.10  # ifInOctets
      - 1.3.6.1.2.1.2.2.1.16  # ifOutOctets
      - 1.3.6.1.2.1.2.2.1.8   # ifOperStatus
    
    lookups:
      - source_indexes: [ifIndex]
        lookup: ifDescr
    
    overrides:
      ifDescr:
        type: DisplayString
      ifOperStatus:
        type: gauge
```

Prometheus config:

yaml

```yaml
scrape_configs:
  - job_name: 'snmp'
    static_configs:
      - targets:
          - 192.168.1.1  # Switch IP
          - 192.168.1.2  # Router IP
    metrics_path: /snmp
    params:
      module: [if_mib]
      auth: [public_v2]
    relabel_configs:
      - source_labels: [__address__]
        target_label: __param_target
      - source_labels: [__param_target]
        target_label: instance
      - target_label: __address__
        replacement: snmp-exporter:9116
```

**3. Cost Monitoring Dashboard**:

–°–æ–∑–¥–∞–π –ø–∞–Ω–µ–ª–∏ –≤ Grafana:

promql

```promql
# AWS Estimated Charges (last value)
aws_billing_estimated_charges_maximum{currency="USD"}

# Daily cost trend
increase(aws_billing_estimated_charges_maximum{currency="USD"}[1d])

# Cost by service (requires detailed billing)
sum by (service) (aws_cloudwatch_billing_estimated_charges_average)

# Top 10 most expensive resources
topk(10, 
  sum by (resource_id) (aws_resource_cost_daily)
)

# Unutilized resources cost
sum(aws_ec2_cpu_utilization_average < 10) * avg(aws_ec2_pricing_hourly)

# Kubernetes cost by namespace
sum by (namespace) (
  avg_over_time(container_cpu_usage_seconds_total[1h]) * 
  scalar(aws_ec2_pricing_hourly / 8)  # assuming 8 vCPUs per instance
) * 24 * 30  # Monthly estimate
```

**4. Infrastructure as Code Monitoring**:

–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ Terraform state:

python

```python
# terraform_exporter.py
from prometheus_client import start_http_server, Gauge
import json
import subprocess
import time

# –ú–µ—Ç—Ä–∏–∫–∏
terraform_resource_count = Gauge('terraform_resource_count', 
                                 'Number of resources in Terraform state',
                                 ['workspace', 'type'])
terraform_drift_detected = Gauge('terraform_drift_detected',
                                'Drift detected in Terraform',
                                ['workspace'])

def check_terraform_state():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ Terraform state"""
    try:
        # Get resource count
        result = subprocess.run(
            ['terraform', 'state', 'list'],
            capture_output=True,
            text=True,
            check=True
        )
        
        resources = result.stdout.strip().split('\n')
        resource_types = {}
        
        for resource in resources:
            if resource:
                resource_type = resource.split('.')[0]
                resource_types[resource_type] = resource_types.get(resource_type, 0) + 1
        
        workspace = subprocess.run(
            ['terraform', 'workspace', 'show'],
            capture_output=True,
            text=True,
            check=True
        ).stdout.strip()
        
        # Update metrics
        for rtype, count in resource_types.items():
            terraform_resource_count.labels(
                workspace=workspace,
                type=rtype
            ).set(count)
        
        # Check for drift
        plan_result = subprocess.run(
            ['terraform', 'plan', '-detailed-exitcode'],
            capture_output=True
        )
        
        # Exit code 2 means changes detected
        if plan_result.returncode == 2:
            terraform_drift_detected.labels(workspace=workspace).set(1)
            print(f"‚ö†Ô∏è  Drift detected in {workspace}")
        else:
            terraform_drift_detected.labels(workspace=workspace).set(0)
            print(f"‚úì No drift in {workspace}")
            
    except Exception as e:
        print(f"Error checking Terraform: {e}")

if __name__ == '__main__':
    start_http_server(8000)
    print("Terraform exporter started on :8000")
    
    while True:
        check_terraform_state()
        time.sleep(300)  # Check every 5 minutes
```

---

**–ß–µ–∫–ª–∏—Å—Ç –º–æ–¥—É–ª—è 11:**

- ‚úÖ –ù–∞—Å—Ç—Ä–æ–∏–ª kube-state-metrics
- ‚úÖ –°–æ–∑–¥–∞–ª Kubernetes –∞–ª–µ—Ä—Ç—ã
- ‚úÖ –ü–æ—Å—Ç—Ä–æ–∏–ª K8s –¥–∞—à–±–æ—Ä–¥—ã
- ‚úÖ  –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–ª cloud monitoring (AWS/GCP/Azure)
- ‚úÖ –ù–∞—Å—Ç—Ä–æ–∏–ª network monitoring (SNMP)
- ‚úÖ  –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã

## –ú–æ–¥—É–ª—å 13: –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –≤ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ - Best Practices (15 –º–∏–Ω—É—Ç)

### üéØ –ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞

**SRE –ø—Ä–∏–Ω—Ü–∏–ø—ã:**

```
SLI (Service Level Indicator)   - –ú–µ—Ç—Ä–∏–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —Å–µ—Ä–≤–∏—Å–∞
SLO (Service Level Objective)   - –¶–µ–ª–µ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ SLI
SLA (Service Level Agreement)   - –î–æ–≥–æ–≤–æ—Ä–Ω–æ–µ –æ–±—è–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ
Error Budget                     - –î–æ–ø—É—Å—Ç–∏–º–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—à–∏–±–æ–∫
```

**–ü—Ä–∏–º–µ—Ä—ã SLI/SLO:**

```
SLI: Availability
SLO: 99.9% uptime (43.2 min downtime/month)

SLI: Latency
SLO: 95% requests < 100ms

SLI: Error Rate
SLO: < 0.1% error rate

SLI: Throughput
SLO: Handle 10,000 req/s
```

**Error Budget:**

```
Uptime SLO: 99.9%
Allowed downtime: 0.1% = 43.2 min/month

If error budget > 0:
  ‚Üí Can take risks, deploy faster
  
If error budget = 0:
  ‚Üí Focus on reliability, slow deploys
```

**Monitoring Best Practices:**

```
‚úÖ DO:
- Monitor symptoms, not causes
- Alert on SLO violations
- Use runbooks for alerts
- Test alerts regularly
- Keep dashboards simple
- Document everything
- Use labels consistently
- Set up test environments
- Automate alert remediation where possible
- Review alerts quarterly

‚ùå DON'T:
- Alert on everything
- Set alert thresholds too tight
- Ignore alert fatigue
- Monitor without context
- Create dashboards without purpose
- Alert without actionable next steps
```

**Dashboard hierarchy:**

```
Level 1: Overview (C-level)
- Overall system health
- Key business metrics
- High-level SLOs

Level 2: Service (Team leads)
- Per-service metrics
- RED/USE metrics
- Resource utilization

Level 3: Detailed (Engineers)
- Detailed metrics
- Debug information
- Deep-dive panels
```

**On-call best practices:**

```
1. Clear escalation paths
2. Comprehensive runbooks
3. Postmortems for incidents
4. Fair rotation schedule
5. Context in alerts
6. Response time SLOs
7. Blameless culture
8. Regular drills
```

**Incident Response Process:**

```
1. Detection    - Alert fires
2. Triage       - Assess severity
3. Mitigation   - Stop the bleeding
4. Investigation - Find root cause
5. Resolution   - Fix permanently
6. Postmortem   - Learn & improve
```

**Cost optimization:**

```
- Use recording rules for expensive queries
- Set appropriate retention periods
- Use downsampling for old data
- Archive cold data
- Monitor cardinality
- Use relabeling to drop unnecessary metrics
- Implement metric limits
```

### üíª –ó–∞–¥–∞–Ω–∏–µ

–°–æ–∑–¥–∞–π production-ready monitoring setup:

1. **–û–ø—Ä–µ–¥–µ–ª–∏ SLIs/SLOs –¥–ª—è —Å–µ—Ä–≤–∏—Å–∞**:

**slo_config.yml**:

yaml

```yaml
slos:
  - name: api_availability
    description: "API –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –¥–æ—Å—Ç—É–ø–µ–Ω 99.9% –≤—Ä–µ–º–µ–Ω–∏"
    target: 0.999
    window: 30d
    sli:
      error_ratio_query: |
        sum(rate(http_requests_total{job="frontend",status=~"5.."}[5m]))
        /
        sum(rate(http_requests_total{job="frontend"}[5m]))

  - name: api_latency
    description: "95% –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–æ–ª–∂–Ω—ã –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å—Å—è < 200ms"
    target: 0.95
    window: 30d
    sli:
      latency_query: |
        histogram_quantile(0.95,
          rate(http_request_duration_seconds_bucket{job="frontend"}[5m])
        ) < 0.2

  - name: error_rate
    description: "–ü—Ä–æ—Ü–µ–Ω—Ç –æ—à–∏–±–æ–∫ < 0.1%"
    target: 0.999
    window: 30d
    sli:
      error_ratio_query: |
        (
          sum(rate(http_requests_total{status=~"5.."}[5m]))
          /
          sum(rate(http_requests_total[5m]))
        ) < 0.001
```

2. **–°–æ–∑–¥–∞–π SLO alerts** (–¥–æ–±–∞–≤—å –≤ `alerts.yml`):

yaml

```yaml
groups:
  - name: slo_alerts
    rules:
    # Error Budget Alert
    - alert: ErrorBudgetBurn
      expr: |
        (
          1 - (
            sum(rate(http_requests_total{status=~"2.."}[1h]))
            /
            sum(rate(http_requests_total[1h]))
          )
        ) > 0.001
      for: 5m
      labels:
        severity: critical
        slo: availability
      annotations:
        summary: "Error budget burning too fast"
        description: "Current error rate {{ $value | humanizePercentage }} exceeds budget"
        runbook: "https://runbook.example.com/error-budget"

    # Latency SLO violation
    - alert: LatencySLOViolation
      expr: |
        histogram_quantile(0.95,
          rate(http_request_duration_seconds_bucket[5m])
        ) > 0.2
      for: 10m
      labels:
        severity: warning
        slo: latency
      annotations:
        summary: "Latency SLO violation"
        description: "p95 latency is {{ $value }}s (SLO: 0.2s)"
        impact: "Users experiencing slow responses"
        action: "Check service performance and database queries"

    # Multi-window burn rate
    - alert: ErrorBudgetFastBurn
      expr: |
        (
          (1 - avg_over_time(up[1h]) < 0.999)
          and
          (1 - avg_over_time(up[5m]) < 0.999)
        )
      labels:
        severity: critical
        burn_rate: fast
      annotations:
        summary: "Error budget burning at fast rate"
        description: "Both short and long windows show SLO violations"
```

3. **–°–æ–∑–¥–∞–π comprehensive dashboard** (`production-overview.json`):

json

```json
{
  "dashboard": {
    "title": "Production Overview",
    "tags": ["production", "slo"],
    "rows": [
      {
        "title": "SLOs",
        "panels": [
          {
            "title": "Availability (30d SLO: 99.9%)",
            "targets": [{
              "expr": "avg_over_time(up[30d]) * 100"
            }],
            "type": "stat",
            "thresholds": [
              {"value": 99.9, "color": "green"},
              {"value": 99.5, "color": "yellow"},
              {"value": 0, "color": "red"}
            ]
          },
          {
            "title": "Error Budget Remaining",
            "targets": [{
              "expr": "(0.999 - (1 - avg_over_time(up[30d]))) / 0.001 * 100"
            }],
            "type": "gauge"
          }
        ]
      },
      {
        "title": "Golden Signals",
        "panels": [
          {
            "title": "Request Rate",
            "targets": [{
              "expr": "sum(rate(http_requests_total[5m]))"
            }]
          },
          {
            "title": "Error Rate",
            "targets": [{
              "expr": "sum(rate(http_requests_total{status=~\"5..\"}[5m])) / sum(rate(http_requests_total[5m])) * 100"
            }]
          },
          {
            "title": "Latency (p50, p95, p99)",
            "targets": [
              {"expr": "histogram_quantile(0.50, rate(http_request_duration_seconds_bucket[5m]))", "legendFormat": "p50"},
              {"expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))", "legendFormat": "p95"},
              {"expr": "histogram_quantile(0.99, rate(http_request_duration_seconds_bucket[5m]))", "legendFormat": "p99"}
            ]
          }
        ]
      }
    ]
  }
}
```

4. **–°–æ–∑–¥–∞–π runbook template** (`runbooks/high-error-rate.md`):

markdown

````markdown
# Runbook: High Error Rate

## Alert
**Name:** HighErrorRate
**Severity:** Critical
**SLO Impact:** Availability

## Symptoms
- Error rate > 1% for more than 5 minutes
- Users experiencing 5xx errors
- Error budget burning

## Impact
- **Users:** Cannot complete requests
- **Business:** Loss of revenue/trust
- **SLO:** Burns error budget

## Diagnosis

### Step 1: Verify the alert
```bash
# Check current error rate
curl -G http://prometheus:9090/api/v1/query \
  --data-urlencode 'query=rate(http_requests_total{status=~"5.."}[5m])/rate(http_requests_total[5m])'
```

### Step 2: Identify affected services
```bash
# Check which services are returning errors
# Grafana ‚Üí Explore ‚Üí Prometheus
sum by (job) (rate(http_requests_total{status=~"5.."}[5m]))
```

### Step 3: Check logs
```bash
# Grafana ‚Üí Explore ‚Üí Loki
{job="frontend"} |= "ERROR" | json
```

### Step 4: Check recent deployments
```bash
# Check if error rate increased after deployment
kubectl get pods -o wide
kubectl describe deployment frontend
```

### Step 5: Check dependencies
```bash
# Database
psql -c "SELECT pg_is_in_recovery();"

# Cache
redis-cli ping

# External APIs
curl https://api.external.com/health
```

## Mitigation

### Immediate (Stop the bleeding)
1. **Rollback deployment** if recent:
```bash
kubectl rollout undo deployment/frontend
```

2. **Scale up** if resource constrained:
```bash
kubectl scale deployment/frontend --replicas=10
```

3. **Enable circuit breaker** for failing dependency:
```bash
# Update config to bypass failing service
```

4. **Put up maintenance page** if critical:
```bash
# Route traffic to maintenance page
```

### Short-term (Stabilize)
1. Investigate root cause
2. Apply proper fix
3. Deploy with gradual rollout
4. Monitor closely

## Resolution
- [ ] Error rate back to < 0.1%
- [ ] Root cause identified
- [ ] Fix deployed and verified
- [ ] Monitoring confirms stability
- [ ] Postmortem scheduled

## Escalation
- **L1:** On-call engineer (You)
- **L2:** Team lead (after 15 min)
- **L3:** Engineering manager (after 30 min)
- **L4:** VP Engineering (critical)

## References
- Dashboard: http://grafana/d/prod-overview
- Logs: http://grafana/explore?loki
- Traces: http://jaeger:16686
- Slack: #incidents

## Related Runbooks
- [Database Connection Issues](./db-connection.md)
- [High Latency](./high-latency.md)
- [Service Down](./service-down.md)
````

5. **–°–æ–∑–¥–∞–π incident response script** (`scripts/incident_response.sh`):

bash

```bash
#!/bin/bash

# Incident Response Helper Script

set -e

ALERT_NAME=$1
SEVERITY=$2

if [ -z "$ALERT_NAME" ]; then
  echo "Usage: ./incident_response.sh <alert_name> <severity>"
  exit 1
fi

echo "üö® Incident Response Started"
echo "Alert: $ALERT_NAME"
echo "Severity: $SEVERITY"
echo "Time: $(date)"
echo ""

# 1. Gather context
echo "üìä Gathering context..."
echo ""

echo "Current Metrics:"
curl -s -G http://localhost:9090/api/v1/query \
  --data-urlencode 'query=up' | jq '.data.result[] | {job: .metric.job, status: .value[1]}'

echo ""
echo "Recent Errors (last 5 min):"
curl -s -G http://localhost:9090/api/v1/query \
  --data-urlencode 'query=sum(rate(http_requests_total{status=~"5.."}[5m]))' | jq '.data.result[0].value[1]'

echo ""
echo "Active Alerts:"
curl -s http://localhost:9090/api/v1/alerts | jq '.data.alerts[] | select(.state=="firing") | {alert: .labels.alertname, severity: .labels.severity}'

# 2. Check recent changes
echo ""
echo "üîç Recent changes..."
docker ps --format "table {{.Names}}\t{{.Status}}\t{{.Image}}"

# 3. Generate incident report
INCIDENT_ID="INC-$(date +%Y%m%d-%H%M%S)"
echo ""
echo "üìù Creating incident report: $INCIDENT_ID"

cat > "incidents/${INCIDENT_ID}.md" <<EOF
# Incident Report: $INCIDENT_ID

## Summary
- **Alert:** $ALERT_NAME
- **Severity:** $SEVERITY
- **Start Time:** $(date)
- **Status:** Investigating

## Timeline
- $(date +%H:%M:%S) - Alert fired
- $(date +%H:%M:%S) - Investigation started

## Impact
- [ ] Users affected: TBD
- [ ] Services affected: TBD
- [ ] Revenue impact: TBD

## Actions Taken
- Gathered initial context
- Reviewed metrics and logs

## Next Steps
1. Identify root cause
2. Implement mitigation
3. Verify resolution
4. Schedule postmortem

## Notes
EOF

echo "Incident report created: incidents/${INCIDENT_ID}.md"
echo ""
echo "‚úÖ Context gathered. Next: Check runbook at runbooks/${ALERT_NAME}.md"
```

### üöÄ –ë–æ–Ω—É—Å (–Ω–æ–≤–æ–µ)

**–°–æ–∑–¥–∞–π automated remediation** –¥–ª—è —á–∞—Å—Ç—ã—Ö –ø—Ä–æ–±–ª–µ–º:

**auto_remediation.py**:

python

```python
import requests
import time
from datetime import datetime

PROMETHEUS_URL = "http://localhost:9090"
ALERTMANAGER_URL = "http://localhost:9093"

def check_alerts():
    """Check for active alerts"""
    response = requests.get(f"{PROMETHEUS_URL}/api/v1/alerts")
    alerts = response.json()['data']['alerts']
    
    firing_alerts = [a for a in alerts if a['state'] == 'firing']
    return firing_alerts

def auto_remediate(alert):
    """Attempt automatic remediation"""
    alert_name = alert['labels']['alertname']
    
    print(f"[{datetime.now()}] Attempting auto-remediation for: {alert_name}")
    
    if alert_name == "HighMemoryUsage":
        # Clear caches
        print("  ‚Üí Clearing application caches")
        requests.post("http://localhost:5000/admin/clear-cache")
        
    elif alert_name == "HighCPUUsage":
        # Scale up service
        print("  ‚Üí Scaling up service")
        # kubectl scale deployment --replicas=+2
        
    elif alert_name == "DiskSpaceLow":
        # Clean old logs
        print("  ‚Üí Cleaning old logs")
        import subprocess
        subprocess.run(["find", "/var/log", "-name", "*.log.gz", "-mtime", "+7", "-delete"])
    
    else:
        print(f"  ‚ö†Ô∏è  No auto-remediation for {alert_name}")
        return False
    
    print(f"  ‚úÖ Auto-remediation completed")
    return True

def create_silence(alert, duration_hours=1):
    """Create silence after remediation"""
    silence = {
        "matchers": [
            {
                "name": "alertname",
                "value": alert['labels']['alertname'],
                "isRegex": False
            }
        ],
        "startsAt": datetime.utcnow().isoformat() + "Z",
        "endsAt": datetime.utcnow().isoformat() + "Z",  # +duration
        "createdBy": "auto-remediation",
        "comment": f"Auto-remediated at {datetime.now()}"
    }
    
    requests.post(f"{ALERTMANAGER_URL}/api/v2/silences", json=silence)

if __name__ == '__main__':
    print("ü§ñ Auto-remediation service started")
    
    while True:
        alerts = check_alerts()
        
        for alert in alerts:
            if auto_remediate(alert):
                create_silence(alert, duration_hours=1)
        
        time.sleep(60)
```


### üöÄ –ë–æ–Ω—É—Å: Chaos Engineering

**–ù–∞—Å—Ç—Ä–æ–π chaos engineering –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞**:

**chaos_test.sh**:

bash

```bash
#!/bin/bash

echo "üî• Starting Chaos Engineering Tests"
echo "Testing monitoring system resilience..."
echo ""

# Test 1: Kill random container
echo "Test 1: Container Failure"
CONTAINER=$(docker ps --format "{{.Names}}" | grep -E "frontend|auth|business" | shuf -n 1)
echo "  ‚Üí Killing: $CONTAINER"
docker kill $CONTAINER
echo "  ‚Üí Waiting 30 seconds..."
sleep 30
echo "  ‚Üí Restoring container..."
docker-compose up -d $CONTAINER
echo "  ‚úÖ Test 1 complete"
echo ""

# Test 2: Simulate high CPU
echo "Test 2: High CPU Load"
echo "  ‚Üí Starting CPU stress on frontend..."
docker exec frontend sh -c "for i in 1 2 3 4; do yes > /dev/null & done" 2>/dev/null
echo "  ‚Üí Stress running for 60 seconds..."
sleep 60
echo "  ‚Üí Stopping stress..."
docker exec frontend sh -c "pkill yes" 2>/dev/null
echo "  ‚úÖ Test 2 complete"
echo ""

# Test 3: Simulate network latency
echo "Test 3: Network Latency"
echo "  ‚Üí Adding 200ms latency..."
docker exec frontend sh -c "tc qdisc add dev eth0 root netem delay 200ms" 2>/dev/null || echo "  ‚ö†Ô∏è  tc not available"
sleep 60
echo "  ‚Üí Removing latency..."
docker exec frontend sh -c "tc qdisc del dev eth0 root" 2>/dev/null
echo "  ‚úÖ Test 3 complete"
echo ""

# Test 4: Disk space pressure
echo "Test 4: Disk Space Pressure"
echo "  ‚Üí Creating 1GB file..."
docker exec frontend sh -c "dd if=/dev/zero of=/tmp/fillfile bs=1M count=1000" 2>/dev/null
sleep 30
echo "  ‚Üí Removing file..."
docker exec frontend sh -c "rm /tmp/fillfile" 2>/dev/null
echo "  ‚úÖ Test 4 complete"
echo ""

# Test 5: Memory pressure
echo "Test 5: Memory Pressure"
echo "  ‚Üí Allocating 512MB..."
docker exec frontend sh -c "stress --vm 1 --vm-bytes 512M --timeout 60s" 2>/dev/null || echo "  ‚ö†Ô∏è  stress tool not available"
echo "  ‚úÖ Test 5 complete"
echo ""

# Test 6: Database connection failure
echo "Test 6: Database Connection Failure"
echo "  ‚Üí Stopping PostgreSQL..."
docker-compose stop postgres
sleep 30
echo "  ‚Üí Restarting PostgreSQL..."
docker-compose start postgres
sleep 10
echo "  ‚úÖ Test 6 complete"
echo ""

# Test 7: Random 500 errors
echo "Test 7: Random Application Errors"
echo "  ‚Üí Injecting errors for 60 seconds..."
# –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –∫–æ–¥ –¥–ª—è –∏–Ω—ä–µ–∫—Ü–∏–∏ –æ—à–∏–±–æ–∫ –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ
for i in {1..20}; do
    curl -X POST http://localhost:5000/api/order 2>/dev/null
    sleep 3
done
echo "  ‚úÖ Test 7 complete"
echo ""

echo "üéâ All chaos tests completed!"
echo ""
echo "üìä Check your monitoring:"
echo "  Prometheus Alerts: http://localhost:9090/alerts"
echo "  Grafana Dashboards: http://localhost:3000"
echo "  Alertmanager: http://localhost:9093"
echo "  Jaeger Traces: http://localhost:16686"
echo ""
echo "Questions to verify:"
echo "  ‚úì Did alerts fire as expected?"
echo "  ‚úì Were all incidents visible in dashboards?"
echo "  ‚úì Did traces show the failures?"
echo "  ‚úì Were logs properly collected?"
echo "  ‚úì Did services recover automatically?"
```

**–ó–∞–ø—É—Å–∫:**

bash

```bash
chmod +x chaos_test.sh
./chaos_test.sh
```
## –ú–æ–¥—É–ª—å 14: –§–∏–Ω–∞–ª—å–Ω—ã–π –ø—Ä–æ–µ–∫—Ç –∏ –∫–∞—Ä—å–µ—Ä–∞ (30 –º–∏–Ω—É—Ç)

### üéØ –§–∏–Ω–∞–ª—å–Ω—ã–π –ø—Ä–æ–µ–∫—Ç: E-Commerce Monitoring Stack

–°–æ–∑–¥–∞–π production-ready –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –¥–ª—è –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–º–∞–≥–∞–∑–∏–Ω–∞.

**–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞:**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Frontend Layer ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  NGINX (Load Balancer) ‚Üí Frontend Services (x3)         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ API Layer ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  API Gateway ‚Üí Authentication ‚Üí Rate Limiting            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Service Layer ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Product Service ‚îÇ Order Service ‚îÇ Payment Service       ‚îÇ
‚îÇ  Inventory Svc   ‚îÇ User Service  ‚îÇ Notification Svc     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Data Layer ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  PostgreSQL (Primary + Replica) ‚îÇ Redis Cache ‚îÇ S3       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Monitoring Layer ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Metrics: Prometheus + Grafana                            ‚îÇ
‚îÇ Logs: Loki + Promtail                                    ‚îÇ
‚îÇ Traces: Tempo + Jaeger                                   ‚îÇ
‚îÇ Alerts: Alertmanager ‚Üí PagerDuty/Slack                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### üíª –ó–∞–¥–∞–Ω–∏–µ: –ü–æ–ª–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è

**–®–∞–≥ 1: –ö–ª–æ–Ω–∏—Ä—É–π —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø—Ä–æ–µ–∫—Ç–∞**

bash

```bash
mkdir ecommerce-monitoring
cd ecommerce-monitoring

# –°—Ç—Ä—É–∫—Ç—É—Ä–∞
mkdir -p {services/{frontend,api-gateway,product,order,payment},monitoring/{prometheus,grafana,loki,alertmanager},scripts,docs}
```

**–®–∞–≥ 2: –°–æ–∑–¥–∞–π docker-compose-final.yml**

yaml

```yaml
version: '3.8'

networks:
  frontend:
  backend:
  monitoring:

services:
  # === Load Balancer ===
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
    networks:
      - frontend
      - monitoring
    depends_on:
      - frontend

  # === Application Services ===
  frontend:
    build: ./services/frontend
    deploy:
      replicas: 3
    environment:
      API_URL: http://api-gateway:8080
      JAEGER_AGENT_HOST: jaeger
    networks:
      - frontend
      - monitoring

  api-gateway:
    build: ./services/api-gateway
    environment:
      PRODUCT_SERVICE: http://product-service:8081
      ORDER_SERVICE: http://order-service:8082
      PAYMENT_SERVICE: http://payment-service:8083
      REDIS_URL: redis://redis:6379
    networks:
      - frontend
      - backend
      - monitoring

  product-service:
    build: ./services/product
    environment:
      DATABASE_URL: postgresql://postgres:password@postgres:5432/products
      REDIS_URL: redis://redis:6379
    networks:
      - backend
      - monitoring

  order-service:
    build: ./services/order
    environment:
      DATABASE_URL: postgresql://postgres:password@postgres:5432/orders
      PAYMENT_SERVICE: http://payment-service:8083
    networks:
      - backend
      - monitoring

  payment-service:
    build: ./services/payment
    environment:
      DATABASE_URL: postgresql://postgres:password@postgres:5432/payments
    networks:
      - backend
      - monitoring

  # === Data Layer ===
  postgres:
    image: postgres:15
    environment:
      POSTGRES_PASSWORD: password
      POSTGRES_DB: ecommerce
    volumes:
      - postgres-data:/var/lib/postgresql/data
    networks:
      - backend
      - monitoring

  redis:
    image: redis:7-alpine
    networks:
      - backend
      - monitoring

  # === Monitoring Stack ===
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus:/etc/prometheus
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.retention.time=30d'
      - '--web.enable-lifecycle'
    networks:
      - monitoring

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      GF_SECURITY_ADMIN_PASSWORD: admin
      GF_INSTALL_PLUGINS: grafana-piechart-panel
    volumes:
      - ./monitoring/grafana:/etc/grafana/provisioning
      - grafana-data:/var/lib/grafana
    networks:
      - monitoring

  loki:
    image: grafana/loki:latest
    ports:
      - "3100:3100"
    volumes:
      - ./monitoring/loki:/etc/loki
      - loki-data:/loki
    networks:
      - monitoring

  promtail:
    image: grafana/promtail:latest
    volumes:
      - ./monitoring/promtail:/etc/promtail
      - /var/log:/var/log:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
    networks:
      - monitoring

  tempo:
    image: grafana/tempo:latest
    ports:
      - "3200:3200"
      - "4317:4317"
    volumes:
      - ./monitoring/tempo:/etc/tempo
      - tempo-data:/tmp/tempo
    networks:
      - monitoring

  jaeger:
    image: jaegertracing/all-in-one:latest
    ports:
      - "16686:16686"
      - "14268:14268"
    environment:
      SPAN_STORAGE_TYPE: badger
      BADGER_EPHEMERAL: "false"
      BADGER_DIRECTORY_VALUE: /badger/data
      BADGER_DIRECTORY_KEY: /badger/key
    volumes:
      - jaeger-data:/badger
    networks:
      - monitoring

  alertmanager:
    image: prom/alertmanager:latest
    ports:
      - "9093:9093"
    volumes:
      - ./monitoring/alertmanager:/etc/alertmanager
    networks:
      - monitoring

volumes:
  postgres-data:
  prometheus-data:
  grafana-data:
  loki-data:
  tempo-data:
  jaeger-data:
```

**–®–∞–≥ 3: –û–ø—Ä–µ–¥–µ–ª–∏ SLIs –∏ SLOs**

**docs/slo-definitions.md**:

markdown

```markdown
# Service Level Objectives (SLOs)

## 1. Availability SLO
**SLI:** Percentage of successful requests
**SLO:** 99.9% (43.2 minutes downtime/month)
**Measurement:** `sum(rate(http_requests{status=~"2.."}[30d])) / sum(rate(http_requests[30d]))`

## 2. Latency SLO
**SLI:** 95th percentile response time
**SLO:** < 500ms for 95% of requests
**Measurement:** `histogram_quantile(0.95, rate(http_duration_bucket[5m]))`

## 3. Error Budget
**Calculation:** (1 - SLO) √ó Total requests
**30-day budget:** 0.1% √ó requests = allowed errors
**Burn rate alerting:**
- Fast burn: 2% budget in 1 hour ‚Üí Page
- Slow burn: 10% budget in 6 hours ‚Üí Ticket

## 4. Business SLOs

### Order Processing
- **SLO:** 99.5% orders processed successfully
- **Target:** < 1 minute processing time

### Payment Success
- **SLO:** 99.9% payment success rate
- **Target:** < 3 seconds payment confirmation

### Search Response
- **SLO:** 95% searches return results
- **Target:** < 200ms search response time
```

**–®–∞–≥ 4: –°–æ–∑–¥–∞–π Production Dashboards**

**monitoring/grafana/dashboards/01-executive-overview.json**:

json

```json
{
  "dashboard": {
    "title": "Executive Overview",
    "tags": ["business", "executive"],
    "panels": [
      {
        "title": "System Health Score",
        "type": "gauge",
        "gridPos": {"h": 8, "w": 6, "x": 0, "y": 0},
        "targets": [{
          "expr": "avg((up{job=~\".*service\"} == 1) * 100)"
        }],
        "fieldConfig": {
          "defaults": {
            "unit": "percent",
            "min": 0,
            "max": 100,
            "thresholds": {
              "steps": [
                {"value": 0, "color": "red"},
                {"value": 95, "color": "yellow"},
                {"value": 99, "color": "green"}
              ]
            }
          }
        }
      },
      {
        "title": "Orders per Hour",
        "type": "stat",
        "gridPos": {"h": 4, "w": 6, "x": 6, "y": 0},
        "targets": [{
          "expr": "sum(increase(orders_total[1h]))"
        }],
        "fieldConfig": {
          "defaults": {
            "unit": "short",
            "color": {"mode": "thresholds"},
            "thresholds": {
              "steps": [
                {"value": 0, "color": "red"},
                {"value": 100, "color": "yellow"},
                {"value": 500, "color": "green"}
              ]
            }
          }
        }
      },
      {
        "title": "Revenue Today",
        "type": "stat",
        "gridPos": {"h": 4, "w": 6, "x": 12, "y": 0},
        "targets": [{
          "expr": "sum(increase(payment_amount_total[24h]))"
        }],
        "fieldConfig": {
          "defaults": {
            "unit": "currencyUSD",
            "decimals": 2
          }
        }
      },
      {
        "title": "Active Users",
        "type": "graph",
        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 8},
        "targets": [{
          "expr": "sum(rate(http_requests_total{endpoint=\"/\"}[5m])) * 60",
          "legendFormat": "Active Users"
        }]
      }
    ]
  }
}
```

**–®–∞–≥ 5: Comprehensive Alerts**

**monitoring/prometheus/alerts/production.yml**:

yaml

```yaml
groups:
  - name: critical_slo_violations
    interval: 30s
    rules:
    # Multi-window burn rate (Google SRE)
    - alert: ErrorBudgetCriticalBurn
      expr: |
        (
          sum(rate(http_requests{status=~"5.."}[1h]))
          / sum(rate(http_requests[1h]))
        ) > (14.4 * 0.001)  # 2% of monthly budget in 1 hour
        and
        (
          sum(rate(http_requests{status=~"5.."}[5m]))
          / sum(rate(http_requests[5m]))
        ) > (14.4 * 0.001)
      labels:
        severity: page
        team: sre
      annotations:
        summary: "üö® Critical error budget burn"
        description: "Burning 2% of 30-day error budget per hour"
        runbook: "https://runbook.company.com/error-budget-burn"
        action: "Page on-call engineer immediately"

    - alert: ServiceDown
      expr: up{job=~".*-service"} == 0
      for: 1m
      labels:
        severity: page
        team: platform
      annotations:
        summary: "üî¥ Service {{ $labels.job }} is DOWN"
        description: "{{ $labels.instance }} unreachable for 1+ minutes"
        impact: "Service unavailable to users"

  - name: slo_approaching_violations
    interval: 1m
    rules:
    - alert: LatencySLOAtRisk
      expr: |
        histogram_quantile(0.95,
          sum(rate(http_duration_bucket[30m])) by (le)
        ) > 0.45  # 90% of 500ms threshold
      for: 15m
      labels:
        severity: warning
        team: backend
      annotations:
        summary: "‚ö†Ô∏è Latency approaching SLO limit"
        description: "p95 latency: {{ $value }}s (SLO: 0.5s)"

  - name: business_kpis
    interval: 5m
    rules:
    - alert: OrderRateDropCritical
      expr: |
        (
          sum(rate(orders_total[10m]))
          /
          sum(rate(orders_total[10m] offset 1h))
        ) < 0.5
      for: 10m
      labels:
        severity: page
        team: business
      annotations:
        summary: "üìâ Order rate dropped 50%+"
        description: "Current: {{ $value | humanizePercentage }} of normal"
        impact: "Severe revenue impact"

    - alert: PaymentFailureSpike
      expr: |
        sum(rate(payments_total{status="failed"}[5m]))
        / sum(rate(payments_total[5m]))
        > 0.05
      for: 5m
      labels:
        severity: page
        team: payments
      annotations:
        summary: "üí≥ Payment failure rate > 5%"
        description: "{{ $value | humanizePercentage }} payments failing"

  - name: infrastructure
    interval: 1m
    rules:
    - alert: HighMemoryPressure
      expr: |
        (
          1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)
        ) > 0.90
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Memory usage > 90%"

    - alert: DiskFillPrediction
      expr: |
        predict_linear(node_filesystem_avail_bytes[1h], 4*3600) < 0
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Disk will fill in ~4 hours"
        action: "Clean logs or expand disk"

    - alert: DatabaseConnectionPoolExhausted
      expr: |
        pg_stat_database_numbackends
        / pg_settings_max_connections
        > 0.8
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "DB connection pool 80%+ utilized"
```

**–®–∞–≥ 6: Runbooks**

**docs/runbooks/high-error-rate.md**:

markdown

````markdown
# Runbook: High Error Rate

## Alert Details
- **Alert:** HighErrorRate  
- **Severity:** Critical (Page)
- **SLO Impact:** Availability

## Symptoms
- Error rate > 1% for 5+ minutes
- Users seeing 5xx errors
- Error budget burning fast

## Initial Response (First 5 minutes)

### 1. Acknowledge alert
```bash
# Silence alert while investigating
amtool silence add alertname=HighErrorRate --duration=30m --author=oncall --comment="Investigating"
```

### 2. Check overall system health
- Grafana: http://grafana.company.com/d/overview
- Look for: spike in errors, latency, resource usage

### 3. Identify affected service(s)
```promql
# Which service has errors?
topk(5, sum by (service) (rate(http_requests{status=~"5.."}[5m])))
```

### 4. Check recent changes
```bash
# Recent deployments
kubectl get events --sort-by='.lastTimestamp' | head -20

# Recent config changes
git log --since="1 hour ago" --oneline
```

## Diagnosis

### Check application logs
```logql
# Loki query
{service="api"} |= "ERROR" | json | line_format "{{.level}}: {{.message}}"
```

### Check traces
- Jaeger: http://jaeger.company.com
- Search for failing requests
- Look for slow/failing spans

### Check dependencies
```bash
# Database
pg_isready -h postgres-primary
SELECT count(*) FROM pg_stat_activity WHERE state = 'active';

# Redis
redis-cli -h redis ping

# External APIs
curl -I https://payment-gateway.external.com/health
```

## Common Causes & Solutions

### 1. Bad Deployment
**Symptoms:** Errors started after recent deploy

**Solution:**
```bash
# Immediate rollback
kubectl rollout undo deployment/api-service

# Verify
kubectl rollout status deployment/api-service
```

### 2. Database Issues
**Symptoms:** Slow queries, timeouts

**Solution:**
```sql
-- Check long-running queries
SELECT pid, age(clock_timestamp(), query_start), query
FROM pg_stat_activity
WHERE state = 'active' AND query_start < now() - interval '1 minute'
ORDER BY query_start;

-- Kill if needed
SELECT pg_terminate_backend(pid);
```

### 3. Resource Exhaustion
**Symptoms:** High CPU/memory, OOMKills

**Solution:**
```bash
# Scale up immediately
kubectl scale deployment/api-service --replicas=10

# Check resource usage
kubectl top pods
```

### 4. External API Failure
**Symptoms:** Timeout errors, circuit breaker open

**Solution:**
```bash
# Enable fallback/cache
kubectl set env deployment/api-service USE_CACHE=true

# Bypass failing dependency if non-critical
kubectl set env deployment/api-service FEATURE_X_ENABLED=false
```

## Mitigation Strategy

### Immediate (Stop the bleeding)
1. Rollback bad deployment
2. Scale up if resource constrained
3. Enable circuit breakers
4. Route to healthy instances

### Short-term (Stabilize)
1. Apply proper fix
2. Gradual rollout with monitoring
3. Load test before full deployment

### Long-term (Prevent)
1. Add pre-deployment tests
2. Improve monitoring/alerting
3. Implement gradual rollouts
4. Add chaos testing

## Verification

- [ ] Error rate < 0.1%
- [ ] Latency back to normal
- [ ] No active alerts
- [ ] Users not reporting issues

## Communication

### During incident
````

Slack: #incidents "Investigating high error rate on API service. ETA for resolution: 15 minutes. Status page: [https://status.company.com](https://status.company.com)"

```

### After resolution
```

"Issue resolved. Root cause: [X]. Total impact: [Y] minutes. Postmortem scheduled for [date]."

```

## Escalation Path
1. **L1** (0-5 min): On-call engineer
2. **L2** (5-15 min): Team lead
3. **L3** (15-30 min): Engineering manager
4. **L4** (30+ min): VP Engineering + CTO

## Postmortem
Schedule within 24 hours. Template: docs/postmortem-template.md

## Related Runbooks
- [Service Down](./service-down.md)
- [High Latency](./high-latency.md)
- [Database Issues](./database-issues.md)
```

### üéØ –ö—Ä–∏—Ç–µ—Ä–∏–∏ —É—Å–ø–µ—à–Ω–æ–π —Å–¥–∞—á–∏ –ø—Ä–æ–µ–∫—Ç–∞

**Must Have (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ):**

- [ ]  –í—Å–µ —Å–µ—Ä–≤–∏—Å—ã –∑–∞–ø—É—Å–∫–∞—é—Ç—Å—è –æ–¥–Ω–æ–π –∫–æ–º–∞–Ω–¥–æ–π
- [ ]  Prometheus —Å–æ–±–∏—Ä–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ —Å–æ –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
- [ ]  3+ dashboard –≤ Grafana (Business, Technical, Infrastructure)
- [ ]  Loki —Å–æ–±–∏—Ä–∞–µ—Ç –ª–æ–≥–∏ –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ
- [ ]  Distributed tracing —Ä–∞–±–æ—Ç–∞–µ—Ç —á–µ—Ä–µ–∑ Jaeger/Tempo
- [ ]  10+ production alerts –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã
- [ ]  3+ SLO –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –∏ –∏–∑–º–µ—Ä—è—é—Ç—Å—è
- [ ]  Runbooks –¥–ª—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö alerts
- [ ]  Load testing –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å
- [ ]  Documentation (README, architecture, SLOs)

**Nice to Have (–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ):**

- [ ]  Multi-environment (dev/staging/prod)
- [ ]  Automated remediation
- [ ]  Chaos engineering suite
- [ ]  Cost analysis dashboard
- [ ]  Security monitoring
- [ ]  Capacity planning dashboard
- [ ]  Custom exporters
- [ ]  Integration tests
- [ ]  Performance benchmarks
- [ ]  Postmortem examples

---

## üìö –ö–∞—Ä—å–µ—Ä–Ω—ã–π –ø—É—Ç—å DevOps/SRE

### Junior DevOps/Monitoring Engineer (0-2 –≥–æ–¥–∞)

**–ù–∞–≤—ã–∫–∏:**

- Linux basics
- Docker basics
- Basic monitoring (Prometheus, Grafana)
- Log aggregation basics
- Alert configuration
- Dashboard creation

**–ó–∞—Ä–ø–ª–∞—Ç–∞:** $40k-70k

### Middle DevOps/SRE (2-4 –≥–æ–¥–∞)

**–ù–∞–≤—ã–∫–∏:**

- Advanced Prometheus (recording rules, federation)
- Distributed tracing
- SLI/SLO management
- Incident response
- CI/CD integration
- Infrastructure as Code

**–ó–∞—Ä–ø–ª–∞—Ç–∞:** $70k-120k

### Senior SRE (4-7 –ª–µ—Ç)

**–ù–∞–≤—ã–∫–∏:**

- System design –¥–ª—è observability
- Multi-cloud monitoring
- Capacity planning
- Cost optimization
- Team leadership
- On-call strategy

**–ó–∞—Ä–ø–ª–∞—Ç–∞:** $120k-180k

### Staff/Principal SRE (7+ –ª–µ—Ç)

**–ù–∞–≤—ã–∫–∏:**

- Organization-wide observability strategy
- Tooling development
- SLO framework design
- Incident management process
- Technical leadership

**–ó–∞—Ä–ø–ª–∞—Ç–∞:** $180k-300k+

### –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ —Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏—è—Ö

**–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ:**

1. **Explain the difference between monitoring and observability**
2. **How would you monitor a microservices architecture?**
3. **What is high cardinality and why is it a problem?**
4. **Design an alerting strategy that avoids alert fatigue**
5. **How do you calculate error budget for 99.9% SLO?**
6. **Explain push vs pull monitoring models**
7. **How would you debug a memory leak in production?**
8. **What metrics would you track for a database?**

**–°–∏—Ç—É–∞—Ü–∏–æ–Ω–Ω—ã–µ:**

1. **Production is down, walk me through your process**
2. **You're getting 100 alerts per minute, what do you do?**
3. **Disk is 99% full but you can't find large files**
4. **Latency increased 10x after deployment, how to investigate?**
5. **Your monitoring system is down, how do you monitor?**

**System Design:**

1. **Design monitoring for a global CDN**
2. **Design alerting for 10,000 microservices**
3. **How would you monitor a mobile app backend?**

---

## üèÜ –§–∏–Ω–∞–ª—å–Ω—ã–π —ç–∫–∑–∞–º–µ–Ω

### –ß–∞—Å—Ç—å 1: –¢–µ–æ—Ä–∏—è (30 –±–∞–ª–ª–æ–≤)

**–í–æ–ø—Ä–æ—Å 1 (10 –±–∞–ª–ª–æ–≤):** –û–±—ä—è—Å–Ω–∏ –∫–æ–Ω—Ü–µ–ø—Ü–∏—é Error Budget –∏ –∫–∞–∫ –µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ —Å–∫–æ—Ä–æ—Å—Ç–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –∏ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏.

**–í–æ–ø—Ä–æ—Å 2 (10 –±–∞–ª–ª–æ–≤):** –°—Ä–∞–≤–Ω–∏ USE –∏ RED –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏. –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–∞–∂–¥—É—é?

**–í–æ–ø—Ä–æ—Å 3 (10 –±–∞–ª–ª–æ–≤):** –ß—Ç–æ —Ç–∞–∫–æ–µ "high cardinality" –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –º–µ—Ç—Ä–∏–∫? –ü–æ—á–µ–º—É —ç—Ç–æ –ø—Ä–æ–±–ª–µ–º–∞ –∏ –∫–∞–∫ –µ—ë —Ä–µ—à–∏—Ç—å?

### –ß–∞—Å—Ç—å 2: –ü—Ä–∞–∫—Ç–∏–∫–∞ (70 –±–∞–ª–ª–æ–≤)

**–ó–∞–¥–∞–Ω–∏–µ 1: Incident Response (25 –±–∞–ª–ª–æ–≤)**

```
–°—Ü–µ–Ω–∞—Ä–∏–π:
- 02:00 AM: PagerDuty alert "API Error Rate High"
- Current error rate: 15% (normal: 0.1%)
- Last deployment: 4 hours ago
- Affected: Payment service

–ó–∞–¥–∞—á–∏:
1. –ù–∞–ø–∏—à–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–µ–π—Å—Ç–≤–∏–π (first 10 minutes)
2. –ö–∞–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏/–ª–æ–≥–∏/traces –ø—Ä–æ–≤–µ—Ä–∏—à—å?
3. 3 –Ω–∞–∏–±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã
4. Mitigation strategy –¥–ª—è –∫–∞–∂–¥–æ–π
5. Communication plan
```

**–ó–∞–¥–∞–Ω–∏–µ 2: Monitoring Design (25 –±–∞–ª–ª–æ–≤)**

```
–°–ø—Ä–æ–µ–∫—Ç–∏—Ä—É–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –¥–ª—è:
- Video streaming platform
- 10M users
- 1M concurrent streams
- Multi-region deployment

–û–ø—Ä–µ–¥–µ–ª–∏:
1. Key metrics (–º–∏–Ω–∏–º—É–º 15)
2. SLIs –∏ SLOs (–º–∏–Ω–∏–º—É–º 5)
3. Critical alerts (–º–∏–Ω–∏–º—É–º 8)
4. Dashboard structure
5. Cost estimation
```

**–ó–∞–¥–∞–Ω–∏–µ 3: PromQL Challenge (20 –±–∞–ª–ª–æ–≤)**

–ù–∞–ø–∏—à–∏ –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è:

```
1. CPU usage per pod (excluding idle)
2. p99 latency for last 24 hours
3. Error rate by endpoint (last 5 min)
4. Predict when disk will be full
5. Cache hit ratio trending down
6. Requests per second by service
7. Top 5 slowest endpoints
8. Database connection pool utilization
9. Apdex score (T=300ms)
10. Memory usage forecast (next
```
