
# –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –¥–ª—è DevOps: –ï–∂–µ–≥–æ–¥–Ω—ã–π/–ü–æ–ª—É–≥–æ–¥–æ–≤–æ–π –∫—É—Ä—Å-–æ—Å–≤–µ–∂–∏—Ç–µ–ª—å

**–¶–µ–ª—å:** –û—Å–≤–µ–∂–∏—Ç—å –≤ –ø–∞–º—è—Ç–∏ –∫–ª—é—á–µ–≤—ã–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∑–∞ 2-3 —á–∞—Å–∞ –ø—Ä–∞–∫—Ç–∏–∫–∏ –∏ —É–∑–Ω–∞—Ç—å 1-2 –Ω–æ–≤—ã–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Ç–µ—Ö–Ω–∏–∫–∏.

**–§–æ—Ä–º–∞—Ç:** –ö–∞–∂–¥—ã–π —Ä–∞–∑–¥–µ–ª —Å–æ—Å—Ç–æ–∏—Ç –∏–∑:
1. **–ö—Ä–∞—Ç–∫–æ–π —Ç–µ–æ—Ä–∏–∏ (–ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞)**: –°–∞–º–æ–µ –≥–ª–∞–≤–Ω–æ–µ, —á—Ç–æ –≤—ã –º–æ–≥–ª–∏ –∑–∞–±—ã—Ç—å
2. **–ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–≥–æ –∑–∞–¥–∞–Ω–∏—è**: –†–µ–∞–ª—å–Ω–∞—è –∑–∞–¥–∞—á–∞, –∫–æ—Ç–æ—Ä—É—é –Ω—É–∂–Ω–æ —Ä–µ—à–∏—Ç—å
3. **–ë–æ–Ω—É—Å–Ω–æ–≥–æ –∑–∞–¥–∞–Ω–∏—è (–¥–ª—è —Ä–æ—Å—Ç–∞)**: –ó–∞–¥–∞—á–∞ –ø–æ—Å–ª–æ–∂–Ω–µ–µ –∏–ª–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –Ω–æ–≤–æ–π —Ñ–∏—á–∏

**–ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è:**
- –ë–∞–∑–æ–≤–æ–µ –ø–æ–Ω–∏–º–∞–Ω–∏–µ Linux/Unix
- –î–æ—Å—Ç—É–ø –∫ —Å–µ—Ä–≤–µ—Ä—É –∏–ª–∏ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–π –º–∞—à–∏–Ω–µ
- Docker —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω (–¥–ª—è –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–∞ –∑–∞–¥–∞–Ω–∏–π)
- –ë–∞–∑–æ–≤—ã–µ –∑–Ω–∞–Ω–∏—è –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏

---

## –ú–æ–¥—É–ª—å 1: –û—Å–Ω–æ–≤—ã –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∏ –º–µ—Ç—Ä–∏–∫–∏ (20 –º–∏–Ω—É—Ç)

### üéØ –ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞

**–ß–µ—Ç—ã—Ä–µ –∑–æ–ª–æ—Ç—ã—Ö —Å–∏–≥–Ω–∞–ª–∞ (Four Golden Signals):**
```
1. Latency (–ó–∞–¥–µ—Ä–∂–∫–∞)      - –í—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –∑–∞–ø—Ä–æ—Å—ã
2. Traffic (–¢—Ä–∞—Ñ–∏–∫)        - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø—Ä–æ—Å–æ–≤
3. Errors (–û—à–∏–±–∫–∏)         - –ü—Ä–æ—Ü–µ–Ω—Ç –Ω–µ—É–¥–∞—á–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
4. Saturation (–ù–∞—Å—ã—â–µ–Ω–∏–µ)  - –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤ (CPU, –ø–∞–º—è—Ç—å, –¥–∏—Å–∫)
```

**–¢–∏–ø—ã –º–µ—Ç—Ä–∏–∫:**
```
Counter   - –ú–æ–Ω–æ—Ç–æ–Ω–Ω–æ –≤–æ–∑—Ä–∞—Å—Ç–∞—é—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ (–∑–∞–ø—Ä–æ—Å—ã, –æ—à–∏–±–∫–∏)
Gauge     - –¢–µ–∫—É—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ (CPU, –ø–∞–º—è—Ç—å, —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞)
Histogram - –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π (latency buckets)
Summary   - –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∑–∞ –ø–µ—Ä–∏–æ–¥ (percentiles)
```

**USE Method (–¥–ª—è —Ä–µ—Å—É—Ä—Å–æ–≤):**
```
Utilization - –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –∑–∞–Ω—è—Ç–æ—Å—Ç–∏ —Ä–µ—Å—É—Ä—Å–∞
Saturation  - –°—Ç–µ–ø–µ–Ω—å –ø–µ—Ä–µ–≥—Ä—É–∑–∫–∏
Errors      - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—à–∏–±–æ–∫
```

**RED Method (–¥–ª—è —Å–µ—Ä–≤–∏—Å–æ–≤):**
```
Rate     - –ó–∞–ø—Ä–æ—Å–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É
Errors   - –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—à–∏–±–æ–∫
Duration - –í—Ä–µ–º—è –æ—Ç–≤–µ—Ç–∞
```

**–£—Ä–æ–≤–Ω–∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Application (APM)             ‚îÇ  - –ö–æ–¥, —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ   Service/Container             ‚îÇ  - Docker, K8s
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ   Operating System              ‚îÇ  - CPU, RAM, Disk
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ   Infrastructure                ‚îÇ  - Network, Hardware
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**–ö–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ Linux:**
```bash
# CPU
top, htop
mpstat -P ALL 1

# Memory
free -m
vmstat 1

# Disk I/O
iostat -x 1
iotop

# Network
iftop
nethogs
ss -s

# Process
ps aux --sort=-%mem | head
ps aux --sort=-%cpu | head
```

**–ú–µ—Ç—Ä–∏–∫–∏ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π:**
```
- Request rate (req/s)
- Error rate (%)
- Response time (ms) - p50, p95, p99
- Active connections
- Queue depth
- Database query time
- Cache hit ratio
```

### üíª –ó–∞–¥–∞–Ω–∏–µ

–ù–∞—Å—Ç—Ä–æ–π –±–∞–∑–æ–≤—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Å–∏—Å—Ç–µ–º—ã:

1. **–£—Å—Ç–∞–Ω–æ–≤–∏ –∏ –∑–∞–ø—É—Å—Ç–∏ Node Exporter** (–¥–ª—è —Å–±–æ—Ä–∞ –º–µ—Ç—Ä–∏–∫ —Ö–æ—Å—Ç–∞):
```bash
# –ß–µ—Ä–µ–∑ Docker
docker run -d \
  --name node-exporter \
  --net="host" \
  --pid="host" \
  -v "/:/host:ro,rslave" \
  prom/node-exporter:latest \
  --path.rootfs=/host

# –ü—Ä–æ–≤–µ—Ä–∫–∞
curl http://localhost:9100/metrics | head -20
```

2. **–ò–∑—É—á–∏ –æ—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏**:
```bash
# CPU
curl -s http://localhost:9100/metrics | grep node_cpu_seconds_total

# Memory
curl -s http://localhost:9100/metrics | grep node_memory

# Disk
curl -s http://localhost:9100/metrics | grep node_disk

# Network
curl -s http://localhost:9100/metrics | grep node_network
```

3. **–°–æ–∑–¥–∞–π –ø—Ä–æ—Å—Ç–æ–π bash —Å–∫—Ä–∏–ø—Ç** –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ (`monitor.sh`):
```bash
#!/bin/bash

echo "=== System Monitoring Report ==="
echo "Date: $(date)"
echo ""

# CPU Usage
echo "CPU Usage:"
top -bn1 | grep "Cpu(s)" | awk '{print "  User: " $2 ", System: " $4 ", Idle: " $8}'

# Memory Usage
echo ""
echo "Memory Usage:"
free -h | awk 'NR==2{printf "  Total: %s, Used: %s (%.2f%%)\n", $2, $3, $3*100/$2}'

# Disk Usage
echo ""
echo "Disk Usage:"
df -h / | awk 'NR==2{printf "  Total: %s, Used: %s (%s)\n", $2, $3, $5}'

# Load Average
echo ""
echo "Load Average:"
uptime | awk -F'load average:' '{print "  " $2}'

# Top 5 processes by CPU
echo ""
echo "Top 5 processes by CPU:"
ps aux --sort=-%cpu | head -6 | tail -5 | awk '{printf "  %s: %.1f%%\n", $11, $3}'

# Top 5 processes by Memory
echo ""
echo "Top 5 processes by Memory:"
ps aux --sort=-%mem | head -6 | tail -5 | awk '{printf "  %s: %.1f%%\n", $11, $4}'
```

4. –ó–∞–ø—É—Å—Ç–∏ —Å–∫—Ä–∏–ø—Ç:
```bash
chmod +x monitor.sh
./monitor.sh
```

### üöÄ –ë–æ–Ω—É—Å (–Ω–æ–≤–æ–µ)

**–ù–∞—Å—Ç—Ä–æ–π cAdvisor** –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ Docker –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤:
```bash
docker run -d \
  --name=cadvisor \
  --volume=/:/rootfs:ro \
  --volume=/var/run:/var/run:ro \
  --volume=/sys:/sys:ro \
  --volume=/var/lib/docker/:/var/lib/docker:ro \
  --publish=8080:8080 \
  --detach=true \
  gcr.io/cadvisor/cadvisor:latest

# –û—Ç–∫—Ä–æ–π –≤ –±—Ä–∞—É–∑–µ—Ä–µ
http://localhost:8080
```

**–°–æ–∑–¥–∞–π —Å–≤–æ–π custom exporter** –Ω–∞ Python:
```python
# custom_exporter.py
from prometheus_client import start_http_server, Gauge, Counter
import time
import random

# –°–æ–∑–¥–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
request_gauge = Gauge('app_requests_in_progress', 'Number of requests in progress')
request_counter = Counter('app_requests_total', 'Total number of requests')
error_counter = Counter('app_errors_total', 'Total number of errors')

def process_request():
    """–°–∏–º—É–ª–∏—Ä—É–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É –∑–∞–ø—Ä–æ—Å–∞"""
    request_gauge.inc()
    request_counter.inc()
    
    # –°–ª—É—á–∞–π–Ω–∞—è –æ—à–∏–±–∫–∞
    if random.random() < 0.1:
        error_counter.inc()
    
    time.sleep(random.uniform(0.1, 0.5))
    request_gauge.dec()

if __name__ == '__main__':
    start_http_server(8000)
    print("Exporter started on port 8000")
    
    while True:
        process_request()
        time.sleep(random.uniform(0.5, 2))
```

---

## –ú–æ–¥—É–ª—å 2: Prometheus - —Å–±–æ—Ä –∏ —Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ (30 –º–∏–Ω—É—Ç)

### üéØ –ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞

**–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ Prometheus:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Targets   ‚îÇ ‚Üê HTTP Pull (scrape)
‚îÇ  (Metrics)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
   ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ Prom-  ‚îÇ
   ‚îÇ etheus ‚îÇ ‚Üê Time Series DB (TSDB)
   ‚îÇ Server ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
   ‚îå‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ Alert- ‚îÇ
   ‚îÇ manager‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Prometheus config structure:**
```yaml
global:
  scrape_interval: 15s      # –ö–∞–∫ —á–∞—Å—Ç–æ —Å–æ–±–∏—Ä–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏
  evaluation_interval: 15s  # –ö–∞–∫ —á–∞—Å—Ç–æ –ø—Ä–æ–≤–µ—Ä—è—Ç—å –ø—Ä–∞–≤–∏–ª–∞

scrape_configs:
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']
```

**PromQL –æ—Å–Ω–æ–≤—ã:**
```promql
# Instant vector - —Ç–µ–∫—É—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
node_cpu_seconds_total

# Range vector - –∑–Ω–∞—á–µ–Ω–∏—è –∑–∞ –ø–µ—Ä–∏–æ–¥
node_cpu_seconds_total[5m]

# –§–∏–ª—å—Ç—Ä—ã
node_cpu_seconds_total{mode="idle"}
node_cpu_seconds_total{mode!="idle"}
node_cpu_seconds_total{mode=~"user|system"}

# –ê–≥—Ä–µ–≥–∞—Ü–∏—è
sum(node_cpu_seconds_total)
avg(node_cpu_seconds_total)
max(node_cpu_seconds_total)
min(node_cpu_seconds_total)
count(node_cpu_seconds_total)

# –ü–æ label
sum(node_cpu_seconds_total) by (mode)
sum(node_cpu_seconds_total) by (cpu)

# –§—É–Ω–∫—Ü–∏–∏
rate(node_cpu_seconds_total[5m])           # –°–∫–æ—Ä–æ—Å—Ç—å –∏–∑–º–µ–Ω–µ–Ω–∏—è
irate(node_cpu_seconds_total[5m])          # –ú–≥–Ω–æ–≤–µ–Ω–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å
increase(node_cpu_seconds_total[5m])       # –£–≤–µ–ª–∏—á–µ–Ω–∏–µ –∑–∞ –ø–µ—Ä–∏–æ–¥
delta(node_cpu_seconds_total[5m])          # –ò–∑–º–µ–Ω–µ–Ω–∏–µ
```

**–†–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã:**
```promql
# CPU utilization
100 - (avg by (instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)

# Memory usage %
(1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100

# Disk usage %
100 - ((node_filesystem_avail_bytes / node_filesystem_size_bytes) * 100)

# Network traffic
rate(node_network_receive_bytes_total[5m])
rate(node_network_transmit_bytes_total[5m])

# HTTP request rate
rate(http_requests_total[5m])

# Error rate
rate(http_requests_total{status=~"5.."}[5m]) / rate(http_requests_total[5m])

# Latency percentiles (–¥–ª—è histogram)
histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))
histogram_quantile(0.99, rate(http_request_duration_seconds_bucket[5m]))
```

**Metric types –≤ –¥–µ—Ç–∞–ª—è—Ö:**
```promql
# Counter - —Ç–æ–ª—å–∫–æ —Ä–∞—Å—Ç–µ—Ç
http_requests_total
# –ò—Å–ø–æ–ª—å–∑—É–π rate() –∏–ª–∏ increase()
rate(http_requests_total[5m])

# Gauge - –º–æ–∂–µ—Ç —Ä–∞—Å—Ç–∏ –∏ –ø–∞–¥–∞—Ç—å
node_memory_MemAvailable_bytes
# –ò—Å–ø–æ–ª—å–∑—É–π –Ω–∞–ø—Ä—è–º—É—é –∏–ª–∏ —Å —Ñ—É–Ω–∫—Ü–∏—è–º–∏ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏
avg(node_memory_MemAvailable_bytes)

# Histogram - —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π
http_request_duration_seconds_bucket
http_request_duration_seconds_sum
http_request_duration_seconds_count
# –ò—Å–ø–æ–ª—å–∑—É–π histogram_quantile()
histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))

# Summary - –ø—Ä–µ–¥—Ä–∞—Å—á–∏—Ç–∞–Ω–Ω—ã–µ –∫–≤–∞–Ω—Ç–∏–ª–∏
http_request_duration_seconds{quantile="0.95"}
```

**Recording rules** (–¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏):
```yaml
groups:
  - name: example
    interval: 30s
    rules:
    - record: job:node_cpu_utilization:avg
      expr: 100 - (avg by (job) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)
```

**Alerting rules:**
```yaml
groups:
  - name: alerts
    rules:
    - alert: HighCPUUsage
      expr: job:node_cpu_utilization:avg > 80
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "High CPU usage on {{ $labels.instance }}"
        description: "CPU usage is {{ $value }}%"
```

### üíª –ó–∞–¥–∞–Ω–∏–µ

–ù–∞—Å—Ç—Ä–æ–π –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π Prometheus:

1. **–°–æ–∑–¥–∞–π docker-compose.yml**:
```yaml
version: '3.8'

services:
  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml
      - ./alerts.yml:/etc/prometheus/alerts.yml
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--web.enable-lifecycle'
    restart: unless-stopped

  node-exporter:
    image: prom/node-exporter:latest
    container_name: node-exporter
    ports:
      - "9100:9100"
    command:
      - '--path.rootfs=/host'
    pid: host
    restart: unless-stopped
    volumes:
      - '/:/host:ro,rslave'

  cadvisor:
    image: gcr.io/cadvisor/cadvisor:latest
    container_name: cadvisor
    ports:
      - "8080:8080"
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
      - /dev/disk/:/dev/disk:ro
    privileged: true
    restart: unless-stopped

volumes:
  prometheus-data:
```

2. **–°–æ–∑–¥–∞–π prometheus.yml**:
```yaml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–∞–≤–∏–ª –∞–ª–µ—Ä—Ç–æ–≤
rule_files:
  - "alerts.yml"

scrape_configs:
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  - job_name: 'node-exporter'
    static_configs:
      - targets: ['node-exporter:9100']

  - job_name: 'cadvisor'
    static_configs:
      - targets: ['cadvisor:8080']
```

3. **–°–æ–∑–¥–∞–π alerts.yml**:
```yaml
groups:
  - name: system_alerts
    rules:
    - alert: HighCPUUsage
      expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "High CPU usage detected"
        description: "CPU usage is above 80% (current value: {{ $value }}%)"

    - alert: HighMemoryUsage
      expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 90
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "High memory usage detected"
        description: "Memory usage is above 90% (current value: {{ $value }}%)"

    - alert: DiskSpaceLow
      expr: (1 - (node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"})) * 100 > 85
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Low disk space"
        description: "Disk usage is above 85% (current value: {{ $value }}%)"

    - alert: InstanceDown
      expr: up == 0
      for: 1m
      labels:
        severity: critical
      annotations:
        summary: "Instance {{ $labels.instance }} down"
        description: "{{ $labels.instance }} has been down for more than 1 minute"
```

4. **–ó–∞–ø—É—Å—Ç–∏ stack**:
```bash
docker-compose up -d

# –ü—Ä–æ–≤–µ—Ä–∫–∞
docker-compose ps
curl http://localhost:9090/api/v1/targets
```

5. **–û—Ç–∫—Ä–æ–π Prometheus UI** –∏ –ø–æ–ø—Ä–æ–±—É–π –∑–∞–ø—Ä–æ—Å—ã:
```
–ü–µ—Ä–µ–π–¥–∏: http://localhost:9090

–ü–æ–ø—Ä–æ–±—É–π –∑–∞–ø—Ä–æ—Å—ã:
- node_cpu_seconds_total
- rate(node_cpu_seconds_total[5m])
- 100 - (avg(irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)
- node_memory_MemAvailable_bytes / 1024 / 1024 / 1024
```

### üöÄ –ë–æ–Ω—É—Å (–Ω–æ–≤–æ–µ)

**–ù–∞—Å—Ç—Ä–æ–π Service Discovery** –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è —Ü–µ–ª–µ–π:

**File-based SD** (`file_sd.json`):
```json
[
  {
    "targets": ["node-exporter:9100"],
    "labels": {
      "job": "node",
      "env": "production"
    }
  },
  {
    "targets": ["cadvisor:8080"],
    "labels": {
      "job": "containers",
      "env": "production"
    }
  }
]
```

–î–æ–±–∞–≤—å –≤ `prometheus.yml`:
```yaml
scrape_configs:
  - job_name: 'dynamic-targets'
    file_sd_configs:
      - files:
        - '/etc/prometheus/file_sd.json'
        refresh_interval: 30s
```

**–ù–∞—Å—Ç—Ä–æ–π Pushgateway** –¥–ª—è –º–µ—Ç—Ä–∏–∫ batch jobs:
```bash
docker run -d \
  --name pushgateway \
  -p 9091:9091 \
  prom/pushgateway

# Push –º–µ—Ç—Ä–∏–∫—É
echo "backup_duration_seconds 125.5" | curl --data-binary @- http://localhost:9091/metrics/job/backup/instance/db1

# –î–æ–±–∞–≤—å –≤ prometheus.yml
scrape_configs:
  - job_name: 'pushgateway'
    static_configs:
      - targets: ['pushgateway:9091']
    honor_labels: true
```

**Recording rules –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏**:
```yaml
# recording_rules.yml
groups:
  - name: performance_rules
    interval: 30s
    rules:
    # CPU utilization per instance
    - record: instance:node_cpu_utilization:rate5m
      expr: 100 - (avg by (instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)
    
    # Memory utilization per instance
    - record: instance:node_memory_utilization:ratio
      expr: 1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)
    
    # Request rate per job
    - record: job:http_requests:rate5m
      expr: sum(rate(http_requests_total[5m])) by (job)
```

---

## –ú–æ–¥—É–ª—å 3: Grafana - –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö (30 –º–∏–Ω—É—Ç)

### üéØ –ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞

**–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ Grafana:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Data     ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ Grafana  ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ Users    ‚îÇ
‚îÇ Sources  ‚îÇ      ‚îÇ Server   ‚îÇ      ‚îÇ          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
   ‚îÇ                    ‚îÇ
   ‚îÇ                    ‚îÇ
   ‚ñº                    ‚ñº
Prometheus       Dashboards
InfluxDB         Alerts
Elasticsearch    Users
Loki             Teams
```

**–¢–∏–ø—ã –ø–∞–Ω–µ–ª–µ–π:**
```
Graph        - –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä—è–¥—ã
Stat         - –û–¥–Ω–æ –∑–Ω–∞—á–µ–Ω–∏–µ
Gauge        - –®–∫–∞–ª–∞
Bar Gauge    - –ì–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω—ã–µ –ø–æ–ª–æ—Å–∫–∏
Table        - –¢–∞–±–ª–∏—Ü–∞
Heatmap      - –¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞
Logs         - –õ–æ–≥–∏
```

**–ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–∞—à–±–æ—Ä–¥–∞:**
```
Query      - –ò–∑ –¥–∞–Ω–Ω—ã—Ö (label_values(metric, label))
Custom     - –°–ø–∏—Å–æ–∫ –∑–Ω–∞—á–µ–Ω–∏–π
Constant   - –ö–æ–Ω—Å—Ç–∞–Ω—Ç–∞
Interval   - –í—Ä–µ–º–µ–Ω–Ω–æ–π –∏–Ω—Ç–µ—Ä–≤–∞–ª
Data source - –í—ã–±–æ—Ä –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö
```

**–ü–æ–ª–µ–∑–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ Grafana:**
```
$__interval        - –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–π –∏–Ω—Ç–µ—Ä–≤–∞–ª
$__rate_interval   - –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª –¥–ª—è rate()
$timeFilter        - –í—Ä–µ–º–µ–Ω–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä
$__from / $__to    - –ù–∞—á–∞–ª–æ/–∫–æ–Ω–µ—Ü –ø–µ—Ä–∏–æ–¥–∞

# –ü—Ä–∏–º–µ—Ä —Å –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
rate(http_requests_total{job="$job"}[$__rate_interval])
```

**Templating examples:**
```promql
# –ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è instance
label_values(node_cpu_seconds_total, instance)

# –ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è job
label_values(up, job)

# –ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è mountpoint
label_values(node_filesystem_size_bytes, mountpoint)

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤ –∑–∞–ø—Ä–æ—Å–µ
node_filesystem_avail_bytes{instance="$instance", mountpoint="$mountpoint"}
```

**Alert channels:**
```
Email
Slack
PagerDuty
Webhook
Telegram
Discord
Teams
OpsGenie
```

**Dashboard best practices:**
```
1. –ò—Å–ø–æ–ª—å–∑—É–π Row –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏ –ø–∞–Ω–µ–ª–µ–π
2. –î–æ–±–∞–≤–ª—è–π –æ–ø–∏—Å–∞–Ω–∏—è –∫ –ø–∞–Ω–µ–ª—è–º
3. –ò—Å–ø–æ–ª—å–∑—É–π –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –≥–∏–±–∫–æ—Å—Ç–∏
4. –£–∫–∞–∑—ã–≤–∞–π –µ–¥–∏–Ω–∏—Ü—ã –∏–∑–º–µ—Ä–µ–Ω–∏—è
5. –ò—Å–ø–æ–ª—å–∑—É–π —Ü–≤–µ—Ç–æ–≤—ã–µ –ø–æ—Ä–æ–≥–∏
6. –î–æ–±–∞–≤–ª—è–π —Å—Å—ã–ª–∫–∏ –Ω–∞ runbook'–∏
7. –ì—Ä—É–ø–ø–∏—Ä—É–π —Å–≤—è–∑–∞–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
8. –ò—Å–ø–æ–ª—å–∑—É–π consistent naming
```

### üíª –ó–∞–¥–∞–Ω–∏–µ

–ù–∞—Å—Ç—Ä–æ–π Grafana –∏ —Å–æ–∑–¥–∞–π dashboard:

1. **–î–æ–±–∞–≤—å Grafana –≤ docker-compose.yml**:
```yaml
  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - grafana-data:/var/lib/grafana
      - ./grafana/provisioning:/etc/grafana/provisioning
    restart: unless-stopped
    depends_on:
      - prometheus

volumes:
  grafana-data:
```

2. **–°–æ–∑–¥–∞–π provisioning –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏** (`grafana/provisioning/datasources/prometheus.yml`):
```yaml
apiVersion: 1

datasources:
  - name: Prometheus
    type: prometheus
    access: proxy
    url: http://prometheus:9090
    isDefault: true
    editable: true
```

3. **–°–æ–∑–¥–∞–π provisioning –¥–ª—è dashboard** (`grafana/provisioning/dashboards/dashboard.yml`):
```yaml
apiVersion: 1

providers:
  - name: 'Default'
    orgId: 1
    folder: ''
    type: file
    disableDeletion: false
    updateIntervalSeconds: 10
    allowUiUpdates: true
    options:
      path: /etc/grafana/provisioning/dashboards
```

4. **–ó–∞–ø—É—Å—Ç–∏ Grafana**:
```bash
# –°–æ–∑–¥–∞–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
mkdir -p grafana/provisioning/datasources
mkdir -p grafana/provisioning/dashboards

docker-compose up -d grafana

# –û—Ç–∫—Ä–æ–π –≤ –±—Ä–∞—É–∑–µ—Ä–µ
http://localhost:3000
# Login: admin
# Password: admin
```

5. **–°–æ–∑–¥–∞–π System Monitoring Dashboard** –≤—Ä—É—á–Ω—É—é:

**Panel 1: CPU Usage**
- Visualization: Time series
- Query: `100 - (avg(irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)`
- Legend: CPU Usage %
- Unit: Percent (0-100)
- Threshold: Yellow at 70, Red at 90

**Panel 2: Memory Usage**
- Visualization: Time series
- Query: `(1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100`
- Legend: Memory Usage %
- Unit: Percent (0-100)

**Panel 3: Disk Usage**
- Visualization: Gauge
- Query: `100 - ((node_filesystem_avail_bytes{mountpoint="/"} / node_filesystem_size_bytes{mountpoint="/"}) * 100)`
- Unit: Percent (0-100)
- Threshold: Green 0-70, Yellow 70-85, Red 85-100

**Panel 4: Network Traffic**
- Visualization: Time series
- Query A: `rate(node_network_receive_bytes_total[5m]) / 1024 / 1024`
- Query B: `rate(node_network_transmit_bytes_total[5m]) / 1024 / 1024`
- Unit: MB/s

**Panel 5: Top Processes by CPU**
- Visualization: Table
- Query: `topk(5, irate(process_cpu_seconds_total[5m]))`

6. **–°–æ–∑–¥–∞–π –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è dashboard**:
- Variable: instance
  - Type: Query
  - Query: `label_values(node_cpu_seconds_total, instance)`
  
–ò–∑–º–µ–Ω–∏ –∑–∞–ø—Ä–æ—Å—ã –Ω–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π:
```promql
100 - (avg(irate(node_cpu_seconds_total{instance="$instance", mode="idle"}[5m])) * 100)
```

### üöÄ –ë–æ–Ω—É—Å (–Ω–æ–≤–æ–µ)

**–°–æ–∑–¥–∞–π JSON dashboard —á–µ—Ä–µ–∑ provisioning** (`grafana/provisioning/dashboards/system-overview.json`):
```json
{
  "dashboard": {
    "title": "System Overview",
    "tags": ["system", "monitoring"],
    "timezone": "browser",
    "panels": [
      {
        "id": 1,
        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 0},
        "type": "timeseries",
        "title": "CPU Usage",
        "targets": [
          {
            "expr": "100 - (avg(irate(node_cpu_seconds_total{instance=\"$instance\",mode=\"idle\"}[5m])) * 100)",
            "legendFormat": "CPU Usage %"
          }
        ],
        "fieldConfig": {
          "defaults": {
            "unit": "percent",
            "thresholds": {
              "steps": [
                {"value": 0, "color": "green"},
                {"value": 70, "color": "yellow"},
                {"value": 90, "color": "red"}
              ]
            }
          }
        }
      }
    ],
    "templating": {
      "list": [
        {
          "name": "instance",
          "type": "query",
          "datasource": "Prometheus",
          "query": "label_values(node_cpu_seconds_total, instance)",
          "refresh": 1
        }
      ]
    }
  }
}
```

**–ù–∞—Å—Ç—Ä–æ–π Alerting –≤ Grafana**:
1. Configuration ‚Üí Alerting ‚Üí Contact points
2. –°–æ–∑–¥–∞–π Email contact point
3. –°–æ–∑–¥–∞–π Alert rule:
   - Name: High CPU Alert
   - Query: `avg(irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100 < 20`
   - Condition: WHEN last() OF query(A) IS BELOW 20
   - For: 5m

**–£—Å—Ç–∞–Ω–æ–≤–∏ Grafana plugins**:
```bash
# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ —á–µ—Ä–µ–∑ UI
Configuration ‚Üí Plugins ‚Üí Search

# –ü–æ–ª–µ–∑–Ω—ã–µ –ø–ª–∞–≥–∏–Ω—ã:
- Pie Chart
- Worldmap Panel
- Clock Panel
- Status Panel

# –ß–µ—Ä–µ–∑ CLI (–≤ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–µ)
docker exec grafana grafana-cli plugins install grafana-piechart-panel
docker restart grafana
```

---

## –ú–æ–¥—É–ª—å 4: –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–∞—Ü–∏—è –ª–æ–≥–æ–≤ (30 –º–∏–Ω—É—Ç)

### üéØ –ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞

**–£—Ä–æ–≤–Ω–∏ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è:**
```
TRACE   - –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
DEBUG   - –û—Ç–ª–∞–¥–æ—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
INFO    - –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
WARN    - –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è
ERROR   - –û—à–∏–±–∫–∏, –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω—ã–µ –¥–ª—è —Ä–∞–±–æ—Ç—ã
FATAL   - –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –æ—à–∏–±–∫–∏, –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –ø–∞–¥–∞–µ—Ç
```

**Structured logging (JSON):**
```json
{
  "timestamp": "2025-01-15T10:30:00Z",
  "level": "ERROR",
  "service": "api",
  "message": "Database connection failed",
  "error": "connection timeout",
  "user_id": "12345",
  "request_id": "abc-123",
  "duration_ms": 5000
}
```

**ELK Stack:**
```
Elasticsearch  - –•—Ä–∞–Ω–µ–Ω–∏–µ –∏ –ø–æ–∏—Å–∫
Logstash       - –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –ø–∞—Ä—Å–∏–Ω–≥
Kibana         - –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
```

**Alternative: Loki Stack:**
```
Loki           - –•—Ä–∞–Ω–µ–Ω–∏–µ –ª–æ–≥–æ–≤
Promtail       - –ê–≥–µ–Ω—Ç —Å–±–æ—Ä–∞
Grafana        - –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
```

**Log aggregation patterns:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   App    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ
                ‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îú‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ Log     ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ Centralized  ‚îÇ
‚îÇ   App    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚î§    ‚îÇ Shipper ‚îÇ    ‚îÇ Log Storage  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îÇ
‚îÇ   App    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**–ü–æ–ª–µ–∑–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã –¥–ª—è –ª–æ–≥–æ–≤:**
```bash
# journalctl (systemd)
journalctl -u nginx                  # –õ–æ–≥–∏ —Å–µ—Ä–≤–∏—Å–∞
journalctl -f                        # Follow –ª–æ–≥–∏
journalctl --since "1 hour ago"
journalctl -p err                    # –¢–æ–ª—å–∫–æ –æ—à–∏–±–∫–∏
journalctl --disk-usage              # –†–∞–∑–º–µ—Ä –ª–æ–≥–æ–≤

# Docker logs
docker logs <container>
docker logs -f <container>
docker logs --tail 100 <container>
docker logs --since 1h <container>

# –ü—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –∫—É—Ä—Å–∞: –ú–æ–¥—É–ª—å 4 - –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ

```bash
# –¢—Ä–∞–¥–∏—Ü–∏–æ–Ω–Ω—ã–µ –ª–æ–≥–∏
tail -f /var/log/syslog
tail -f /var/log/nginx/access.log
grep "ERROR" /var/log/app.log
grep -i "error" /var/log/* | tail -20

# –ê–Ω–∞–ª–∏–∑ –ª–æ–≥–æ–≤
awk '{print $1}' access.log | sort | uniq -c | sort -rn  # Top IP
awk '{print $9}' access.log | sort | uniq -c | sort -rn  # HTTP codes
```

**LogQL (Loki Query Language):**
```logql
# Stream selector
{job="varlogs"}
{job="varlogs", level="error"}

# Line filter
{job="varlogs"} |= "error"
{job="varlogs"} |~ "error|fail"
{job="varlogs"} != "debug"

# JSON parsing
{job="app"} | json | level="error"

# Rate
rate({job="varlogs"}[5m])

# Count
count_over_time({job="varlogs"}[5m])
```

### üíª –ó–∞–¥–∞–Ω–∏–µ

–ù–∞—Å—Ç—Ä–æ–π —Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å Loki:

1. **–î–æ–±–∞–≤—å Loki stack –≤ docker-compose.yml**:
```yaml
  loki:
    image: grafana/loki:latest
    container_name: loki
    ports:
      - "3100:3100"
    volumes:
      - ./loki-config.yml:/etc/loki/local-config.yaml
      - loki-data:/loki
    command: -config.file=/etc/loki/local-config.yaml
    restart: unless-stopped

  promtail:
    image: grafana/promtail:latest
    container_name: promtail
    volumes:
      - ./promtail-config.yml:/etc/promtail/config.yml
      - /var/log:/var/log:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
    command: -config.file=/etc/promtail/config.yml
    restart: unless-stopped
    depends_on:
      - loki

volumes:
  loki-data:
```

2. **–°–æ–∑–¥–∞–π loki-config.yml**:
```yaml
auth_enabled: false

server:
  http_listen_port: 3100

ingester:
  lifecycler:
    ring:
      kvstore:
        store: inmemory
      replication_factor: 1
  chunk_idle_period: 5m
  chunk_retain_period: 30s

schema_config:
  configs:
    - from: 2024-01-01
      store: boltdb-shipper
      object_store: filesystem
      schema: v11
      index:
        prefix: index_
        period: 24h

storage_config:
  boltdb_shipper:
    active_index_directory: /loki/boltdb-shipper-active
    cache_location: /loki/boltdb-shipper-cache
    shared_store: filesystem
  filesystem:
    directory: /loki/chunks

limits_config:
  enforce_metric_name: false
  reject_old_samples: true
  reject_old_samples_max_age: 168h

chunk_store_config:
  max_look_back_period: 0s

table_manager:
  retention_deletes_enabled: false
  retention_period: 0s
```

3. **–°–æ–∑–¥–∞–π promtail-config.yml**:
```yaml
server:
  http_listen_port: 9080
  grpc_listen_port: 0

positions:
  filename: /tmp/positions.yaml

clients:
  - url: http://loki:3100/loki/api/v1/push

scrape_configs:
  # System logs
  - job_name: system
    static_configs:
      - targets:
          - localhost
        labels:
          job: varlogs
          __path__: /var/log/*log

  # Docker containers
  - job_name: containers
    docker_sd_configs:
      - host: unix:///var/run/docker.sock
        refresh_interval: 5s
    relabel_configs:
      - source_labels: ['__meta_docker_container_name']
        regex: '/(.*)'
        target_label: 'container'
      - source_labels: ['__meta_docker_container_log_stream']
        target_label: 'stream'
```

4. **–ó–∞–ø—É—Å—Ç–∏ Loki stack**:
```bash
docker-compose up -d loki promtail

# –ü—Ä–æ–≤–µ—Ä–∫–∞
curl http://localhost:3100/ready
docker logs promtail
```

5. **–î–æ–±–∞–≤—å Loki –≤ Grafana**:
- Configuration ‚Üí Data Sources ‚Üí Add data source
- –í—ã–±–µ—Ä–∏ Loki
- URL: http://loki:3100
- Save & Test

6. **–°–æ–∑–¥–∞–π Log Dashboard –≤ Grafana**:
- Create ‚Üí Dashboard ‚Üí Add new panel
- Data source: Loki
- Query: `{job="varlogs"}`
- Visualization: Logs

**–ü–æ–ø—Ä–æ–±—É–π LogQL –∑–∞–ø—Ä–æ—Å—ã:**
```logql
# –í—Å–µ –ª–æ–≥–∏
{job="varlogs"}

# –¢–æ–ª—å–∫–æ –æ—à–∏–±–∫–∏
{job="varlogs"} |= "error"

# Docker –ª–æ–≥–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞
{container="prometheus"}

# Rate –ª–æ–≥–æ–≤
rate({job="varlogs"}[5m])

# –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ª–æ–≥–æ–≤
count_over_time({job="varlogs"}[1h])

# –¢–æ–ø IP –∞–¥—Ä–µ—Å–æ–≤
topk(10, sum by (ip) (rate({job="nginx"}[5m])))
```

### üöÄ –ë–æ–Ω—É—Å (–Ω–æ–≤–æ–µ)

**–°–æ–∑–¥–∞–π –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ —Å structured logging**:

**app-with-logs.py**:
```python
import logging
import json
import time
from datetime import datetime
from flask import Flask, request
import random

# Structured JSON logging
class JSONFormatter(logging.Formatter):
    def format(self, record):
        log_obj = {
            "timestamp": datetime.utcnow().isoformat(),
            "level": record.levelname,
            "message": record.getMessage(),
            "service": "demo-app",
        }
        if hasattr(record, 'request_id'):
            log_obj['request_id'] = record.request_id
        if hasattr(record, 'user_id'):
            log_obj['user_id'] = record.user_id
        if hasattr(record, 'duration_ms'):
            log_obj['duration_ms'] = record.duration_ms
        return json.dumps(log_obj)

# Setup logging
logger = logging.getLogger()
handler = logging.StreamHandler()
handler.setFormatter(JSONFormatter())
logger.addHandler(handler)
logger.setLevel(logging.INFO)

app = Flask(__name__)

@app.route('/')
def home():
    request_id = f"req-{random.randint(1000, 9999)}"
    start = time.time()
    
    logger.info("Request received", extra={
        'request_id': request_id,
        'path': request.path
    })
    
    # Simulate work
    time.sleep(random.uniform(0.1, 0.5))
    
    # Random error
    if random.random() < 0.1:
        logger.error("Internal error occurred", extra={
            'request_id': request_id,
            'error': "Database connection failed"
        })
        return "Error", 500
    
    duration = (time.time() - start) * 1000
    logger.info("Request completed", extra={
        'request_id': request_id,
        'duration_ms': duration
    })
    
    return "OK", 200

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
```

**Dockerfile**:
```dockerfile
FROM python:3.9-slim
WORKDIR /app
RUN pip install flask
COPY app-with-logs.py .
CMD ["python", "app-with-logs.py"]
```

**–î–æ–±–∞–≤—å –≤ docker-compose.yml**:
```yaml
  demo-app:
    build: ./demo-app
    container_name: demo-app
    ports:
      - "5000:5000"
    restart: unless-stopped
```

**–ì–µ–Ω–µ—Ä–∏—Ä—É–π —Ç—Ä–∞—Ñ–∏–∫**:
```bash
# Install hey (HTTP load generator)
# –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π curl –≤ —Ü–∏–∫–ª–µ
while true; do curl http://localhost:5000; sleep 0.5; done
```

**–ê–Ω–∞–ª–∏–∑–∏—Ä—É–π –ª–æ–≥–∏ –≤ Grafana**:
```logql
# Error rate
sum(rate({container="demo-app"} |= "ERROR" [5m]))

# Average duration
avg_over_time({container="demo-app"} | json | unwrap duration_ms [5m])

# Request count by level
sum by (level) (count_over_time({container="demo-app"} [5m]))
```

---

## –ú–æ–¥—É–ª—å 5: –ê–ª–µ—Ä—Ç–∏–Ω–≥ –∏ On-Call (25 –º–∏–Ω—É—Ç)

### üéØ –ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞

**Alertmanager –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Prometheus ‚îÇ‚îÄ‚îÄ‚îê
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
                ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îú‚îÄ‚ñ∫‚îÇ Alertmanager ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ Receivers‚îÇ
‚îÇ Grafana    ‚îÇ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ                    ‚îÇ
                         ‚îÇ              Email, Slack
                         ‚îÇ              PagerDuty
                         ‚ñº              Webhook
                    Grouping
                    Throttling
                    Silencing
```

**Alert states:**
```
Inactive  - –£—Å–ª–æ–≤–∏–µ –Ω–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ
Pending   - –£—Å–ª–æ–≤–∏–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ, –Ω–æ –º–µ–Ω—å—à–µ "for:"
Firing    - –£—Å–ª–æ–≤–∏–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–æ –¥–æ–ª—å—à–µ "for:"
```

**Alert best practices:**
```
1. Alert –Ω–∞ —Å–∏–º–ø—Ç–æ–º—ã, –Ω–µ –Ω–∞ –ø—Ä–∏—á–∏–Ω—ã
2. –ö–∞–∂–¥—ã–π alert –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å actionable
3. –í–∫–ª—é—á–∞–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤ annotations
4. –ò—Å–ø–æ–ª—å–∑—É–π severity levels
5. –î–æ–∫—É–º–µ–Ω—Ç–∏—Ä—É–π runbook
6. –¢–µ—Å—Ç–∏—Ä—É–π alerts
7. –ò–∑–±–µ–≥–∞–π alert fatigue
```

**Severity levels:**
```
critical  - –ù—É–∂–Ω–æ –Ω–µ–º–µ–¥–ª–µ–Ω–Ω–æ–µ –¥–µ–π—Å—Ç–≤–∏–µ (pager)
warning   - –ù—É–∂–Ω–æ –¥–µ–π—Å—Ç–≤–∏–µ –≤ —Ä–∞–±–æ—á–µ–µ –≤—Ä–µ–º—è
info      - –î–ª—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –¥–µ–π—Å—Ç–≤–∏–µ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è
```

**Good vs Bad alerts:**
```
‚úÖ GOOD:
- "API error rate > 5% for 5 minutes"
- "Disk will be full in 4 hours"
- "Payment processing failed"

‚ùå BAD:
- "CPU usage > 80%"  (–Ω–µ actionable)
- "Log contains ERROR"  (—Å–ª–∏—à–∫–æ–º —à—É–º–Ω–æ)
- "Service restarted"  (–µ—Å–ª–∏ —ç—Ç–æ –Ω–æ—Ä–º–∞)
```

**Alertmanager routing:**
```yaml
route:
  group_by: ['alertname', 'cluster']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 12h
  receiver: 'default'
  routes:
    - match:
        severity: critical
      receiver: 'pagerduty'
    - match:
        severity: warning
      receiver: 'slack'
```

### üíª –ó–∞–¥–∞–Ω–∏–µ

–ù–∞—Å—Ç—Ä–æ–π –ø–æ–ª–Ω–æ—Ü–µ–Ω–Ω—ã–π Alertmanager:

1. **–î–æ–±–∞–≤—å Alertmanager –≤ docker-compose.yml**:
```yaml
  alertmanager:
    image: prom/alertmanager:latest
    container_name: alertmanager
    ports:
      - "9093:9093"
    volumes:
      - ./alertmanager.yml:/etc/alertmanager/alertmanager.yml
      - alertmanager-data:/alertmanager
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
    restart: unless-stopped

volumes:
  alertmanager-data:
```

2. **–°–æ–∑–¥–∞–π alertmanager.yml**:
```yaml
global:
  resolve_timeout: 5m
  slack_api_url: 'YOUR_SLACK_WEBHOOK_URL'

route:
  group_by: ['alertname', 'cluster', 'service']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 12h
  receiver: 'default'
  routes:
    # Critical alerts
    - match:
        severity: critical
      receiver: 'critical-alerts'
      continue: true
    
    # Warning alerts
    - match:
        severity: warning
      receiver: 'warning-alerts'
      continue: true

receivers:
  - name: 'default'
    webhook_configs:
      - url: 'http://localhost:5001/webhook'
        send_resolved: true

  - name: 'critical-alerts'
    slack_configs:
      - channel: '#alerts-critical'
        title: 'üö® Critical Alert'
        text: "{{ range .Alerts }}{{ .Annotations.summary }}\n{{ .Annotations.description }}\n{{ end }}"
        send_resolved: true

  - name: 'warning-alerts'
    slack_configs:
      - channel: '#alerts-warning'
        title: '‚ö†Ô∏è Warning Alert'
        text: "{{ range .Alerts }}{{ .Annotations.summary }}\n{{ .Annotations.description }}\n{{ end }}"
        send_resolved: true

inhibit_rules:
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['alertname', 'instance']
```

3. **–û–±–Ω–æ–≤–∏ prometheus.yml**:
```yaml
# –î–æ–±–∞–≤—å –≤ —Å–µ–∫—Ü–∏—é alerting
alerting:
  alertmanagers:
    - static_configs:
        - targets: ['alertmanager:9093']
```

4. **–°–æ–∑–¥–∞–π –±–æ–ª–µ–µ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ alerts** (–æ–±–Ω–æ–≤–∏ `alerts.yml`):
```yaml
groups:
  - name: slo_alerts
    rules:
    # SLO: 99.9% availability
    - alert: HighErrorRate
      expr: |
        (
          sum(rate(http_requests_total{status=~"5.."}[5m]))
          /
          sum(rate(http_requests_total[5m]))
        ) > 0.001
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "High error rate detected"
        description: "Error rate is {{ $value | humanizePercentage }} (threshold: 0.1%)"
        runbook: "https://runbook.example.com/high-error-rate"

    # SLO: p99 latency < 500ms
    - alert: HighLatency
      expr: |
        histogram_quantile(0.99,
          rate(http_request_duration_seconds_bucket[5m])
        ) > 0.5
      for: 10m
      labels:
        severity: warning
      annotations:
        summary: "High latency detected"
        description: "p99 latency is {{ $value }}s (threshold: 0.5s)"

  - name: capacity_alerts
    rules:
    # Predict disk full
    - alert: DiskWillFillIn4Hours
      expr: |
        predict_linear(
          node_filesystem_avail_bytes{mountpoint="/"}[1h], 
          4 * 3600
        ) < 0
      for: 5m
      labels:
        severity: critical
      annotations:
        summary: "Disk will be full soon"
        description: "Disk on {{ $labels.instance }} will be full in ~4 hours"
        action: "Clean up logs or expand disk"

    # Memory pressure
    - alert: HighMemoryPressure
      expr: |
        (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) > 0.95
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "High memory pressure"
        description: "Memory usage is {{ $value | humanizePercentage }} on {{ $labels.instance }}"

  - name: service_alerts
    rules:
    # Service down
    - alert: ServiceDown
      expr: up == 0
      for: 1m
      labels:
        severity: critical
      annotations:
        summary: "Service {{ $labels.job }} is down"
        description: "{{ $labels.instance }} has been down for more than 1 minute"
        action: "Check service status and logs"

    # Container restarting
    - alert: ContainerRestarting
      expr: |
        rate(container_last_seen{name!=""}[5m]) > 0
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Container {{ $labels.name }} is restarting"
        description: "Container has restarted {{ $value }} times in the last 5 minutes"
```

5. **–ó–∞–ø—É—Å—Ç–∏ –∏ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–π**:
```bash
docker-compose up -d alertmanager

# Reload Prometheus config
curl -X POST http://localhost:9090/-/reload

# –ü—Ä–æ–≤–µ—Ä—å Alertmanager UI
http://localhost:9093

# –ü—Ä–æ–≤–µ—Ä—å alerts –≤ Prometheus
http://localhost:9090/alerts
```

6. **–°–æ–∑–¥–∞–π webhook receiver –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è** (`webhook_receiver.py`):
```python
from flask import Flask, request
import json

app = Flask(__name__)

@app.route('/webhook', methods=['POST'])
def webhook():
    data = request.json
    print("=" * 50)
    print("Alert received:")
    print(json.dumps(data, indent=2))
    print("=" * 50)
    
    for alert in data.get('alerts', []):
        status = alert['status']
        labels = alert['labels']
        annotations = alert['annotations']
        
        print(f"\nStatus: {status}")
        print(f"Alert: {labels.get('alertname')}")
        print(f"Severity: {labels.get('severity')}")
        print(f"Summary: {annotations.get('summary')}")
        print(f"Description: {annotations.get('description')}")
    
    return "OK", 200

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5001)
```

### üöÄ –ë–æ–Ω—É—Å (–Ω–æ–≤–æ–µ)

**–ù–∞—Å—Ç—Ä–æ–π Silences –∏ Inhibit rules**:

**–°–æ–∑–¥–∞–π silence —á–µ—Ä–µ–∑ API**:
```bash
# Silence alert –Ω–∞ 2 —á–∞—Å–∞
curl -XPOST http://localhost:9093/api/v2/silences \
  -H "Content-Type: application/json" \
  -d '{
    "matchers": [
      {
        "name": "alertname",
        "value": "HighCPUUsage",
        "isRegex": false
      }
    ],
    "startsAt": "2025-01-15T10:00:00Z",
    "endsAt": "2025-01-15T12:00:00Z",
    "createdBy": "admin",
    "comment": "Maintenance window"
  }'
```

**–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ inhibit rules**:
```yaml
inhibit_rules:
  # –ï—Å–ª–∏ —Å–µ—Ä–≤–∏—Å down, –Ω–µ –∞–ª–µ—Ä—Ç–∏—Ç—å –ø—Ä–æ –µ–≥–æ –º–µ—Ç—Ä–∏–∫–∏
  - source_match:
      alertname: 'ServiceDown'
    target_match_re:
      alertname: '.*'
    equal: ['instance']
  
  # –ï—Å–ª–∏ critical alert firing, –ø–æ–¥–∞–≤–∏—Ç—å warning
  - source_match:
      severity: 'critical'
    target_match:
      severity: 'warning'
    equal: ['alertname', 'cluster', 'service']
```

**–ù–∞—Å—Ç—Ä–æ–π PagerDuty integration**:
```yaml
receivers:
  - name: 'pagerduty'
    pagerduty_configs:
      - service_key: 'YOUR_PAGERDUTY_SERVICE_KEY'
        description: '{{ .GroupLabels.alertname }}: {{ .CommonAnnotations.summary }}'
        details:
          firing: '{{ .Alerts.Firing | len }}'
          resolved: '{{ .Alerts.Resolved | len }}'
```

---

## –ú–æ–¥—É–ª—å 6: –°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏ Uptime (20 –º–∏–Ω—É—Ç)

### üéØ –ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞

**Synthetic monitoring** - –ø—Ä–æ–∞–∫—Ç–∏–≤–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:
```
Real User Monitoring (RUM)  - –†–µ–∞–ª—å–Ω—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏
Synthetic Monitoring        - –†–æ–±–æ—Ç—ã/–±–æ—Ç—ã –ø—Ä–æ–≤–µ—Ä—è—é—Ç
```

**–¢–∏–ø—ã –ø—Ä–æ–≤–µ—Ä–æ–∫:**
```
- HTTP/HTTPS endpoints
- SSL certificate expiration
- DNS resolution
- TCP port availability
- API response validation
- Multi-step transactions
- Browser scenarios (Selenium)
```

**Blackbox Exporter** - –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ endpoints:
```
Protocols:
- HTTP/HTTPS
- TCP
- ICMP (ping)
- DNS
- GRPC
```

### üíª –ó–∞–¥–∞–Ω–∏–µ

–ù–∞—Å—Ç—Ä–æ–π —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥:

1. **–î–æ–±–∞–≤—å Blackbox Exporter –≤ docker-compose.yml**:
```yaml
  blackbox-exporter:
    image: prom/blackbox-exporter:latest
    container_name: blackbox-exporter
    ports:
      - "9115:9115"
    volumes:
      - ./blackbox.yml:/etc/blackbox_exporter/config.yml
    command:
      - '--config.file=/etc/blackbox_exporter/config.yml'
    restart: unless-stopped
```

2. **–°–æ–∑–¥–∞–π blackbox.yml**:
```yaml
modules:
  http_2xx:
    prober: http
    timeout: 5s
    http:
      valid_status_codes: [200]
      method: GET
      follow_redirects: true
      preferred_ip_protocol: "ip4"

  http_post_2xx:
    prober: http
    timeout: 5s
    http:
      method: POST
      headers:
        Content-Type: application/json
      body: '{"test": "data"}'

  tcp_connect:
    prober: tcp
    timeout: 5s

  icmp:
    prober: icmp
    timeout: 5s
    icmp:
      preferred_ip_protocol: "ip4"

  dns_example:
    prober: dns
    timeout: 5s
    dns:
      query_name: "example.com"
      query_type: "A"

  ssl_expire:
    prober: http
    timeout: 5s
    http:
      method: GET
      tls_config:
        insecure_skip_verify: false
```

3. **–î–æ–±–∞–≤—å –≤ prometheus.yml**:
```yaml
scrape_configs:
  - job_name: 'blackbox'
    metrics_path: /probe
    params:
      module: [http_2xx]
    static_configs:
      - targets:
        - https://google.com
        - https://github.com
        - http://localhost:3000  # Grafana
        - http://localhost:9090  # Prometheus
    relabel_configs:
      - source_labels: [__address__]
        target_label: __param_target
      - source_labels: [__param_target]
        target_label: instance
      - target_label: __address__
        replacement: blackbox-exporter:9115

  - job_name: 'blackbox-ssl'
    metrics_path: /probe
    params:
      module: [ssl_expire]
    static_configs:
      - targets:
        - https://google.com
        - https://github.com
    relabel_configs:
      - source_labels: [__address__]
        target_label: __param_target
      - source_labels: [__param_target]
        target_label: instance
      - target_label: __address__
        replacement: blackbox-exporter:9115
```

4. **–°–æ–∑–¥–∞–π uptime alerts** (–¥–æ–±–∞–≤—å –≤ `alerts.yml`):
```yaml
groups:
  - name: uptime_alerts
    rules:
    # Endpoint down
    - alert: EndpointDown
      expr: probe_success == 0
      for: 2m
      labels:
        severity: critical
      annotations:
        summary: "Endpoint {{ $labels.instance }} is down"
        description: "Endpoint has been down for more than 2 minutes"

    # SSL certificate expires soon
    - alert: SSLCertExpiringSoon
      expr: (probe_ssl_earliest_cert_expiry - time()) / 86400 < 30
      for: 1h
      labels:
        severity: warning
      annotations:
        summary: "SSL certificate expiring soon"
        description: "SSL certificate for {{ $labels.instance }} expires in {{ $value }} days"

    # Slow response
    - alert: SlowResponse
      expr: probe_duration_seconds > 5
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Slow response from {{ $labels.instance }}"
        description: "Response time is {{ $value }}s (threshold: 5s)"

    # HTTP status code error
    - alert: HTTPStatusError
      expr: probe_http_status_code >= 400
      for: 2m
      labels:
        severity: critical
      annotations:
        summary: "HTTP error on {{ $labels.instance }}"
        description: "HTTP status code is {{ $value }}"
```

5. **–ó–∞–ø—É—Å—Ç–∏ –∏ –ø—Ä–æ–≤–µ—Ä—å**:
```bash
docker-compose up -d blackbox-exporter

# –ü—Ä–æ–≤–µ—Ä—å metrics
curl "http://localhost:9115/probe?target=https://google.com&module=http_2xx"

# Reload Prometheus
curl -X POST http://localhost:9090/-/reload
```

6. **–°–æ–∑–¥–∞–π Uptime Dashboard –≤ Grafana**:

**Panel 1: Service Availability**
```promql
avg_over_time(probe_success[24h]) * 100
```
Visualization: Stat
Unit: Percent (0-100)

**Panel 2: Response Time**
```promql
probe_duration_seconds
```
Visualization: Time series

**Panel 3: SSL Certificate Days Left**
```promql
(probe_ssl_earliest_cert_expiry - time()) / 86400
```
Visualization: Gauge
Thresholds: Green >30, Yellow 15-30, Red <15

**Panel 4: Uptime Table**
```promql
probe_success
```
Visualization: Table
Transform: Add field from calculation ‚Üí "Uptime" ‚Üí `$probe_success * 100`

### üöÄ –ë–æ–Ω—É—Å (–Ω–æ–≤–æ–µ)

**–°–æ–∑–¥–∞–π multi-step transaction check** —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Selenium:

**synthetic_test.py**:
```python
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from prometheus_client import Gauge, Counter, start_http_server
import time

# Metrics
transaction_duration = Gauge('transaction_duration_seconds', 'Transaction duration')
transaction_success = Counter('transaction_success_total', 'Successful transactions')
transaction_failure = Counter('transaction_failure_total', 'Failed transactions')

def run_transaction():
    """Simulate user transaction"""
    start = time.time()
    
    try:
        options = Options()
        options.add_argument('--headless')
        driver = webdriver.Chrome(options=options)
        
        # Step 1: Navigate
        driver.get('https://example.com')
        time.sleep(1)
        
        # Step 2: Find element
        element = driver.find_element(By.TAG_NAME, 'h1')
        assert 'Example' in element.text
        
        # Step 3: Click (if needed)
        # link = driver.find_element(By.LINK_TEXT, 'More information...')
        # link.click()
        
        driver.quit()
        
        duration = time.time() - start
        transaction_duration.set(duration)
        transaction_success.inc()
        print(f"‚úÖ Transaction successful in {duration:.2f}s")
        
    except Exception as e:
        transaction_failure.inc()
        print(f"‚ùå Transaction failed: {e}")

if __name__ == '__main__':
    start_http_server(8001)
    print("Synthetic monitoring started on port 8001")
    
    while True:
        run_transaction()
        time.sleep(60)  # Run every minute
```

**–°–æ–∑–¥–∞–π API endpoint checker**:
```python
# api_checker.py
import requests
import time
from prometheus_client import Histogram, Counter, Gauge, start_http_server

api_request_duration = Histogram('api_request_duration_seconds', 
                                 'API request duration',
                                 ['endpoint', 'method'])
api_request_total = Counter('api_request_total',
                           'Total API requests',
                           ['endpoint', 'method', 'status'])
api_request_errors = Counter('api_request_errors_total',
                            'Total API errors',
                            ['endpoint', 'error_type'])

endpoints = [
    {'url': 'https://api.github.com', 'method': 'GET'},
    {'url': 'https://jsonplaceholder.typicode.com/posts/1', 'method': 'GET'},
]

def check_endpoint(endpoint):
    url = endpoint['url']
    method = endpoint['method']
    
    try:
        start = time.time()
        
        if method == 'GET':
            response = requests.get(url, timeout=10)
        elif method == 'POST':
            response = requests.post(url, json={}, timeout=10)
        
        duration = time.time() - start
        
        api_request_duration.labels(endpoint=url, method=method).observe(duration)
        api_request_total.labels(endpoint=url, method=method, status=response.status_code).inc()
        
        print(f"‚úÖ {method} {url}: {response.status_code} ({duration:.2f}s)")
        
    except requests.Timeout:
        api_request_errors.labels(endpoint=url, error_type='timeout').inc()
        print(f"‚è±Ô∏è {method} {url}: Timeout")
    except requests.ConnectionError:
        api_request_errors.labels(endpoint=url, error_type='connection').inc()
        print(f"üîå {method} {url}: Connection error")
    except Exception as e:
        api_request_errors.labels(endpoint=url, error_type='other').inc()
        print(f"‚ùå {method} {url}: {e}")

if __name__ == '__main__':
    start_http_server(8002)
    print("API checker started on port 8002")
    
    while True:
        for endpoint in endpoints:
            check_endpoint(endpoint)
        time.sleep(30)
```

---

## –ú–æ–¥—É–ª—å 7: Application Performance Monitoring (APM) (20 –º–∏–Ω—É—Ç)

### üéØ –ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞

**APM –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:**
```
- Distributed Tracing  - –¢—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤ –º–µ–∂–¥—É —Å–µ—Ä–≤–∏—Å–∞–º–∏
- Profiling           - –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∫–æ–¥–∞
- Error Tracking      - –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –æ—à–∏–±–æ–∫
- Transaction Monitoring - –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–π
- Real User Monitoring  - –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ä–µ–∞–ª—å–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
```

**Distributed Tracing –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏:**
```
Trace    - –ü–æ–ª–Ω—ã–π –ø—É—Ç—å –∑–∞–ø—Ä–æ—Å–∞ —á–µ—Ä–µ–∑ —Å–∏—Å—Ç–µ–º—É
Span     - –û—Ç–¥–µ–ª—å–Ω–∞—è –æ–ø–µ—Ä–∞—Ü–∏—è –≤–Ω—É—Ç—Ä–∏ trace
Context  - –ü–µ—Ä–µ–¥–∞—á–∞ trace ID –º–µ–∂–¥—É —Å–µ—Ä–≤–∏—Å–∞–º–∏

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Trace ID: abc123                        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Span 1: API Gateway [200ms]             ‚îÇ
‚îÇ   ‚îî‚îÄ Span 2: Auth Service [50ms]        ‚îÇ
‚îÇ   ‚îî‚îÄ Span 3: Business Logic [100ms]     ‚îÇ
‚îÇ       ‚îî‚îÄ Span 4: Database Query [80ms]  ‚îÇ
‚îÇ   ‚îî‚îÄ Span 5: Cache Lookup [20ms]        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**OpenTelemetry (OTEL):**
```
–°—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–π
- Traces
- Metrics
- Logs

–ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:
- SDK        - –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏ –¥–ª—è —è–∑—ã–∫–æ–≤
- Collector  - –ê–≥—Ä–µ–≥–∞—Ü–∏—è –∏ —ç–∫—Å–ø–æ—Ä—Ç
- Exporters  - –û—Ç–ø—Ä–∞–≤–∫–∞ –≤ backends (Jaeger, Tempo)
```

**Jaeger –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   App    ‚îÇ‚îÄ‚îÄ‚ñ∫ Jaeger Agent ‚îÄ‚îÄ‚ñ∫ Jaeger Collector ‚îÄ‚îÄ‚ñ∫ Storage
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                                             (Cassandra/
                                                          Elasticsearch/
                                                          Badger)
                                                             ‚îÇ
                                                             ‚ñº
                                                        Jaeger Query
                                                             ‚îÇ
                                                             ‚ñº
                                                         Jaeger UI
```

**–ö–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ APM:**
```
- Request rate (req/s)
- Error rate (%)
- Duration (p50, p95, p99)
- Throughput
- Apdex score (Application Performance Index)
- Service dependencies
- Slow queries
- N+1 queries
```

**Apdex Score:**
```
Apdex = (Satisfied + Tolerating/2) / Total Requests

Satisfied:   Response time ‚â§ T
Tolerating:  Response time > T and ‚â§ 4T
Frustrated:  Response time > 4T

Score: 0.0 (worst) to 1.0 (best)
```

### üíª –ó–∞–¥–∞–Ω–∏–µ

–ù–∞—Å—Ç—Ä–æ–π distributed tracing —Å Jaeger:

1. **–î–æ–±–∞–≤—å Jaeger –≤ docker-compose.yml**:
```yaml
  jaeger:
    image: jaegertracing/all-in-one:latest
    container_name: jaeger
    ports:
      - "5775:5775/udp"    # Zipkin compatible
      - "6831:6831/udp"    # Jaeger compact thrift
      - "6832:6832/udp"    # Jaeger binary thrift
      - "5778:5778"        # Configs
      - "16686:16686"      # Jaeger UI
      - "14268:14268"      # Jaeger collector
      - "14250:14250"      # Jaeger gRPC
      - "9411:9411"        # Zipkin compatible
    environment:
      - COLLECTOR_ZIPKIN_HOST_PORT=:9411
    restart: unless-stopped
```

2. **–°–æ–∑–¥–∞–π –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ —Å —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–æ–π**:

**frontend.py** (—Å–µ—Ä–≤–∏—Å 1):
```python
from flask import Flask, jsonify
import requests
import time
import random
from jaeger_client import Config
from flask_opentracing import FlaskTracing
from opentracing_instrumentation.client_hooks import install_all_patches

app = Flask(__name__)

# Jaeger configuration
config = Config(
    config={
        'sampler': {'type': 'const', 'param': 1},
        'logging': True,
        'local_agent': {
            'reporting_host': 'jaeger',
            'reporting_port': 6831,
        },
    },
    service_name='frontend',
    validate=True,
)
jaeger_tracer = config.initialize_tracer()
tracing = FlaskTracing(jaeger_tracer, True, app)

# Instrument requests library
install_all_patches()

@app.route('/')
def index():
    return jsonify({"service": "frontend", "status": "ok"})

@app.route('/api/order', methods=['POST'])
def create_order():
    with jaeger_tracer.start_active_span('create_order') as scope:
        scope.span.set_tag('http.method', 'POST')
        scope.span.log_kv({'event': 'order_created'})
        
        # Call auth service
        with jaeger_tracer.start_active_span('call_auth', child_of=scope.span):
            auth_response = requests.get('http://auth-service:5001/verify')
            time.sleep(random.uniform(0.01, 0.05))
        
        if auth_response.status_code != 200:
            scope.span.set_tag('error', True)
            return jsonify({"error": "Auth failed"}), 401
        
        # Call business service
        with jaeger_tracer.start_active_span('call_business', child_of=scope.span):
            business_response = requests.post('http://business-service:5002/process')
            time.sleep(random.uniform(0.05, 0.15))
        
        return jsonify({
            "order_id": "ORD-" + str(random.randint(1000, 9999)),
            "status": "created"
        })

@app.route('/health')
def health():
    return jsonify({"status": "healthy"})

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000)
```

**auth_service.py** (—Å–µ—Ä–≤–∏—Å 2):
```python
from flask import Flask, jsonify
import time
import random
from jaeger_client import Config
from flask_opentracing import FlaskTracing

app = Flask(__name__)

config = Config(
    config={
        'sampler': {'type': 'const', 'param': 1},
        'logging': True,
        'local_agent': {
            'reporting_host': 'jaeger',
            'reporting_port': 6831,
        },
    },
    service_name='auth-service',
    validate=True,
)
jaeger_tracer = config.initialize_tracer()
tracing = FlaskTracing(jaeger_tracer, True, app)

@app.route('/verify')
def verify():
    with jaeger_tracer.start_active_span('verify_token') as scope:
        scope.span.set_tag('auth.method', 'token')
        
        # Simulate auth check
        time.sleep(random.uniform(0.02, 0.08))
        
        # Random auth failure
        if random.random() < 0.05:
            scope.span.set_tag('error', True)
            scope.span.log_kv({'event': 'auth_failed'})
            return jsonify({"error": "Unauthorized"}), 401
        
        scope.span.log_kv({'event': 'auth_success'})
        return jsonify({"status": "verified"})

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5001)
```

**business_service.py** (—Å–µ—Ä–≤–∏—Å 3):
```python
from flask import Flask, jsonify
import time
import random
from jaeger_client import Config
from flask_opentracing import FlaskTracing

app = Flask(__name__)

config = Config(
    config={
        'sampler': {'type': 'const', 'param': 1},
        'logging': True,
        'local_agent': {
            'reporting_host': 'jaeger',
            'reporting_port': 6831,
        },
    },
    service_name='business-service',
    validate=True,
)
jaeger_tracer = config.initialize_tracer()
tracing = FlaskTracing(jaeger_tracer, True, app)

@app.route('/process', methods=['POST'])
def process():
    with jaeger_tracer.start_active_span('process_order') as scope:
        # Simulate database query
        with jaeger_tracer.start_active_span('db_query', child_of=scope.span):
            scope.span.set_tag('db.type', 'postgresql')
            scope.span.set_tag('db.statement', 'INSERT INTO orders...')
            time.sleep(random.uniform(0.05, 0.15))
        
        # Simulate cache lookup
        with jaeger_tracer.start_active_span('cache_lookup', child_of=scope.span):
            scope.span.set_tag('cache.hit', random.choice([True, False]))
            time.sleep(random.uniform(0.01, 0.03))
        
        return jsonify({"status": "processed"})

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5002)
```

3. **–°–æ–∑–¥–∞–π requirements.txt**:
```txt
flask==2.3.0
requests==2.31.0
jaeger-client==4.8.0
flask-opentracing==1.1.0
opentracing-instrumentation==3.3.1
```

4. **–°–æ–∑–¥–∞–π Dockerfile –¥–ª—è —Å–µ—Ä–≤–∏—Å–æ–≤**:
```dockerfile
FROM python:3.9-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt
COPY *.py .
CMD ["python"]
```

5. **–û–±–Ω–æ–≤–∏ docker-compose.yml** –¥–ª—è —Å–µ—Ä–≤–∏—Å–æ–≤:
```yaml
  frontend:
    build: ./services
    container_name: frontend
    command: python frontend.py
    ports:
      - "5000:5000"
    depends_on:
      - jaeger
      - auth-service
      - business-service
    restart: unless-stopped

  auth-service:
    build: ./services
    container_name: auth-service
    command: python auth_service.py
    depends_on:
      - jaeger
    restart: unless-stopped

  business-service:
    build: ./services
    container_name: business-service
    command: python business_service.py
    depends_on:
      - jaeger
    restart: unless-stopped
```

6. **–ó–∞–ø—É—Å—Ç–∏ –∏ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä—É–π**:
```bash
# –°–æ–∑–¥–∞–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —Å–µ—Ä–≤–∏—Å–æ–≤
mkdir -p services
# –°–∫–æ–ø–∏—Ä—É–π —Ñ–∞–π–ª—ã –≤ services/

# –ó–∞–ø—É—Å—Ç–∏
docker-compose up -d jaeger frontend auth-service business-service

# –ì–µ–Ω–µ—Ä–∏—Ä—É–π —Ç—Ä–∞—Ñ–∏–∫
for i in {1..50}; do
  curl -X POST http://localhost:5000/api/order
  sleep 0.5
done

# –û—Ç–∫—Ä–æ–π Jaeger UI
http://localhost:16686
```

7. **–ò–∑—É—á–∏ Jaeger UI**:
- Service ‚Üí frontend
- Operation ‚Üí POST /api/order
- Find Traces
- –ò–∑—É—á–∏ waterfall view
- –ü–æ—Å–º–æ—Ç—Ä–∏ –Ω–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —Å–µ—Ä–≤–∏—Å–æ–≤

### üöÄ –ë–æ–Ω—É—Å (–Ω–æ–≤–æ–µ)

**–î–æ–±–∞–≤—å Grafana Tempo –¥–ª—è long-term storage**:

```yaml
  tempo:
    image: grafana/tempo:latest
    container_name: tempo
    command: [ "-config.file=/etc/tempo.yaml" ]
    volumes:
      - ./tempo.yaml:/etc/tempo.yaml
      - tempo-data:/tmp/tempo
    ports:
      - "3200:3200"   # tempo
      - "4317:4317"   # otlp grpc
      - "4318:4318"   # otlp http
    restart: unless-stopped

volumes:
  tempo-data:
```

**tempo.yaml**:
```yaml
server:
  http_listen_port: 3200

distributor:
  receivers:
    otlp:
      protocols:
        grpc:
        http:

storage:
  trace:
    backend: local
    local:
      path: /tmp/tempo/blocks

query_frontend:
  search:
    duration_slo: 5s
    throughput_bytes_slo: 1.073741824e+09
  trace_by_id:
    duration_slo: 5s
```

**–°–æ–∑–¥–∞–π exemplars —Å–≤—è–∑–∫—É –º–µ–∂–¥—É metrics –∏ traces**:
```python
from prometheus_client import Histogram, Info
from opentracing.ext import tags

# Histogram with exemplar support
request_duration = Histogram(
    'http_request_duration_seconds',
    'HTTP request duration',
    ['method', 'endpoint']
)

@app.route('/api/order', methods=['POST'])
def create_order():
    with request_duration.labels(method='POST', endpoint='/api/order').time():
        with jaeger_tracer.start_active_span('create_order') as scope:
            # Add trace ID to exemplar
            trace_id = scope.span.context.trace_id
            # Process request
            return process_order(trace_id)
```

**–ù–∞—Å—Ç—Ä–æ–π Service Map –≤ Grafana**:
1. Add Tempo data source –≤ Grafana
2. URL: http://tempo:3200
3. Explore ‚Üí Tempo
4. Search ‚Üí Service Graph

---

## –ú–æ–¥—É–ª—å 8: –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –≤ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ - Best Practices (15 –º–∏–Ω—É—Ç)

### üéØ –ù–∞–ø–æ–º–∏–Ω–∞–ª–∫–∞

**SRE –ø—Ä–∏–Ω—Ü–∏–ø—ã:**
```
SLI (Service Level Indicator)   - –ú–µ—Ç—Ä–∏–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —Å–µ—Ä–≤–∏—Å–∞
SLO (Service Level Objective)   - –¶–µ–ª–µ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ SLI
SLA (Service Level Agreement)   - –î–æ–≥–æ–≤–æ—Ä–Ω–æ–µ –æ–±—è–∑–∞—Ç–µ–ª—å—Å—Ç–≤–æ
Error Budget                     - –î–æ–ø—É—Å—Ç–∏–º–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—à–∏–±–æ–∫
```

**–ü—Ä–∏–º–µ—Ä—ã SLI/SLO:**
```
SLI: Availability
SLO: 99.9% uptime (43.2 min downtime/month)

SLI: Latency
SLO: 95% requests < 100ms

SLI: Error Rate
SLO: < 0.1% error rate

SLI: Throughput
SLO: Handle 10,000 req/s
```

**Error Budget:**
```
Uptime SLO: 99.9%
Allowed downtime: 0.1% = 43.2 min/month

If error budget > 0:
  ‚Üí Can take risks, deploy faster
  
If error budget = 0:
  ‚Üí Focus on reliability, slow deploys
```

**Monitoring Best Practices:**
```
‚úÖ DO:
- Monitor symptoms, not causes
- Alert on SLO violations
- Use runbooks for alerts
- Test alerts regularly
- Keep dashboards simple
- Document everything
- Use labels consistently
- Set up test environments
- Automate alert remediation where possible
- Review alerts quarterly

‚ùå DON'T:
- Alert on everything
- Set alert thresholds too tight
- Ignore alert fatigue
- Monitor without context
- Create dashboards without purpose
- Alert without actionable next steps
```

**Dashboard hierarchy:**
```
Level 1: Overview (C-level)
- Overall system health
- Key business metrics
- High-level SLOs

Level 2: Service (Team leads)
- Per-service metrics
- RED/USE metrics
- Resource utilization

Level 3: Detailed (Engineers)
- Detailed metrics
- Debug information
- Deep-dive panels
```

**On-call best practices:**
```
1. Clear escalation paths
2. Comprehensive runbooks
3. Postmortems for incidents
4. Fair rotation schedule
5. Context in alerts
6. Response time SLOs
7. Blameless culture
8. Regular drills
```

**Incident Response Process:**
```
1. Detection    - Alert fires
2. Triage       - Assess severity
3. Mitigation   - Stop the bleeding
4. Investigation - Find root cause
5. Resolution   - Fix permanently
6. Postmortem   - Learn & improve
```

**Cost optimization:**
```
- Use recording rules for expensive queries
- Set appropriate retention periods
- Use downsampling for old data
- Archive cold data
- Monitor cardinality
- Use relabeling to drop unnecessary metrics
- Implement metric limits
```

### üíª –ó–∞–¥–∞–Ω–∏–µ

–°–æ–∑–¥–∞–π production-ready monitoring setup:

1. **–û–ø—Ä–µ–¥–µ–ª–∏ SLIs/SLOs –¥–ª—è —Å–µ—Ä–≤–∏—Å–∞**:

**slo_config.yml**:
```yaml
slos:
  - name: api_availability
    description: "API –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –¥–æ—Å—Ç—É–ø–µ–Ω 99.9% –≤—Ä–µ–º–µ–Ω–∏"
    target: 0.999
    window: 30d
    sli:
      error_ratio_query: |
        sum(rate(http_requests_total{job="frontend",status=~"5.."}[5m]))
        /
        sum(rate(http_requests_total{job="frontend"}[5m]))

  - name: api_latency
    description: "95% –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–æ–ª–∂–Ω—ã –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å—Å—è < 200ms"
    target: 0.95
    window: 30d
    sli:
      latency_query: |
        histogram_quantile(0.95,
          rate(http_request_duration_seconds_bucket{job="frontend"}[5m])
        ) < 0.2

  - name: error_rate
    description: "–ü—Ä–æ—Ü–µ–Ω—Ç –æ—à–∏–±–æ–∫ < 0.1%"
    target: 0.999
    window: 30d
    sli:
      error_ratio_query: |
        (
          sum(rate(http_requests_total{status=~"5.."}[5m]))
          /
          sum(rate(http_requests_total[5m]))
        ) < 0.001
```

2. **–°–æ–∑–¥–∞–π SLO alerts** (–¥–æ–±–∞–≤—å –≤ `alerts.yml`):
```yaml
groups:
  - name: slo_alerts
    rules:
    # Error Budget Alert
    - alert: ErrorBudgetBurn
      expr: |
        (
          1 - (
            sum(rate(http_requests_total{status=~"2.."}[1h]))
            /
            sum(rate(http_requests_total[1h]))
          )
        ) > 0.001
      for: 5m
      labels:
        severity: critical
        slo: availability
      annotations:
        summary: "Error budget burning too fast"
        description: "Current error rate {{ $value | humanizePercentage }} exceeds budget"
        runbook: "https://runbook.example.com/error-budget"

    # Latency SLO violation
    - alert: LatencySLOViolation
      expr: |
        histogram_quantile(0.95,
          rate(http_request_duration_seconds_bucket[5m])
        ) > 0.2
      for: 10m
      labels:
        severity: warning
        slo: latency
      annotations:
        summary: "Latency SLO violation"
        description: "p95 latency is {{ $value }}s (SLO: 0.2s)"
        impact: "Users experiencing slow responses"
        action: "Check service performance and database queries"

    # Multi-window burn rate
    - alert: ErrorBudgetFastBurn
      expr: |
        (
          (1 - avg_over_time(up[1h]) < 0.999)
          and
          (1 - avg_over_time(up[5m]) < 0.999)
        )
      labels:
        severity: critical
        burn_rate: fast
      annotations:
        summary: "Error budget burning at fast rate"
        description: "Both short and long windows show SLO violations"
```

3. **–°–æ–∑–¥–∞–π comprehensive dashboard** (`production-overview.json`):
```json
{
  "dashboard": {
    "title": "Production Overview",
    "tags": ["production", "slo"],
    "rows": [
      {
        "title": "SLOs",
        "panels": [
          {
            "title": "Availability (30d SLO: 99.9%)",
            "targets": [{
              "expr": "avg_over_time(up[30d]) * 100"
            }],
            "type": "stat",
            "thresholds": [
              {"value": 99.9, "color": "green"},
              {"value": 99.5, "color": "yellow"},
              {"value": 0, "color": "red"}
            ]
          },
          {
            "title": "Error Budget Remaining",
            "targets": [{
              "expr": "(0.999 - (1 - avg_over_time(up[30d]))) / 0.001 * 100"
            }],
            "type": "gauge"
          }
        ]
      },
      {
        "title": "Golden Signals",
        "panels": [
          {
            "title": "Request Rate",
            "targets": [{
              "expr": "sum(rate(http_requests_total[5m]))"
            }]
          },
          {
            "title": "Error Rate",
            "targets": [{
              "expr": "sum(rate(http_requests_total{status=~\"5..\"}[5m])) / sum(rate(http_requests_total[5m])) * 100"
            }]
          },
          {
            "title": "Latency (p50, p95, p99)",
            "targets": [
              {"expr": "histogram_quantile(0.50, rate(http_request_duration_seconds_bucket[5m]))", "legendFormat": "p50"},
              {"expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))", "legendFormat": "p95"},
              {"expr": "histogram_quantile(0.99, rate(http_request_duration_seconds_bucket[5m]))", "legendFormat": "p99"}
            ]
          }
        ]
      }
    ]
  }
}
```

4. **–°–æ–∑–¥–∞–π runbook template** (`runbooks/high-error-rate.md`):
```markdown
# Runbook: High Error Rate

## Alert
**Name:** HighErrorRate
**Severity:** Critical
**SLO Impact:** Availability

## Symptoms
- Error rate > 1% for more than 5 minutes
- Users experiencing 5xx errors
- Error budget burning

## Impact
- **Users:** Cannot complete requests
- **Business:** Loss of revenue/trust
- **SLO:** Burns error budget

## Diagnosis

### Step 1: Verify the alert
```bash
# Check current error rate
curl -G http://prometheus:9090/api/v1/query \
  --data-urlencode 'query=rate(http_requests_total{status=~"5.."}[5m])/rate(http_requests_total[5m])'
```

### Step 2: Identify affected services
```bash
# Check which services are returning errors
# Grafana ‚Üí Explore ‚Üí Prometheus
sum by (job) (rate(http_requests_total{status=~"5.."}[5m]))
```

### Step 3: Check logs
```bash
# Grafana ‚Üí Explore ‚Üí Loki
{job="frontend"} |= "ERROR" | json
```

### Step 4: Check recent deployments
```bash
# Check if error rate increased after deployment
kubectl get pods -o wide
kubectl describe deployment frontend
```

### Step 5: Check dependencies
```bash
# Database
psql -c "SELECT pg_is_in_recovery();"

# Cache
redis-cli ping

# External APIs
curl https://api.external.com/health
```

## Mitigation

### Immediate (Stop the bleeding)
1. **Rollback deployment** if recent:
```bash
kubectl rollout undo deployment/frontend
```

2. **Scale up** if resource constrained:
```bash
kubectl scale deployment/frontend --replicas=10
```

3. **Enable circuit breaker** for failing dependency:
```bash
# Update config to bypass failing service
```

4. **Put up maintenance page** if critical:
```bash
# Route traffic to maintenance page
```

### Short-term (Stabilize)
1. Investigate root cause
2. Apply proper fix
3. Deploy with gradual rollout
4. Monitor closely

## Resolution
- [ ] Error rate back to < 0.1%
- [ ] Root cause identified
- [ ] Fix deployed and verified
- [ ] Monitoring confirms stability
- [ ] Postmortem scheduled

## Escalation
- **L1:** On-call engineer (You)
- **L2:** Team lead (after 15 min)
- **L3:** Engineering manager (after 30 min)
- **L4:** VP Engineering (critical)

## References
- Dashboard: http://grafana/d/prod-overview
- Logs: http://grafana/explore?loki
- Traces: http://jaeger:16686
- Slack: #incidents

## Related Runbooks
- [Database Connection Issues](./db-connection.md)
- [High Latency](./high-latency.md)
- [Service Down](./service-down.md)
```

5. **–°–æ–∑–¥–∞–π incident response script** (`scripts/incident_response.sh`):
```bash
#!/bin/bash

# Incident Response Helper Script

set -e

ALERT_NAME=$1
SEVERITY=$2

if [ -z "$ALERT_NAME" ]; then
  echo "Usage: ./incident_response.sh <alert_name> <severity>"
  exit 1
fi

echo "üö® Incident Response Started"
echo "Alert: $ALERT_NAME"
echo "Severity: $SEVERITY"
echo "Time: $(date)"
echo ""

# 1. Gather context
echo "üìä Gathering context..."
echo ""

echo "Current Metrics:"
curl -s -G http://localhost:9090/api/v1/query \
  --data-urlencode 'query=up' | jq '.data.result[] | {job: .metric.job, status: .value[1]}'

echo ""
echo "Recent Errors (last 5 min):"
curl -s -G http://localhost:9090/api/v1/query \
  --data-urlencode 'query=sum(rate(http_requests_total{status=~"5.."}[5m]))' | jq '.data.result[0].value[1]'

echo ""
echo "Active Alerts:"
curl -s http://localhost:9090/api/v1/alerts | jq '.data.alerts[] | select(.state=="firing") | {alert: .labels.alertname, severity: .labels.severity}'

# 2. Check recent changes
echo ""
echo "üîç Recent changes..."
docker ps --format "table {{.Names}}\t{{.Status}}\t{{.Image}}"

# 3. Generate incident report
INCIDENT_ID="INC-$(date +%Y%m%d-%H%M%S)"
echo ""
echo "üìù Creating incident report: $INCIDENT_ID"

cat > "incidents/${INCIDENT_ID}.md" <<EOF
# Incident Report: $INCIDENT_ID

## Summary
- **Alert:** $ALERT_NAME
- **Severity:** $SEVERITY
- **Start Time:** $(date)
- **Status:** Investigating

## Timeline
- $(date +%H:%M:%S) - Alert fired
- $(date +%H:%M:%S) - Investigation started

## Impact
- [ ] Users affected: TBD
- [ ] Services affected: TBD
- [ ] Revenue impact: TBD

## Actions Taken
- Gathered initial context
- Reviewed metrics and logs

## Next Steps
1. Identify root cause
2. Implement mitigation
3. Verify resolution
4. Schedule postmortem

## Notes
EOF

echo "Incident report created: incidents/${INCIDENT_ID}.md"
echo ""
echo "‚úÖ Context gathered. Next: Check runbook at runbooks/${ALERT_NAME}.md"
```

### üöÄ –ë–æ–Ω—É—Å (–Ω–æ–≤–æ–µ)

**–°–æ–∑–¥–∞–π automated remediation** –¥–ª—è —á–∞—Å—Ç—ã—Ö –ø—Ä–æ–±–ª–µ–º:

**auto_remediation.py**:
```python
import requests
import time
from datetime import datetime

PROMETHEUS_URL = "http://localhost:9090"
ALERTMANAGER_URL = "http://localhost:9093"

def check_alerts():
    """Check for active alerts"""
    response = requests.get(f"{PROMETHEUS_URL}/api/v1/alerts")
    alerts = response.json()['data']['alerts']
    
    firing_alerts = [a for a in alerts if a['state'] == 'firing']
    return firing_alerts

def auto_remediate(alert):
    """Attempt automatic remediation"""
    alert_name = alert['labels']['alertname']
    
    print(f"[{datetime.now()}] Attempting auto-remediation for: {alert_name}")
    
    if alert_name == "HighMemoryUsage":
        # Clear caches
        print("  ‚Üí Clearing application caches")
        requests.post("http://localhost:5000/admin/clear-cache")
        
    elif alert_name == "HighCPUUsage":
        # Scale up service
        print("  ‚Üí Scaling up service")
        # kubectl scale deployment --replicas=+2
        
    elif alert_name == "DiskSpaceLow":
        # Clean old logs
        print("  ‚Üí Cleaning old logs")
        import subprocess
        subprocess.run(["find", "/var/log", "-name", "*.log.gz", "-mtime", "+7", "-delete"])
    
    else:
        print(f"  ‚ö†Ô∏è  No auto-remediation for {alert_name}")
        return False
    
    print(f"  ‚úÖ Auto-remediation completed")
    return True

def create_silence(alert, duration_hours=1):
    """Create silence after remediation"""
    silence = {
        "matchers": [
            {
                "name": "alertname",
                "value": alert['labels']['alertname'],
                "isRegex": False
            }
        ],
        "startsAt": datetime.utcnow().isoformat() + "Z",
        "endsAt": datetime.utcnow().isoformat() + "Z",  # +duration
        "createdBy": "auto-remediation",
        "comment": f"Auto-remediated at {datetime.now()}"
    }
    
    requests.post(f"{ALERTMANAGER_URL}/api/v2/silences", json=silence)

if __name__ == '__main__':
    print("ü§ñ Auto-remediation service started")
    
    while True:
        alerts = check_alerts()
        
        for alert in alerts:
            if auto_remediate(alert):
                create_silence(alert, duration_hours=1)
        
        time.sleep(60)
```

**–ù–∞—Å—Ç—Ä–æ–π chaos engineering –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞**:

**chaos_test.sh**:
```bash
#!/bin/bash

echo "üî• Starting Chaos Engineering Tests"
echo "Testing monitoring system resilience..."
echo ""

# Test 1: Kill random container
echo "Test 1: Container Failure"
CONTAINER=$(docker ps --format "{{.Names}}" | grep -E "frontend|auth|business" | shuf -n 1)
echo "  ‚Üí Killing: $CONTAINER"
docker kill $CONTAINER
echo "  ‚Üí Waiting 30 seconds..."
sleep 30
echo "  ‚Üí Restoring container..."
docker-compose up -d $CONTAINER
echo "  ‚úÖ Test 1 complete"
echo ""

# Test 2: Simulate high CPU
echo "Test 2: High CPU Load"
echo "  ‚Üí Starting CPU stress on frontend..."
docker exec frontend sh -c "for i in 1 2 3 4; do yes > /dev/null & done" 2>/dev/null
echo "  ‚Üí Stress running for 60 seconds..."
sleep 60
echo "  ‚Üí Stopping stress..."
docker exec frontend sh -c "pkill yes" 2>/dev/null
echo "  ‚úÖ Test 2 complete"
echo ""

# Test 3: Simulate network latency
echo "Test 3: Network Latency"
echo "  ‚Üí Adding 200ms latency..."
docker exec frontend sh -c "tc qdisc add dev eth0 root netem delay 200ms" 2>/dev/null || echo "  ‚ö†Ô∏è  tc not available"
sleep 60
echo "  ‚Üí Removing latency..."
docker exec frontend sh -c "tc qdisc del dev eth0 root" 2>/dev/null
echo "  ‚úÖ Test 3 complete"
echo ""

# Test 4: Disk space pressure
echo "Test 4: Disk Space Pressure"
echo "  ‚Üí Creating 1GB file..."
docker exec frontend sh -c "dd if=/dev/zero of=/tmp/fillfile bs=1M count=1000" 2>/dev/null
sleep 30
echo "  ‚Üí Removing file..."
docker exec frontend sh -c "rm /tmp/fillfile" 2>/dev/null
echo "  ‚úÖ Test 4 complete"
echo ""

# Test 5: Memory pressure
echo "Test 5: Memory Pressure"
echo "  ‚Üí Allocating 512MB..."
docker exec frontend sh -c "stress --vm 1 --vm-bytes 512M --timeout 60s" 2>/dev/null || echo "  ‚ö†Ô∏è  stress tool not available"
echo "  ‚úÖ Test 5 complete"
echo ""

# Test 6: Database connection failure
echo "Test 6: Database Connection Failure"
echo "  ‚Üí Stopping PostgreSQL..."
docker-compose stop postgres
sleep 30
echo "  ‚Üí Restarting PostgreSQL..."
docker-compose start postgres
sleep 10
echo "  ‚úÖ Test 6 complete"
echo ""

# Test 7: Random 500 errors
echo "Test 7: Random Application Errors"
echo "  ‚Üí Injecting errors for 60 seconds..."
# –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –∫–æ–¥ –¥–ª—è –∏–Ω—ä–µ–∫—Ü–∏–∏ –æ—à–∏–±–æ–∫ –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ
for i in {1..20}; do
    curl -X POST http://localhost:5000/api/order 2>/dev/null
    sleep 3
done
echo "  ‚úÖ Test 7 complete"
echo ""

echo "üéâ All chaos tests completed!"
echo ""
echo "üìä Check your monitoring:"
echo "  Prometheus Alerts: http://localhost:9090/alerts"
echo "  Grafana Dashboards: http://localhost:3000"
echo "  Alertmanager: http://localhost:9093"
echo "  Jaeger Traces: http://localhost:16686"
echo ""
echo "Questions to verify:"
echo "  ‚úì Did alerts fire as expected?"
echo "  ‚úì Were all incidents visible in dashboards?"
echo "  ‚úì Did traces show the failures?"
echo "  ‚úì Were logs properly collected?"
echo "  ‚úì Did services recover automatically?"
```

**–ó–∞–ø—É—Å–∫:**
```bash
chmod +x chaos_test.sh
./chaos_test.sh
```

---

## –ú–æ–¥—É–ª—å 9: –§–∏–Ω–∞–ª—å–Ω—ã–π –ø—Ä–æ–µ–∫—Ç –∏ –∫–∞—Ä—å–µ—Ä–∞ (30 –º–∏–Ω—É—Ç)

### üéØ –§–∏–Ω–∞–ª—å–Ω—ã–π –ø—Ä–æ–µ–∫—Ç: E-Commerce Monitoring Stack

–°–æ–∑–¥–∞–π production-ready –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –¥–ª—è –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–º–∞–≥–∞–∑–∏–Ω–∞.

**–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Frontend Layer ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  NGINX (Load Balancer) ‚Üí Frontend Services (x3)         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ API Layer ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  API Gateway ‚Üí Authentication ‚Üí Rate Limiting            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Service Layer ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Product Service ‚îÇ Order Service ‚îÇ Payment Service       ‚îÇ
‚îÇ  Inventory Svc   ‚îÇ User Service  ‚îÇ Notification Svc     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Data Layer ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  PostgreSQL (Primary + Replica) ‚îÇ Redis Cache ‚îÇ S3       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Monitoring Layer ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Metrics: Prometheus + Grafana                            ‚îÇ
‚îÇ Logs: Loki + Promtail                                    ‚îÇ
‚îÇ Traces: Tempo + Jaeger                                   ‚îÇ
‚îÇ Alerts: Alertmanager ‚Üí PagerDuty/Slack                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### üíª –ó–∞–¥–∞–Ω–∏–µ: –ü–æ–ª–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è

**–®–∞–≥ 1: –ö–ª–æ–Ω–∏—Ä—É–π —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø—Ä–æ–µ–∫—Ç–∞**
```bash
mkdir ecommerce-monitoring
cd ecommerce-monitoring

# –°—Ç—Ä—É–∫—Ç—É—Ä–∞
mkdir -p {services/{frontend,api-gateway,product,order,payment},monitoring/{prometheus,grafana,loki,alertmanager},scripts,docs}
```

**–®–∞–≥ 2: –°–æ–∑–¥–∞–π docker-compose-final.yml**
```yaml
version: '3.8'

networks:
  frontend:
  backend:
  monitoring:

services:
  # === Load Balancer ===
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
    networks:
      - frontend
      - monitoring
    depends_on:
      - frontend

  # === Application Services ===
  frontend:
    build: ./services/frontend
    deploy:
      replicas: 3
    environment:
      API_URL: http://api-gateway:8080
      JAEGER_AGENT_HOST: jaeger
    networks:
      - frontend
      - monitoring

  api-gateway:
    build: ./services/api-gateway
    environment:
      PRODUCT_SERVICE: http://product-service:8081
      ORDER_SERVICE: http://order-service:8082
      PAYMENT_SERVICE: http://payment-service:8083
      REDIS_URL: redis://redis:6379
    networks:
      - frontend
      - backend
      - monitoring

  product-service:
    build: ./services/product
    environment:
      DATABASE_URL: postgresql://postgres:password@postgres:5432/products
      REDIS_URL: redis://redis:6379
    networks:
      - backend
      - monitoring

  order-service:
    build: ./services/order
    environment:
      DATABASE_URL: postgresql://postgres:password@postgres:5432/orders
      PAYMENT_SERVICE: http://payment-service:8083
    networks:
      - backend
      - monitoring

  payment-service:
    build: ./services/payment
    environment:
      DATABASE_URL: postgresql://postgres:password@postgres:5432/payments
    networks:
      - backend
      - monitoring

  # === Data Layer ===
  postgres:
    image: postgres:15
    environment:
      POSTGRES_PASSWORD: password
      POSTGRES_DB: ecommerce
    volumes:
      - postgres-data:/var/lib/postgresql/data
    networks:
      - backend
      - monitoring

  redis:
    image: redis:7-alpine
    networks:
      - backend
      - monitoring

  # === Monitoring Stack ===
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus:/etc/prometheus
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.retention.time=30d'
      - '--web.enable-lifecycle'
    networks:
      - monitoring

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      GF_SECURITY_ADMIN_PASSWORD: admin
      GF_INSTALL_PLUGINS: grafana-piechart-panel
    volumes:
      - ./monitoring/grafana:/etc/grafana/provisioning
      - grafana-data:/var/lib/grafana
    networks:
      - monitoring

  loki:
    image: grafana/loki:latest
    ports:
      - "3100:3100"
    volumes:
      - ./monitoring/loki:/etc/loki
      - loki-data:/loki
    networks:
      - monitoring

  promtail:
    image: grafana/promtail:latest
    volumes:
      - ./monitoring/promtail:/etc/promtail
      - /var/log:/var/log:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
    networks:
      - monitoring

  tempo:
    image: grafana/tempo:latest
    ports:
      - "3200:3200"
      - "4317:4317"
    volumes:
      - ./monitoring/tempo:/etc/tempo
      - tempo-data:/tmp/tempo
    networks:
      - monitoring

  jaeger:
    image: jaegertracing/all-in-one:latest
    ports:
      - "16686:16686"
      - "14268:14268"
    environment:
      SPAN_STORAGE_TYPE: badger
      BADGER_EPHEMERAL: "false"
      BADGER_DIRECTORY_VALUE: /badger/data
      BADGER_DIRECTORY_KEY: /badger/key
    volumes:
      - jaeger-data:/badger
    networks:
      - monitoring

  alertmanager:
    image: prom/alertmanager:latest
    ports:
      - "9093:9093"
    volumes:
      - ./monitoring/alertmanager:/etc/alertmanager
    networks:
      - monitoring

volumes:
  postgres-data:
  prometheus-data:
  grafana-data:
  loki-data:
  tempo-data:
  jaeger-data:
```

**–®–∞–≥ 3: –û–ø—Ä–µ–¥–µ–ª–∏ SLIs –∏ SLOs**

**docs/slo-definitions.md**:
```markdown
# Service Level Objectives (SLOs)

## 1. Availability SLO
**SLI:** Percentage of successful requests
**SLO:** 99.9% (43.2 minutes downtime/month)
**Measurement:** `sum(rate(http_requests{status=~"2.."}[30d])) / sum(rate(http_requests[30d]))`

## 2. Latency SLO
**SLI:** 95th percentile response time
**SLO:** < 500ms for 95% of requests
**Measurement:** `histogram_quantile(0.95, rate(http_duration_bucket[5m]))`

## 3. Error Budget
**Calculation:** (1 - SLO) √ó Total requests
**30-day budget:** 0.1% √ó requests = allowed errors
**Burn rate alerting:**
- Fast burn: 2% budget in 1 hour ‚Üí Page
- Slow burn: 10% budget in 6 hours ‚Üí Ticket

## 4. Business SLOs

### Order Processing
- **SLO:** 99.5% orders processed successfully
- **Target:** < 1 minute processing time

### Payment Success
- **SLO:** 99.9% payment success rate
- **Target:** < 3 seconds payment confirmation

### Search Response
- **SLO:** 95% searches return results
- **Target:** < 200ms search response time
```

**–®–∞–≥ 4: –°–æ–∑–¥–∞–π Production Dashboards**

**monitoring/grafana/dashboards/01-executive-overview.json**:
```json
{
  "dashboard": {
    "title": "Executive Overview",
    "tags": ["business", "executive"],
    "panels": [
      {
        "title": "System Health Score",
        "type": "gauge",
        "gridPos": {"h": 8, "w": 6, "x": 0, "y": 0},
        "targets": [{
          "expr": "avg((up{job=~\".*service\"} == 1) * 100)"
        }],
        "fieldConfig": {
          "defaults": {
            "unit": "percent",
            "min": 0,
            "max": 100,
            "thresholds": {
              "steps": [
                {"value": 0, "color": "red"},
                {"value": 95, "color": "yellow"},
                {"value": 99, "color": "green"}
              ]
            }
          }
        }
      },
      {
        "title": "Orders per Hour",
        "type": "stat",
        "gridPos": {"h": 4, "w": 6, "x": 6, "y": 0},
        "targets": [{
          "expr": "sum(increase(orders_total[1h]))"
        }],
        "fieldConfig": {
          "defaults": {
            "unit": "short",
            "color": {"mode": "thresholds"},
            "thresholds": {
              "steps": [
                {"value": 0, "color": "red"},
                {"value": 100, "color": "yellow"},
                {"value": 500, "color": "green"}
              ]
            }
          }
        }
      },
      {
        "title": "Revenue Today",
        "type": "stat",
        "gridPos": {"h": 4, "w": 6, "x": 12, "y": 0},
        "targets": [{
          "expr": "sum(increase(payment_amount_total[24h]))"
        }],
        "fieldConfig": {
          "defaults": {
            "unit": "currencyUSD",
            "decimals": 2
          }
        }
      },
      {
        "title": "Active Users",
        "type": "graph",
        "gridPos": {"h": 8, "w": 12, "x": 0, "y": 8},
        "targets": [{
          "expr": "sum(rate(http_requests_total{endpoint=\"/\"}[5m])) * 60",
          "legendFormat": "Active Users"
        }]
      }
    ]
  }
}
```

**–®–∞–≥ 5: Comprehensive Alerts**

**monitoring/prometheus/alerts/production.yml**:
```yaml
groups:
  - name: critical_slo_violations
    interval: 30s
    rules:
    # Multi-window burn rate (Google SRE)
    - alert: ErrorBudgetCriticalBurn
      expr: |
        (
          sum(rate(http_requests{status=~"5.."}[1h]))
          / sum(rate(http_requests[1h]))
        ) > (14.4 * 0.001)  # 2% of monthly budget in 1 hour
        and
        (
          sum(rate(http_requests{status=~"5.."}[5m]))
          / sum(rate(http_requests[5m]))
        ) > (14.4 * 0.001)
      labels:
        severity: page
        team: sre
      annotations:
        summary: "üö® Critical error budget burn"
        description: "Burning 2% of 30-day error budget per hour"
        runbook: "https://runbook.company.com/error-budget-burn"
        action: "Page on-call engineer immediately"

    - alert: ServiceDown
      expr: up{job=~".*-service"} == 0
      for: 1m
      labels:
        severity: page
        team: platform
      annotations:
        summary: "üî¥ Service {{ $labels.job }} is DOWN"
        description: "{{ $labels.instance }} unreachable for 1+ minutes"
        impact: "Service unavailable to users"

  - name: slo_approaching_violations
    interval: 1m
    rules:
    - alert: LatencySLOAtRisk
      expr: |
        histogram_quantile(0.95,
          sum(rate(http_duration_bucket[30m])) by (le)
        ) > 0.45  # 90% of 500ms threshold
      for: 15m
      labels:
        severity: warning
        team: backend
      annotations:
        summary: "‚ö†Ô∏è Latency approaching SLO limit"
        description: "p95 latency: {{ $value }}s (SLO: 0.5s)"

  - name: business_kpis
    interval: 5m
    rules:
    - alert: OrderRateDropCritical
      expr: |
        (
          sum(rate(orders_total[10m]))
          /
          sum(rate(orders_total[10m] offset 1h))
        ) < 0.5
      for: 10m
      labels:
        severity: page
        team: business
      annotations:
        summary: "üìâ Order rate dropped 50%+"
        description: "Current: {{ $value | humanizePercentage }} of normal"
        impact: "Severe revenue impact"

    - alert: PaymentFailureSpike
      expr: |
        sum(rate(payments_total{status="failed"}[5m]))
        / sum(rate(payments_total[5m]))
        > 0.05
      for: 5m
      labels:
        severity: page
        team: payments
      annotations:
        summary: "üí≥ Payment failure rate > 5%"
        description: "{{ $value | humanizePercentage }} payments failing"

  - name: infrastructure
    interval: 1m
    rules:
    - alert: HighMemoryPressure
      expr: |
        (
          1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)
        ) > 0.90
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Memory usage > 90%"

    - alert: DiskFillPrediction
      expr: |
        predict_linear(node_filesystem_avail_bytes[1h], 4*3600) < 0
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "Disk will fill in ~4 hours"
        action: "Clean logs or expand disk"

    - alert: DatabaseConnectionPoolExhausted
      expr: |
        pg_stat_database_numbackends
        / pg_settings_max_connections
        > 0.8
      for: 5m
      labels:
        severity: warning
      annotations:
        summary: "DB connection pool 80%+ utilized"
```

**–®–∞–≥ 6: Runbooks**

**docs/runbooks/high-error-rate.md**:
```markdown
# Runbook: High Error Rate

## Alert Details
- **Alert:** HighErrorRate  
- **Severity:** Critical (Page)
- **SLO Impact:** Availability

## Symptoms
- Error rate > 1% for 5+ minutes
- Users seeing 5xx errors
- Error budget burning fast

## Initial Response (First 5 minutes)

### 1. Acknowledge alert
```bash
# Silence alert while investigating
amtool silence add alertname=HighErrorRate --duration=30m --author=oncall --comment="Investigating"
```

### 2. Check overall system health
- Grafana: http://grafana.company.com/d/overview
- Look for: spike in errors, latency, resource usage

### 3. Identify affected service(s)
```promql
# Which service has errors?
topk(5, sum by (service) (rate(http_requests{status=~"5.."}[5m])))
```

### 4. Check recent changes
```bash
# Recent deployments
kubectl get events --sort-by='.lastTimestamp' | head -20

# Recent config changes
git log --since="1 hour ago" --oneline
```

## Diagnosis

### Check application logs
```logql
# Loki query
{service="api"} |= "ERROR" | json | line_format "{{.level}}: {{.message}}"
```

### Check traces
- Jaeger: http://jaeger.company.com
- Search for failing requests
- Look for slow/failing spans

### Check dependencies
```bash
# Database
pg_isready -h postgres-primary
SELECT count(*) FROM pg_stat_activity WHERE state = 'active';

# Redis
redis-cli -h redis ping

# External APIs
curl -I https://payment-gateway.external.com/health
```

## Common Causes & Solutions

### 1. Bad Deployment
**Symptoms:** Errors started after recent deploy

**Solution:**
```bash
# Immediate rollback
kubectl rollout undo deployment/api-service

# Verify
kubectl rollout status deployment/api-service
```

### 2. Database Issues
**Symptoms:** Slow queries, timeouts

**Solution:**
```sql
-- Check long-running queries
SELECT pid, age(clock_timestamp(), query_start), query
FROM pg_stat_activity
WHERE state = 'active' AND query_start < now() - interval '1 minute'
ORDER BY query_start;

-- Kill if needed
SELECT pg_terminate_backend(pid);
```

### 3. Resource Exhaustion
**Symptoms:** High CPU/memory, OOMKills

**Solution:**
```bash
# Scale up immediately
kubectl scale deployment/api-service --replicas=10

# Check resource usage
kubectl top pods
```

### 4. External API Failure
**Symptoms:** Timeout errors, circuit breaker open

**Solution:**
```bash
# Enable fallback/cache
kubectl set env deployment/api-service USE_CACHE=true

# Bypass failing dependency if non-critical
kubectl set env deployment/api-service FEATURE_X_ENABLED=false
```

## Mitigation Strategy

### Immediate (Stop the bleeding)
1. Rollback bad deployment
2. Scale up if resource constrained
3. Enable circuit breakers
4. Route to healthy instances

### Short-term (Stabilize)
1. Apply proper fix
2. Gradual rollout with monitoring
3. Load test before full deployment

### Long-term (Prevent)
1. Add pre-deployment tests
2. Improve monitoring/alerting
3. Implement gradual rollouts
4. Add chaos testing

## Verification

- [ ] Error rate < 0.1%
- [ ] Latency back to normal
- [ ] No active alerts
- [ ] Users not reporting issues

## Communication

### During incident
```
Slack: #incidents
"Investigating high error rate on API service. 
ETA for resolution: 15 minutes. 
Status page: https://status.company.com"
```

### After resolution
```
"Issue resolved. Root cause: [X]. 
Total impact: [Y] minutes. 
Postmortem scheduled for [date]."
```

## Escalation Path
1. **L1** (0-5 min): On-call engineer
2. **L2** (5-15 min): Team lead
3. **L3** (15-30 min): Engineering manager
4. **L4** (30+ min): VP Engineering + CTO

## Postmortem
Schedule within 24 hours. Template: docs/postmortem-template.md

## Related Runbooks
- [Service Down](./service-down.md)
- [High Latency](./high-latency.md)
- [Database Issues](./database-issues.md)
```

### üéØ –ö—Ä–∏—Ç–µ—Ä–∏–∏ —É—Å–ø–µ—à–Ω–æ–π —Å–¥–∞—á–∏ –ø—Ä–æ–µ–∫—Ç–∞

**Must Have (–æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ):**
- [ ] –í—Å–µ —Å–µ—Ä–≤–∏—Å—ã –∑–∞–ø—É—Å–∫–∞—é—Ç—Å—è –æ–¥–Ω–æ–π –∫–æ–º–∞–Ω–¥–æ–π
- [ ] Prometheus —Å–æ–±–∏—Ä–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ —Å–æ –≤—Å–µ—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
- [ ] 3+ dashboard –≤ Grafana (Business, Technical, Infrastructure)
- [ ] Loki —Å–æ–±–∏—Ä–∞–µ—Ç –ª–æ–≥–∏ –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ
- [ ] Distributed tracing —Ä–∞–±–æ—Ç–∞–µ—Ç —á–µ—Ä–µ–∑ Jaeger/Tempo
- [ ] 10+ production alerts –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã
- [ ] 3+ SLO –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã –∏ –∏–∑–º–µ—Ä—è—é—Ç—Å—è
- [ ] Runbooks –¥–ª—è –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö alerts
- [ ] Load testing –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å
- [ ] Documentation (README, architecture, SLOs)

**Nice to Have (–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ):**
- [ ] Multi-environment (dev/staging/prod)
- [ ] Automated remediation
- [ ] Chaos engineering suite
- [ ] Cost analysis dashboard
- [ ] Security monitoring
- [ ] Capacity planning dashboard
- [ ] Custom exporters
- [ ] Integration tests
- [ ] Performance benchmarks
- [ ] Postmortem examples

---

## üìö –ö–∞—Ä—å–µ—Ä–Ω—ã–π –ø—É—Ç—å DevOps/SRE

### Junior DevOps/Monitoring Engineer (0-2 –≥–æ–¥–∞)
**–ù–∞–≤—ã–∫–∏:**
- Linux basics
- Docker basics
- Basic monitoring (Prometheus, Grafana)
- Log aggregation basics
- Alert configuration
- Dashboard creation

**–ó–∞—Ä–ø–ª–∞—Ç–∞:** $40k-70k

### Middle DevOps/SRE (2-4 –≥–æ–¥–∞)
**–ù–∞–≤—ã–∫–∏:**
- Advanced Prometheus (recording rules, federation)
- Distributed tracing
- SLI/SLO management
- Incident response
- CI/CD integration
- Infrastructure as Code

**–ó–∞—Ä–ø–ª–∞—Ç–∞:** $70k-120k

### Senior SRE (4-7 –ª–µ—Ç)
**–ù–∞–≤—ã–∫–∏:**
- System design –¥–ª—è observability
- Multi-cloud monitoring
- Capacity planning
- Cost optimization
- Team leadership
- On-call strategy

**–ó–∞—Ä–ø–ª–∞—Ç–∞:** $120k-180k

### Staff/Principal SRE (7+ –ª–µ—Ç)
**–ù–∞–≤—ã–∫–∏:**
- Organization-wide observability strategy
- Tooling development
- SLO framework design
- Incident management process
- Technical leadership

**–ó–∞—Ä–ø–ª–∞—Ç–∞:** $180k-300k+

### –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ —Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏—è—Ö

**–¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ:**
1. **Explain the difference between monitoring and observability**
2. **How would you monitor a microservices architecture?**
3. **What is high cardinality and why is it a problem?**
4. **Design an alerting strategy that avoids alert fatigue**
5. **How do you calculate error budget for 99.9% SLO?**
6. **Explain push vs pull monitoring models**
7. **How would you debug a memory leak in production?**
8. **What metrics would you track for a database?**

**–°–∏—Ç—É–∞—Ü–∏–æ–Ω–Ω—ã–µ:**
1. **Production is down, walk me through your process**
2. **You're getting 100 alerts per minute, what do you do?**
3. **Disk is 99% full but you can't find large files**
4. **Latency increased 10x after deployment, how to investigate?**
5. **Your monitoring system is down, how do you monitor?**

**System Design:**
1. **Design monitoring for a global CDN**
2. **Design alerting for 10,000 microservices**
3. **How would you monitor a mobile app backend?**

---

## üèÜ –§–∏–Ω–∞–ª—å–Ω—ã–π —ç–∫–∑–∞–º–µ–Ω

### –ß–∞—Å—Ç—å 1: –¢–µ–æ—Ä–∏—è (30 –±–∞–ª–ª–æ–≤)

**–í–æ–ø—Ä–æ—Å 1 (10 –±–∞–ª–ª–æ–≤):** –û–±—ä—è—Å–Ω–∏ –∫–æ–Ω—Ü–µ–ø—Ü–∏—é Error Budget –∏ –∫–∞–∫ –µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ —Å–∫–æ—Ä–æ—Å—Ç–∏ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏ –∏ –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏.

**–í–æ–ø—Ä–æ—Å 2 (10 –±–∞–ª–ª–æ–≤):** –°—Ä–∞–≤–Ω–∏ USE –∏ RED –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏. –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∫–∞–∂–¥—É—é?

**–í–æ–ø—Ä–æ—Å 3 (10 –±–∞–ª–ª–æ–≤):** –ß—Ç–æ —Ç–∞–∫–æ–µ "high cardinality" –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –º–µ—Ç—Ä–∏–∫? –ü–æ—á–µ–º—É —ç—Ç–æ –ø—Ä–æ–±–ª–µ–º–∞ –∏ –∫–∞–∫ –µ—ë —Ä–µ—à–∏—Ç—å?

### –ß–∞—Å—Ç—å 2: –ü—Ä–∞–∫—Ç–∏–∫–∞ (70 –±–∞–ª–ª–æ–≤)

**–ó–∞–¥–∞–Ω–∏–µ 1: Incident Response (25 –±–∞–ª–ª–æ–≤)**
```
–°—Ü–µ–Ω–∞—Ä–∏–π:
- 02:00 AM: PagerDuty alert "API Error Rate High"
- Current error rate: 15% (normal: 0.1%)
- Last deployment: 4 hours ago
- Affected: Payment service

–ó–∞–¥–∞—á–∏:
1. –ù–∞–ø–∏—à–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å –¥–µ–π—Å—Ç–≤–∏–π (first 10 minutes)
2. –ö–∞–∫–∏–µ –º–µ—Ç—Ä–∏–∫–∏/–ª–æ–≥–∏/traces –ø—Ä–æ–≤–µ—Ä–∏—à—å?
3. 3 –Ω–∞–∏–±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã
4. Mitigation strategy –¥–ª—è –∫–∞–∂–¥–æ–π
5. Communication plan
```

**–ó–∞–¥–∞–Ω–∏–µ 2: Monitoring Design (25 –±–∞–ª–ª–æ–≤)**
```
–°–ø—Ä–æ–µ–∫—Ç–∏—Ä—É–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –¥–ª—è:
- Video streaming platform
- 10M users
- 1M concurrent streams
- Multi-region deployment

–û–ø—Ä–µ–¥–µ–ª–∏:
1. Key metrics (–º–∏–Ω–∏–º—É–º 15)
2. SLIs –∏ SLOs (–º–∏–Ω–∏–º—É–º 5)
3. Critical alerts (–º–∏–Ω–∏–º—É–º 8)
4. Dashboard structure
5. Cost estimation
```

**–ó–∞–¥–∞–Ω–∏–µ 3: PromQL Challenge (20 –±–∞–ª–ª–æ–≤)**

–ù–∞–ø–∏—à–∏ –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è:
```
1. CPU usage per pod (excluding idle)
2. p99 latency for last 24 hours
3. Error rate by endpoint (last 5 min)
4. Predict when disk will be full
5. Cache hit ratio trending down
6. Requests per second by service
7. Top 5 slowest endpoints
8. Database connection pool utilization
9. Apdex score (T=300ms)
10. Memory usage forecast (next 2 hours)


---

## üéì –°–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç –æ –ø—Ä–æ—Ö–æ–∂–¥–µ–Ω–∏–∏

```

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë                                                            ‚ïë
‚ïë            üéì –°–ï–†–¢–ò–§–ò–ö–ê–¢ –û –ü–†–û–•–û–ñ–î–ï–ù–ò–ò –ö–£–†–°–ê               ‚ïë
‚ïë                                                            ‚ïë
‚ïë                 "–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –¥–ª—è DevOps"                    ‚ïë
‚ïë                                                            ‚ïë
‚ïë  –ù–∞—Å—Ç–æ—è—â–∏–π —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ—Ç, —á—Ç–æ                   ‚ïë
‚ïë                                                            ‚ïë
‚ïë                    _______________________                 ‚ïë
‚ïë                                                            ‚ïë
‚ïë  —É—Å–ø–µ—à–Ω–æ –ø—Ä–æ—à—ë–ª(–ª–∞) –∏–Ω—Ç–µ–Ω—Å–∏–≤–Ω—ã–π –∫—É—Ä—Å –ø–æ DevOps             ‚ïë
‚ïë  –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥—É –∏ –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª(–∞) –≥–ª—É–±–æ–∫–∏–µ               ‚ïë
‚ïë  –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –Ω–∞–≤—ã–∫–∏ –≤ —Å–ª–µ–¥—É—é—â–∏—Ö –æ–±–ª–∞—Å—Ç—è—Ö:                 ‚ïë
‚ïë                                                            ‚ïë
‚ïë  ‚úì Prometheus & PromQL                                     ‚ïë
‚ïë  ‚úì Grafana Dashboards & Visualization                      ‚ïë
‚ïë  ‚úì Distributed Tracing (Jaeger/Tempo)                      ‚ïë
‚ïë  ‚úì Centralized Logging (Loki)                              ‚ïë
‚ïë  ‚úì Alerting & Incident Response                            ‚ïë
‚ïë  ‚úì SLI/SLO/SLA Management                                  ‚ïë
‚ïë  ‚úì Production Best Practices                               ‚ïë
‚ïë  ‚úì Chaos Engineering                                       ‚ïë
‚ïë                                                            ‚ïë
‚ïë  –î–∞—Ç–∞ –≤—ã–¥–∞—á–∏: _______________                              ‚ïë
‚ïë                                                            ‚ïë
‚ïë  –ü–æ–¥–ø–∏—Å—å –∏–Ω—Å—Ç—Ä—É–∫—Ç–æ—Ä–∞: _______________                      ‚ïë
‚ïë                                                            ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
```

---

## üìñ –ü–æ–ª–µ–∑–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è

### –ö–Ω–∏–≥–∏ üìö

**–û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ –∫ –ø—Ä–æ—á—Ç–µ–Ω–∏—é:**
1. **"Site Reliability Engineering"** - Google
   - –ë–∏–±–ª–∏—è SRE, –±–µ—Å–ø–ª–∞—Ç–Ω–æ –æ–Ω–ª–∞–π–Ω
   - https://sre.google/books/

2. **"The Site Reliability Workbook"** - Google
   - –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–º–µ—Ä—ã SRE –ø—Ä–∞–∫—Ç–∏–∫

3. **"Prometheus: Up & Running"** - Brian Brazil
   - –ì–ª—É–±–æ–∫–æ–µ –ø–æ–≥—Ä—É–∂–µ–Ω–∏–µ –≤ Prometheus

4. **"Observability Engineering"** - Charity Majors et al.
   - –°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ –Ω–∞–±–ª—é–¥–∞–µ–º–æ—Å—Ç–∏

5. **"Database Reliability Engineering"** - Laine Campbell
   - –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ë–î –≤ –¥–µ—Ç–∞–ª—è—Ö

**–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ:**
- "The Phoenix Project" - Gene Kim (DevOps –∫—É–ª—å—Ç—É—Ä–∞)
- "Systems Performance" - Brendan Gregg (Performance analysis)
- "Designing Data-Intensive Applications" - Martin Kleppmann

### –û–Ω–ª–∞–π–Ω –∫—É—Ä—Å—ã üéì

**–ë–µ—Å–ø–ª–∞—Ç–Ω—ã–µ:**
- **Prometheus Official Tutorial** - prometheus.io/docs/tutorials
- **Grafana Fundamentals** - grafana.com/tutorials
- **Google SRE Course** - linkedin.com/learning
- **Linux Performance** - brendangregg.com

**–ü–ª–∞—Ç–Ω—ã–µ:**
- **Coursera**: "Site Reliability Engineering"
- **Udemy**: "Prometheus | The Complete Hands-On for Monitoring"
- **A Cloud Guru**: "Monitoring and Observability"
- **Linux Foundation**: "Prometheus Certified Associate (PCA)"

### –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–µ –ª–∞–±—ã üî¨

```
killercoda.com          - Interactive Kubernetes & monitoring labs
katacoda.com            - Free interactive scenarios
play-with-docker.com    - Docker playground
instruqt.com            - Hands-on labs
```

### –°–æ–æ–±—â–µ—Å—Ç–≤–∞ üë•

**Slack:**
- CNCF Slack (#prometheus, #grafana)
- Kubernetes Slack
- SRE Community

**Reddit:**
- r/devops
- r/sre
- r/kubernetes
- r/prometheus

**Telegram (—Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–µ):**
- DevOps Russia
- SRE Russia
- Kubernetes Russia

**Discord:**
- DevOps Chat
- Cloud Native Computing

### YouTube –∫–∞–Ω–∞–ª—ã üì∫

- **TechWorld with Nana** - DevOps tutorials
- **That DevOps Guy** - Practical DevOps
- **DevOps Toolkit** - Advanced topics
- **KodeKloud** - Structured courses
- **Brendan Gregg** - Performance analysis
- **Google Cloud Tech** - SRE practices

### –ë–ª–æ–≥–∏ –∏ —Å–∞–π—Ç—ã üåê

**–û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ —á–∏—Ç–∞—Ç—å:**
- https://sre.google - Google SRE
- https://brendangregg.com - Performance
- https://prometheus.io/blog - Prometheus news
- https://grafana.com/blog - Grafana insights
- https://www.honeycomb.io/blog - Observability

**–†—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–µ:**
- habr.com (—Ç–µ–≥ #devops, #monitoring)
- highload.ru
- devops.ru

### –ö–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏–∏ üé§

**–ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–µ:**
- **KubeCon + CloudNativeCon** - CNCF flagship
- **SREcon** - USENIX SRE conference
- **GrafanaCON** - Observability
- **PromCon** - Prometheus conference
- **Monitorama** - Monitoring practices

**–†—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–µ:**
- **HighLoad++** - –ú–æ—Å–∫–≤–∞
- **DevOops** - –°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥
- **Saint TeamLead Conf**

### –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è –ø—Ä–∞–∫—Ç–∏–∫–∏ üõ†Ô∏è

**Monitoring Playground:**
```bash
# –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç —Å –≥–æ—Ç–æ–≤—ã–º —Å—Ç–µ–∫–æ–º
git clone https://github.com/vegasbrianc/prometheus
cd prometheus
docker-compose up -d

# Demo –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
https://github.com/prometheus/demo
https://github.com/grafana/tns
```

**–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä—ã –Ω–∞–≥—Ä—É–∑–∫–∏:**
- **k6** - Modern load testing (k6.io)
- **hey** - HTTP load generator
- **ab** - Apache Benchmark
- **wrk** - HTTP benchmark tool
- **locust** - Python load testing

**Chaos Engineering:**
- **Chaos Mesh** - Kubernetes chaos
- **Gremlin** - Chaos as a Service
- **Pumba** - Docker chaos
- **Chaos Toolkit** - Generic chaos

---

## üéØ 30-–¥–Ω–µ–≤–Ω—ã–π –ø–ª–∞–Ω —Ä–∞–∑–≤–∏—Ç–∏—è –ø–æ—Å–ª–µ –∫—É—Ä—Å–∞

### –ù–µ–¥–µ–ª—è 1: –ó–∞–∫—Ä–µ–ø–ª–µ–Ω–∏–µ –æ—Å–Ω–æ–≤
**–î–µ–Ω—å 1-2:** –ü–æ–≤—Ç–æ—Ä–∏ –º–æ–¥—É–ª–∏ 1-3
- –£—Å—Ç–∞–Ω–æ–≤–∏ Prometheus + Grafana –ª–æ–∫–∞–ª—å–Ω–æ
- –°–æ–∑–¥–∞–π 3 custom dashboards

**–î–µ–Ω—å 3-4:** –ü—Ä–∞–∫—Ç–∏–∫–∞ PromQL
- –†–µ—à–∞–π –∑–∞–¥–∞—á–∏ –Ω–∞ promql.io
- –ù–∞–ø–∏—à–∏ 50 —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤

**–î–µ–Ω—å 5-7:** Pet project
- –í—ã–±–µ—Ä–∏ —Å–≤–æ–π –ø—Ä–æ–µ–∫—Ç –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
- –ù–∞—Å—Ç—Ä–æ–π –ø–æ–ª–Ω—ã–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Å—Ç–µ–∫

### –ù–µ–¥–µ–ª—è 2: –£–≥–ª—É–±–ª–µ–Ω–∏–µ
**–î–µ–Ω—å 8-10:** Distributed Tracing
- –î–æ–±–∞–≤—å tracing –≤ —Å–≤–æ–π –ø—Ä–æ–µ–∫—Ç
- –ò–∑—É—á–∏ –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ Jaeger UI

**–î–µ–Ω—å 11-12:** –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
- –ù–∞—Å—Ç—Ä–æ–π structured logging
- –ü—Ä–∞–∫—Ç–∏–∫–∞ LogQL –∑–∞–ø—Ä–æ—Å–æ–≤

**–î–µ–Ω—å 13-14:** Alerting
- –°–æ–∑–¥–∞–π 20 production-ready alerts
- –ù–∞–ø–∏—à–∏ runbooks –¥–ª—è –∫–∞–∂–¥–æ–≥–æ

### –ù–µ–¥–µ–ª—è 3: Production –ø—Ä–∞–∫—Ç–∏–∫–∏
**–î–µ–Ω—å 15-17:** SLO/SLI
- –û–ø—Ä–µ–¥–µ–ª–∏ SLO –¥–ª—è —Å–≤–æ–µ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞
- –ù–∞—Å—Ç—Ä–æ–π error budget tracking
- –°–æ–∑–¥–∞–π SLO dashboard

**–î–µ–Ω—å 18-19:** Chaos Engineering
- –ü—Ä–æ–≤–µ–¥–∏ 10 chaos experiments
- –ü—Ä–æ–≤–µ—Ä—å, —á—Ç–æ –≤—Å–µ alerts —Å—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç

**–î–µ–Ω—å 20-21:** Performance
- –û–ø—Ç–∏–º–∏–∑–∏—Ä—É–π Prometheus (cardinality)
- –î–æ–±–∞–≤—å recording rules
- –ù–∞—Å—Ç—Ä–æ–π federation (–µ—Å–ª–∏ –Ω—É–∂–Ω–æ)

### –ù–µ–¥–µ–ª—è 4: –†–µ–∞–ª—å–Ω—ã–µ –ø—Ä–æ–µ–∫—Ç—ã
**–î–µ–Ω—å 22-24:** Open Source –≤–∫–ª–∞–¥
- –ù–∞–π–¥–∏ issue –≤ Prometheus/Grafana
- –ò–ª–∏ —Å–æ–∑–¥–∞–π –ø–æ–ª–µ–∑–Ω—ã–π dashboard
- –ü–æ–¥–µ–ª–∏—Å—å –≤ —Å–æ–æ–±—â–µ—Å—Ç–≤–µ

**–î–µ–Ω—å 25-27:** –ü–æ—Ä—Ç—Ñ–æ–ª–∏–æ
- –ó–∞–¥–æ–∫—É–º–µ–Ω—Ç–∏—Ä—É–π –≤—Å–µ –ø—Ä–æ–µ–∫—Ç—ã
- –í—ã–ª–æ–∂–∏ –Ω–∞ GitHub
- –ù–∞–ø–∏—à–∏ —Å—Ç–∞—Ç—å—é –Ω–∞ Habr/Medium

**–î–µ–Ω—å 28-30:** –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫ —Å–æ–±–µ—Å–µ–¥–æ–≤–∞–Ω–∏—è–º
- –ü–æ–≤—Ç–æ—Ä–∏ —Ç–µ–æ—Ä–∏—é
- –ü—Ä–∞–∫—Ç–∏–∫—É–π –æ—Ç–≤–µ—Ç—ã –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã
- Mock interviews

---

## üöÄ –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏ –≤ –∫–∞—Ä—å–µ—Ä–µ

### –°–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏

**Prometheus & Kubernetes:**
- **Prometheus Certified Associate (PCA)** - $300
  - –û—Ñ–∏—Ü–∏–∞–ª—å–Ω–∞—è —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è CNCF
  - –≠–∫–∑–∞–º–µ–Ω: 90 –º–∏–Ω—É—Ç, multiple choice
  
- **Certified Kubernetes Administrator (CKA)** - $395
  - –û—Å–Ω–æ–≤–∞ –¥–ª—è SRE —Ä–æ–ª–∏
  - –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–π —ç–∫–∑–∞–º–µ–Ω: 2 —á–∞—Å–∞

- **Certified Kubernetes Application Developer (CKAD)** - $395

**Cloud —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏:**
- **AWS Certified Solutions Architect**
- **Google Cloud Professional Cloud Architect**
- **Azure Administrator**

### –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –ø–æ—Ä—Ç—Ñ–æ–ª–∏–æ

**–ß—Ç–æ –≤–∫–ª—é—á–∏—Ç—å:**

1. **GitHub —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏**
```
monitoring-stack/           - –í–∞—à monitoring setup
‚îú‚îÄ‚îÄ prometheus/
‚îú‚îÄ‚îÄ grafana/
‚îú‚îÄ‚îÄ alerts/
‚îî‚îÄ‚îÄ README.md              - –î–µ—Ç–∞–ª—å–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ

custom-exporters/          - –í–∞—à–∏ exporters
slo-dashboards/            - Production dashboards
chaos-tests/               - Chaos experiments
```

2. **–ë–ª–æ–≥/—Å—Ç–∞—Ç—å–∏**
- Medium/Habr —Å—Ç–∞—Ç—å–∏
- –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ —Ç—É—Ç–æ—Ä–∏–∞–ª—ã
- Case studies –∏–∑ –ø—Ä–∞–∫—Ç–∏–∫–∏

3. **–í—ã—Å—Ç—É–ø–ª–µ–Ω–∏—è**
- –ú–∏—Ç–∞–ø—ã
- –ö–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏–∏
- YouTube –≤–∏–¥–µ–æ

### –°–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏

–ü–æ—Å–ª–µ –±–∞–∑–æ–≤–æ–≥–æ DevOps –º–æ–∂–Ω–æ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å—Å—è:

**1. Site Reliability Engineering (SRE)**
- –§–æ–∫—É—Å –Ω–∞ reliability –∏ scale
- SLO/SLA management
- Incident management
- Capacity planning

**2. Platform Engineering**
- –í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã
- Developer experience
- Self-service –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã

**3. Security Engineering (DevSecOps)**
- Security monitoring
- SIEM
- Compliance
- Vulnerability management

**4. Cloud Architect**
- Multi-cloud solutions
- Cost optimization
- Architecture design

**5. Observability Engineer**
- Advanced tracing
- AIOps
- Custom instrumentation

---

## üí° Pro Tips –æ—Ç –æ–ø—ã—Ç–Ω—ã—Ö SRE

### –ö—É–ª—å—Ç—É—Ä–∞ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞

**DO:**
‚úÖ –ù–∞—á–∏–Ω–∞–π —Å –±–∏–∑–Ω–µ—Å-–º–µ—Ç—Ä–∏–∫
‚úÖ –ú–æ–Ω–∏—Ç–æ—Ä—å user experience
‚úÖ –î–µ–ª–∞–π –ø–æ—Å—Ç–º–æ—Ä—Ç–µ–º—ã –±–µ–∑ –æ–±–≤–∏–Ω–µ–Ω–∏–π
‚úÖ –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä—É–π —Ä—É—Ç–∏–Ω—É
‚úÖ –î–æ–∫—É–º–µ–Ω—Ç–∏—Ä—É–π –≤—Å—ë
‚úÖ –î–µ–ª–∏—Å—å –∑–Ω–∞–Ω–∏—è–º–∏ —Å –∫–æ–º–∞–Ω–¥–æ–π
‚úÖ –¢–µ—Å—Ç–∏—Ä—É–π alerts —Ä–µ–≥—É–ª—è—Ä–Ω–æ

**DON'T:**
‚ùå –ù–µ –º–æ–Ω–∏—Ç–æ—Ä—å —Ä–∞–¥–∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
‚ùå –ù–µ —Å–æ–∑–¥–∞–≤–∞–π alerts –±–µ–∑ action
‚ùå –ù–µ –∏–≥–Ω–æ—Ä–∏—Ä—É–π alert fatigue
‚ùå –ù–µ –¥–µ–ª–∞–π dashboards "–¥–ª—è –≥–∞–ª–æ—á–∫–∏"
‚ùå –ù–µ —Ö—Ä–∞–Ω–∏ –ª–æ–≥–∏ –≤–µ—á–Ω–æ (cost!)
‚ùå –ù–µ –∑–∞–±—ã–≤–∞–π –ø—Ä–æ security

### –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å

```promql
# –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è PromQL
# ‚ùå –ú–µ–¥–ª–µ–Ω–Ω–æ
sum(rate(http_requests_total[5m])) by (method, status, path)

# ‚úÖ –ë—ã—Å—Ç—Ä–µ–µ (–º–µ–Ω—å—à–µ labels)
sum(rate(http_requests_total[5m])) by (method, status)

# ‚úÖ –ï—â—ë –±—ã—Å—Ç—Ä–µ–µ (recording rule)
job:http_requests:rate5m
```

### –ö–∞—Ä—å–µ—Ä–Ω—ã–µ —Å–æ–≤–µ—Ç—ã

1. **Soft skills –≤–∞–∂–Ω—ã**
   - Communication
   - Incident management
   - Teaching others
   - Documentation

2. **–ù–µ –∑–∞—Ü–∏–∫–ª–∏–≤–∞–π—Å—è –Ω–∞ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞—Ö**
   - –ü–æ–Ω–∏–º–∞–π –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏
   - –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –º–µ–Ω—è—é—Ç—Å—è
   - –ü—Ä–∏–Ω—Ü–∏–ø—ã –æ—Å—Ç–∞—é—Ç—Å—è

3. **–£—á–∞—Å—Ç–≤—É–π –≤ on-call**
   - –õ—É—á—à–µ–µ –æ–±—É—á–µ–Ω–∏–µ
   - –ü–æ–Ω–∏–º–∞–µ—à—å —Å–∏—Å—Ç–µ–º—É –≥–ª—É–±–∂–µ
   - –†–∞—Å—Ç—ë—à—å –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ

4. **–ü–∏—à–∏ –∫–æ–¥**
   - Automation scripts
   - Custom exporters
   - Internal tools

5. **–û—Å—Ç–∞–≤–∞–π—Å—è –≤ –∫—É—Ä—Å–µ**
   - CNCF projects
   - New monitoring tools
   - Industry trends

---

## üéâ –ü–æ–∑–¥—Ä–∞–≤–ª—è–µ–º —Å –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ–º –∫—É—Ä—Å–∞!

–í—ã –ø—Ä–æ—à–ª–∏ –ø—É—Ç—å –æ—Ç –æ—Å–Ω–æ–≤ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –¥–æ production-ready —Ä–µ—à–µ–Ω–∏–π. –¢–µ–ø–µ—Ä—å —É –≤–∞—Å –µ—Å—Ç—å:

‚úÖ –ü–æ–Ω–∏–º–∞–Ω–∏–µ –ø—Ä–∏–Ω—Ü–∏–ø–æ–≤ observability
‚úÖ –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –Ω–∞–≤—ã–∫–∏ —Å Prometheus, Grafana, Loki, Jaeger
‚úÖ –û–ø—ã—Ç —Å–æ–∑–¥–∞–Ω–∏—è dashboards –∏ alerts
‚úÖ –ó–Ω–∞–Ω–∏–µ SRE –ø—Ä–∞–∫—Ç–∏–∫ –∏ SLO management
‚úÖ –ì–æ—Ç–æ–≤–Ω–æ—Å—Ç—å –∫ production incidents
‚úÖ –ü–æ—Ä—Ç—Ñ–æ–ª–∏–æ –ø—Ä–æ–µ–∫—Ç–æ–≤

### –ß—Ç–æ –¥–∞–ª—å—à–µ?

1. **–ü—Ä–∞–∫—Ç–∏–∫—É–π—Å—è –µ–∂–µ–¥–Ω–µ–≤–Ω–æ** - –º–æ–Ω–∏—Ç–æ—Ä—å —Å–≤–æ–∏ –ø—Ä–æ–µ–∫—Ç—ã
2. **–í–Ω–æ—Å–∏ –≤–∫–ª–∞–¥ –≤ Open Source** - –ø–æ–º–æ–≥–∞–π —Å–æ–æ–±—â–µ—Å—Ç–≤—É
3. **–£—á–∏ –¥—Ä—É–≥–∏—Ö** - –ª—É—á—à–∏–π —Å–ø–æ—Å–æ–± –∑–∞–∫—Ä–µ–ø–∏—Ç—å –∑–Ω–∞–Ω–∏—è
4. **–û—Å—Ç–∞–≤–∞–π—Å—è –ª—é–±–æ–ø—ã—Ç–Ω—ã–º** - —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ —Ä–∞–∑–≤–∏–≤–∞—é—Ç—Å—è

### –û–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å

–ú—ã –±—É–¥–µ–º —Ä–∞–¥—ã —É—Å–ª—ã—à–∞—Ç—å:
- –ß—Ç–æ –ø–æ–º–æ–≥–ª–æ –±–æ–ª—å—à–µ –≤—Å–µ–≥–æ?
- –ß—Ç–æ –º–æ–∂–Ω–æ —É–ª—É—á—à–∏—Ç—å?
- –ö–∞–∫–∏–µ —Ç–µ–º—ã –¥–æ–±–∞–≤–∏—Ç—å?

**–ö–æ–Ω—Ç–∞–∫—Ç—ã:**
- GitHub: [—Å—Å—ã–ª–∫–∞]
- Email: monitoring-course@example.com
- Telegram: @devops_monitoring_course

---

## üìú –õ–∏—Ü–µ–Ω–∑–∏—è –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ

–≠—Ç–æ—Ç –∫—É—Ä—Å —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω—è–µ—Ç—Å—è –ø–æ–¥ –ª–∏—Ü–µ–Ω–∑–∏–µ–π **MIT**.

–í—ã –º–æ–∂–µ—Ç–µ:
‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è –ª–∏—á–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
‚úÖ –î–µ–ª–∏—Ç—å—Å—è —Å –∫–æ–ª–ª–µ–≥–∞–º–∏
‚úÖ –ê–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–æ–¥ —Å–≤–æ–∏ –Ω—É–∂–¥—ã
‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≤ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–º –æ–±—É—á–µ–Ω–∏–∏

–ü—Ä–æ—Å–∏–º —Ç–æ–ª—å–∫–æ:
- –°–æ—Ö—Ä–∞–Ω—è—Ç—å —Å—Å—ã–ª–∫—É –Ω–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª
- –î–µ–ª–∏—Ç—å—Å—è —É–ª—É—á—à–µ–Ω–∏—è–º–∏
- –ù–µ –ø—Ä–æ–¥–∞–≤–∞—Ç—å –∫—É—Ä—Å

---

## üôè –ë–ª–∞–≥–æ–¥–∞—Ä–Ω–æ—Å—Ç–∏

–°–ø–∞—Å–∏–±–æ:
- **Prometheus community** - –∑–∞ –æ—Ç–ª–∏—á–Ω—ã–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç
- **Grafana Labs** - –∑–∞ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é
- **Google SRE team** - –∑–∞ –ø—Ä–∞–∫—Ç–∏–∫–∏ –∏ –∫–Ω–∏–≥–∏
- **CNCF** - –∑–∞ —ç–∫–æ—Å–∏—Å—Ç–µ–º—É
- **–í—Å–µ–º –∫–æ–Ω—Ç—Ä–∏–±—å—é—Ç–æ—Ä–∞–º Open Source**

---

## üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫—É—Ä—Å–∞

```
üìö –ú–æ–¥—É–ª–µ–π: 9
‚è±Ô∏è –û–±—â–µ–µ –≤—Ä–µ–º—è: ~3-4 —á–∞—Å–∞
üíª –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞–Ω–∏–π: 27
üéØ –ë–æ–Ω—É—Å–Ω—ã—Ö –∑–∞–¥–∞–Ω–∏–π: 18
üìñ –°—Ç—Ä–æ–∫ –∫–æ–¥–∞: 5000+
üéì –ù–∞–≤—ã–∫–æ–≤: 50+
```

---

## üî• –ú–æ—Ç–∏–≤–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ü–∏—Ç–∞—Ç—ã

> "You can't improve what you don't measure."  
> ‚Äî Peter Drucker

> "Hope is not a strategy."  
> ‚Äî Traditional SRE wisdom

> "If you can't monitor it, you can't manage it."  
> ‚Äî Unknown

> "The best monitoring is the one that alerts you before your users do."  
> ‚Äî SRE Proverb

> "Measure twice, deploy once."  
> ‚Äî DevOps wisdom

---

## üåü –§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–ª–æ–≤–æ

–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ - —ç—Ç–æ –Ω–µ –ø—Ä–æ—Å—Ç–æ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã. –≠—Ç–æ **–∫—É–ª—å—Ç—É—Ä–∞**, **–º—ã—à–ª–µ–Ω–∏–µ** –∏ **–ø—Ä–∞–∫—Ç–∏–∫–∏**, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–º–æ–≥–∞—é—Ç —Å–æ–∑–¥–∞–≤–∞—Ç—å –Ω–∞–¥—ë–∂–Ω—ã–µ —Å–∏—Å—Ç–µ–º—ã.

–í–∞—à–µ –ø—É—Ç–µ—à–µ—Å—Ç–≤–∏–µ —Ç–æ–ª—å–∫–æ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è. –í–ø–µ—Ä–µ–¥–∏:
- –†–µ–∞–ª—å–Ω—ã–µ production –∏–Ω—Ü–∏–¥–µ–Ω—Ç—ã
- –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –º–∏–ª–ª–∏–æ–Ω—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
- –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º
- –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∫–æ–º–∞–Ω–¥—ã
- –í–ª–∏—è–Ω–∏–µ –Ω–∞ –ø—Ä–æ–¥—É–∫—Ç

**–£–¥–∞—á–∏ –≤ –≤–∞—à–µ–º DevOps/SRE –ø—É—Ç–µ—à–µ—Å—Ç–≤–∏–∏! üöÄ**

---

**–í–µ—Ä—Å–∏—è:** 1.0  
**–î–∞—Ç–∞:** –Ø–Ω–≤–∞—Ä—å 2025  
**–ê–≤—Ç–æ—Ä:** DevOps Monitoring Course Team  
**–õ–∏—Ü–µ–Ω–∑–∏—è:** MIT

**Happy Monitoring! üìäüéØüî•**
